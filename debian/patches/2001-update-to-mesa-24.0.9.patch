diff --git a/VERSION b/VERSION
index 1b3e74f84e7..b57d498036e 100644
--- a/VERSION
+++ b/VERSION
@@ -1 +1 @@
-24.0.1
+24.0.9
diff --git a/docs/egl.rst b/docs/egl.rst
index 9d17ca266ae..6f4f75c8a76 100644
--- a/docs/egl.rst
+++ b/docs/egl.rst
@@ -1,7 +1,7 @@
 EGL
 ===
 
-The current version of EGL in Mesa implements EGL 1.4. More information
+The current version of EGL in Mesa implements EGL 1.5. More information
 about EGL can be found at https://www.khronos.org/egl/.
 
 The Mesa's implementation of EGL uses a driver architecture. The main
diff --git a/docs/envvars.rst b/docs/envvars.rst
index 300f2cd40e1..a6af0532467 100644
--- a/docs/envvars.rst
+++ b/docs/envvars.rst
@@ -1357,7 +1357,7 @@ RADV driver environment variables
    ``video_decode``
       enable experimental video decoding support
    ``gsfastlaunch2``
-      use GS_FAST_LAUNCH=2 for Mesh shaders (GFX11+)
+      use GS_FAST_LAUNCH=2 for Mesh shaders (GFX11+ dGPUs only)
 
 .. envvar:: RADV_TEX_ANISO
 
diff --git a/docs/relnotes.rst b/docs/relnotes.rst
index 40164bf1d42..1238954e1c3 100644
--- a/docs/relnotes.rst
+++ b/docs/relnotes.rst
@@ -3,6 +3,14 @@ Release Notes
 
 The release notes summarize what's new or changed in each Mesa release.
 
+-  :doc:`24.0.9 release notes <relnotes/24.0.9>`
+-  :doc:`24.0.8 release notes <relnotes/24.0.8>`
+-  :doc:`24.0.7 release notes <relnotes/24.0.7>`
+-  :doc:`24.0.6 release notes <relnotes/24.0.6>`
+-  :doc:`24.0.5 release notes <relnotes/24.0.5>`
+-  :doc:`24.0.4 release notes <relnotes/24.0.4>`
+-  :doc:`24.0.3 release notes <relnotes/24.0.3>`
+-  :doc:`24.0.2 release notes <relnotes/24.0.2>`
 -  :doc:`24.0.1 release notes <relnotes/24.0.1>`
 -  :doc:`24.0.0 release notes <relnotes/24.0.0>`
 -  :doc:`23.3.3 release notes <relnotes/23.3.3>`
@@ -409,6 +417,14 @@ The release notes summarize what's new or changed in each Mesa release.
    :maxdepth: 1
    :hidden:
 
+   24.0.9 <relnotes/24.0.9>
+   24.0.8 <relnotes/24.0.8>
+   24.0.7 <relnotes/24.0.7>
+   24.0.6 <relnotes/24.0.6>
+   24.0.5 <relnotes/24.0.5>
+   24.0.4 <relnotes/24.0.4>
+   24.0.3 <relnotes/24.0.3>
+   24.0.2 <relnotes/24.0.2>
    24.0.1 <relnotes/24.0.1>
    24.0.0 <relnotes/24.0.0>
    23.3.3 <relnotes/23.3.3>
diff --git a/docs/relnotes/24.0.1.rst b/docs/relnotes/24.0.1.rst
index 21478e67f51..b77cad62740 100644
--- a/docs/relnotes/24.0.1.rst
+++ b/docs/relnotes/24.0.1.rst
@@ -19,7 +19,7 @@ SHA256 checksum
 
 ::
 
-    TBD.
+    f387192b08c471c545590dd12230a2a343244804b5fe866fec6aea02eab57613  mesa-24.0.1.tar.xz
 
 
 New features
diff --git a/docs/relnotes/24.0.2.rst b/docs/relnotes/24.0.2.rst
new file mode 100644
index 00000000000..282fd331869
--- /dev/null
+++ b/docs/relnotes/24.0.2.rst
@@ -0,0 +1,230 @@
+Mesa 24.0.2 Release Notes / 2024-02-28
+======================================
+
+Mesa 24.0.2 is a bug fix release which fixes bugs found since the 24.0.1 release.
+
+Mesa 24.0.2 implements the OpenGL 4.6 API, but the version reported by
+glGetString(GL_VERSION) or glGetIntegerv(GL_MAJOR_VERSION) /
+glGetIntegerv(GL_MINOR_VERSION) depends on the particular driver being used.
+Some drivers don't support all the features required in OpenGL 4.6. OpenGL
+4.6 is **only** available if requested at context creation.
+Compatibility contexts may report a lower version depending on each driver.
+
+Mesa 24.0.2 implements the Vulkan 1.3 API, but the version reported by
+the apiVersion property of the VkPhysicalDeviceProperties struct
+depends on the particular driver being used.
+
+SHA256 checksum
+---------------
+
+::
+
+    94e28a8edad06d8ed2b83eb53f253b9eb5aa62c3080f939702e1b3039b56c9e8  mesa-24.0.2.tar.xz
+
+
+New features
+------------
+
+- None
+
+
+Bug fixes
+---------
+
+- KHR-Single-GL46.arrays_of_arrays_gl.AtomicUsage fails on MTL
+- GTF-GL46.gtf42.GL3Tests.texture_storage.texture_storage_texture_as_framebuffer_attachment fails on MTL
+- [intel][anv][build][regression] - genX_grl.h:27:10: fatal error: grl/grl_cl_kernel.h: No such file or directory
+- RX 6600 VDPAU not recognizing HEVC_MAIN_10 correctly
+- Running an app on another AMD GPU (offload, DRI_PRIME) produces corrupted frames on Wayland.
+- VDPAU declares a texture as "immutable" without also setting its ImmutableLevels attribute.
+- RX6600 hardware HEVC video decode fails for VDPAU but works for VA-API. (Can lock up GPU!)
+- Rusticl panics when getting program build logs using opencl.hpp
+- ue5 game issues lighting Rog Ally 7080u (z1e)
+- Missing textures in RoboCop: Rogue City with mesh shaders enabled
+- radv: Multiview PSO forgets to export layer in some cases.
+- zink: flickering artifacts in Selaco
+
+
+Changes
+-------
+
+Boyuan Zhang (1):
+
+- radeonsi/vcn: only use multi slices reflist when available
+
+Chia-I Wu (1):
+
+- radv: fix pipeline stats mask
+
+Chris Rankin (2):
+
+- vdpau: Declare texture object as immutable using helper function.
+- vdpau: Refactor query for video surface formats.
+
+Connor Abbott (1):
+
+- tu: Follow pipeline compatibility rules for dynamic descriptors
+
+Daniel Schürmann (1):
+
+- spirv: Fix SpvOpExpectKHR
+
+Daniel Stone (2):
+
+- egl/wayland: Add opaque-equivalent FourCCs
+- egl/wayland: Fix EGL_EXT_present_opaque
+
+Dave Airlie (2):
+
+- nouveau/winsys: fix bda heap leak.
+- nvk: fix dri options leak.
+
+David Rosca (1):
+
+- frontends/va: Only set VP9 segmentation fields when segmentation is enabled
+
+Eric Engestrom (10):
+
+- docs: add sha256sum for 24.0.1
+- [24.0-only change] ci: increase the kernel+rootfs builds timeout to 2h
+- .pick_status.json: Update to c6e855b64b9015235462959b2b7f3e9fc34b2f1f
+- .pick_status.json: Update to dce20690542c84ac00509a6db7902dcfc90b25bb
+- .pick_status.json: Update to c12300844d3f084ca011a3f54f0cbaa9807418f0
+- .pick_status.json: Mark 3b927567ac927316eb11901f50ee1573ead44fd2 as denominated
+- .pick_status.json: Update to 423add61e2d5b6ab6b5505d1feec01b93609f8fc
+- .pick_status.json: Update to 4071c399a27932ea9253eb8a65d5725504bac6f3
+- .pick_status.json: Update to 82ff9204abab5267f82a9ce73f9dca1541ef5ee6
+- [24.0 only] disable clang-format
+
+Erik Faye-Lund (1):
+
+- mesa/main: allow GL_BGRA for FBOs
+
+Faith Ekstrand (1):
+
+- nvk: Invalidate the texture cache before MSAA resolves
+
+Hans-Kristian Arntzen (1):
+
+- radv: export multiview in VS/TES/GS for depth-only rendering
+
+Iago Toral Quiroga (1):
+
+- v3d,v3dv: fix BO allocation for shared vars
+
+Ian Romanick (1):
+
+- nir: Mark nir_intrinsic_load_global_block_intel as divergent
+
+Jesse Natalie (1):
+
+- dzn: Don't set view instancing mask until after the PSO
+
+Jordan Justen (1):
+
+- intel/dev: Add 2 additional ADL-N PCI ids
+
+Juston Li (1):
+
+- venus: fix image reqs cache store locking
+
+Karol Herbst (3):
+
+- zink: lower unaligned memory accesses
+- rusticl/program: fix CL_PROGRAM_BINARIES for devs with no builds
+- meson: do not pull in clc for clover
+
+Konstantin Seurer (5):
+
+- zink: Always set mfence->submit_count to the fence submit_count
+- Revert "zink: always force flushes when originating from api frontend"
+- llvmpipe: Use full subgroups when possible
+- gallivm: Consider the initial mask when terminating loops
+- ci: Update llvmpipe trace checksums
+
+Lionel Landwerlin (8):
+
+- vulkan/runtime: add helper to query attachment layout
+- anv: fixup push descriptor shader analysis
+- anv: reenable ANV_ALWAYS_BINDLESS
+- anv: fix Wa_16013994831 macros
+- anv: disable Wa_16013994831
+- intel/nir: only consider ray query variables in lowering
+- anv: limit depth flush on dynamic render pass suspend
+- anv: add missing generated file dep
+
+Martin Roukala (né Peres) (1):
+
+- radv/ci: switch vkcts-polaris10 from mupuf to KWS' farm
+
+Michel Dänzer (1):
+
+- egl/wayland: Flush after blitting to linear copy
+
+Mike Blumenkrantz (25):
+
+- zink: prune dmabuf export tracking when adding resource binds
+- zink: fix sparse bo placement
+- zink: zero allocate resident_defs array in ntv
+- zink: move sparse lowering up in file
+- zink: run sparse lowering after all optimization passes
+- zink: adjust swizzled deref loads by the variable component offset
+- zink: clamp zink_gfx_lib_cache::stages_present for generated tcs
+- zink: promote gpl libs freeing during shader destroy out of prog loop
+- zink: don't add VK_IMAGE_CREATE_2D_ARRAY_COMPATIBLE_BIT for sparse textures
+- zink: delete maxDescriptorBufferBindings checks
+- zink: avoid infinite recursion on (very) small BAR systems in bo alloc
+- zink: add checks/compat for low-spec descriptor buffer implementations
+- zink: add a second fence disambiguation case
+- zink: force host-visible allocations for MAP_COHERENT resources
+- zink: handle stencil_fallback in zink_clear_depth_stencil
+- zink: don't destroy the current batch state on context destroy
+- mesa: check driver format support for certain GetInternalformativ queries
+- vk/wsi/x11/sw: use swapchain depth for putimage
+- zink: only scan active batch states for free states if > 1 exist
+- zink: fix longstanding issue with active batch state recycling
+- zink: assert that batch_id is valid in zink_screen_check_last_finished()
+- zink: clamp in_rp clears to fb size
+- zink: fix (dynamic rendering) execution of scissored clears during flush
+- zink: lock buffer age when chundering swapchain for readback
+- zink: flag acquired swapchain image as readback target on acquire, not present
+
+Patrick Lerda (3):
+
+- r300: fix vertex_buffer related refcnt imbalance
+- r300: fix r300_destroy_context() related memory leaks
+- r300: fix memory leaks when register allocation fails
+
+Pavel Ondračka (1):
+
+- r300: add explicit flrp lowering
+
+Rhys Perry (2):
+
+- aco/ra: don't initialize assigned in initializer list
+- aco/ra: fix GFX9- writelane
+
+Sagar Ghuge (1):
+
+- nir: Allow nir_texop_tg4 in implicit derivative
+
+Samuel Pitoiset (4):
+
+- radv: fix RGP barrier reason for RP barriers inserted by the runtime
+- radv: enable GS_FAST_LAUNCH=2 by default for RDNA3 APUs (Phoenix)
+- spirv: only consider IO variables when adjusting patch locations for TES
+- radv: fix indirect dispatches on compute queue with conditional rendering on GFX7
+
+Tapani Pälli (2):
+
+- intel/blorp: disable use of REP16 independent of format
+- iris: make sure DS and TE are sent in pairs on >= gfx125
+
+Yiwei Zhang (2):
+
+- venus: force async pipeline create on threads creating descriptor pools
+- venus: fix the cmd stride used for qfb recording
+
+thfrwn (1):
+
+- mesa: fix off-by-one for newblock allocation in dlist_alloc
diff --git a/docs/relnotes/24.0.3.rst b/docs/relnotes/24.0.3.rst
new file mode 100644
index 00000000000..2ee29a6d002
--- /dev/null
+++ b/docs/relnotes/24.0.3.rst
@@ -0,0 +1,262 @@
+Mesa 24.0.3 Release Notes / 2024-03-13
+======================================
+
+Mesa 24.0.3 is a bug fix release which fixes bugs found since the 24.0.2 release.
+
+Mesa 24.0.3 implements the OpenGL 4.6 API, but the version reported by
+glGetString(GL_VERSION) or glGetIntegerv(GL_MAJOR_VERSION) /
+glGetIntegerv(GL_MINOR_VERSION) depends on the particular driver being used.
+Some drivers don't support all the features required in OpenGL 4.6. OpenGL
+4.6 is **only** available if requested at context creation.
+Compatibility contexts may report a lower version depending on each driver.
+
+Mesa 24.0.3 implements the Vulkan 1.3 API, but the version reported by
+the apiVersion property of the VkPhysicalDeviceProperties struct
+depends on the particular driver being used.
+
+SHA256 checksum
+---------------
+
+::
+
+    77aec9a2a37b7d3596ea1640b3cc53d0b5d9b3b52abed89de07e3717e91bfdbe  mesa-24.0.3.tar.xz
+
+
+New features
+------------
+
+- None
+
+
+Bug fixes
+---------
+
+- v3d: Line rendering broken when smoothing is enabled
+- DR crashes with mesa 24 and rusticl (radeonsi)
+- RADV: GPU crash when setting 'RADV_DEBUG=allbos'
+- [intel] mesa ftbfs with time_t64
+- [radv] Crash when VkGraphicsPipelineCreateInfo::flags = ~0u
+- Gen4 assertion \`force_writemask_all' failed.
+- [radv] Holographic projection texture glitch in Rage 2
+- [build failure] [armhf] - error: #error "_TIME_BITS=64 is allowed only with _FILE_OFFSET_BITS=64"
+- RustiCL: Callbacks are not called upon errors
+- MTL: regressions in vulkancts due to BO CCS allocations
+- zink: spec\@ext_external_objects\@vk-image-overwrite fail
+
+
+Changes
+-------
+
+Boyuan Zhang (1):
+
+- meson: bump the minimal required vdpau version to 1.4
+
+Caio Oliveira (1):
+
+- intel/compiler: Fix SIMD lowering when instruction needs a larger SIMD
+
+Chia-I Wu (1):
+
+- aco: fix nir_op_pack_32_4x8 handling
+
+Christian Gmeiner (1):
+
+- etnaviv: Fix how we determine the max supported number of varyings
+
+Corentin Noël (1):
+
+- zink: Return early if the file descriptor could not have been duplicated/acquired
+
+Daniel Schürmann (1):
+
+- radv: fix initialization of radv_shader_layout->use_dynamic_descriptors
+
+Danylo Piliaiev (1):
+
+- tu: Fix dynamic state not always being emitted
+
+David Heidelberg (6):
+
+- drm-shim: Avoid invalid file and time bits combination
+- ci/intel: decompose anv-tgl-test so we can specify custom devices for TGL
+- ci/intel: add acer-cp514-2h-11{30,60}g7-volteer
+- ci/intel: move machine definition to the intel-tgl-skqp job
+- ci/intel: split asus-cx9400-volteer into acer-cp514-2h-11{30,60}g7-volteer
+- intel/tools: avoid invalid time and file bits combination
+
+David Rosca (1):
+
+- radeonsi/vcn: Use temporal_layer_index to select temporal layer
+
+Eric Engestrom (6):
+
+- docs: add sha256sum for 24.0.2
+- .pick_status.json: Update to 7792ee1c15379d95ccb20ce34352473f2bb2bfbd
+- .pick_status.json: Update to f3fe1f2f18d7ccc8a7cf85cd88c4bdf426445702
+- .pick_status.json: Update to e1afffe7fa7bd8e1cd1f7e58cfa2f33faf889628
+- .pick_status.json: Mark a367cd49314a993d09168e790d3090a2303a48d9 as denominated
+- .pick_status.json: Update to 9a57b1df5395bbcaa6f48ea851860bedc7ceefb9
+
+Eric R. Smith (1):
+
+- panfrost: protect alpha calculation from accessing non-existent component
+
+Faith Ekstrand (4):
+
+- nvk: Return os_page_size for minMemoryMapAlignment
+- nvk: Document the register name for the helper load workaround
+- nvk: Always wait for the FALCON in set_priv_reg
+- nvk: Disable the Out Of Range Address exception
+
+Felix DeGrood (1):
+
+- driconf: add SotTR DX12 to Intel XeSS workaround
+
+Friedrich Vock (3):
+
+- radv/rt: Handle monolithic pipelines in capture/replay
+- radv: Set SCRATCH_EN for RT pipelines based on dynamic stack size
+- radv/rt: Fix frontface culling with emulated RT
+
+Georg Lehmann (6):
+
+- aco: create pseudo instructions with correct struct
+- aco/post-ra: rename overwritten_subdword to allow additional uses
+- aco/post-ra: assume scc is going to be overwritten by phis at end of blocks
+- aco: store if pseudo instr needs scratch reg
+- aco/post-ra: track pseudo scratch sgpr/scc clobber
+- aco/ssa_elimination: check if pseudo scratch reg overwrittes regs used for v_cmpx opt
+
+Gert Wollny (2):
+
+- zink: use only ZINK_BIND_DESCRIPTOR
+- zink/nir-to-spirv: Make sure sampleid for InterpolateAtSample is int
+
+Ian Romanick (1):
+
+- i915: Fix value returned for PIPE_CAP_MAX_TEXTURE_CUBE_LEVELS
+
+Jesse Natalie (3):
+
+- wgl: Check for stw_device->screen before trying to destroy it
+- wgl: Initialize DEVMODE struct
+- nir_lower_tex_shadow: For old-style shadows, use vec4(result, 0, 0, 1)
+
+Job Noorman (1):
+
+- ir3: fix alignment of spill slots
+
+Jonathan Gray (1):
+
+- intel/dev: update DG2 device names
+
+Jose Maria Casanova Crespo (1):
+
+- ci: Adds /usr/local/bin to PATH at piglit-traces.sh
+
+José Roberto de Souza (1):
+
+- iris/xe: Consider pat_index while unbinding the bo
+
+Juan A. Suarez Romero (2):
+
+- v3d: add load_fep_w_v3d intrinsic
+- v3d: fix line coords with perspective projection
+
+Karol Herbst (1):
+
+- rusticl/event: we need to call the CL_COMPLETE callback on errors as well
+
+Kenneth Graunke (2):
+
+- intel/brw: Allow CSE on TXF_CMS_W_GFX12_LOGICAL
+- iris: Fix tessellation evaluation shaders that use scratch
+
+Konstantin Seurer (2):
+
+- radv/rt: Use doubles inside intersect_ray_amd_software_tri
+- radv/rt: Fix raygen_imported condition
+
+Lionel Landwerlin (3):
+
+- anv: fix non matching image/view format attachment resolve
+- anv: fix incorrect ISL usage in buffer view creation
+- anv/iris/blorp: use the right MOCS values for each engine
+
+Mike Blumenkrantz (16):
+
+- zink: apply all storage memory masks to control barriers if no modes are specified
+- zink: emit SpvCapabilityImageMSArray for ms arrayed storage images
+- zink: null out bo usage when allocating from slab
+- zink: fix unsynchronized read-mapping of device-local buffers
+- zink: force max buffer alignment on return ptrs for mapped staging buffers
+- zink: fix stencil-only blitting with stencil fallback
+- vulkan/dispatch_table: add an uncompacted version of the table
+- zink: use uncompacted vk_dispatch_table
+- egl/dri2: use the right egl platform enum
+- zink: stop enabling EXT_conservative_rasterization
+- zink: fix PIPE_CAP_MAX_SHADER_PATCH_VARYINGS
+- zink: call CmdSetRasterizationStreamEXT when using shader objects
+- nvk: bump NVK_PUSH_MAX_SYNCS to 256
+- util/blitter: iterate samples in stencil_fallback
+- mesa: fix CopyTexImage format compatibility checks for ES
+- driconf: add radv_zero_vram for Crystal Project (1637730)
+
+Oskar Viljasaar (1):
+
+- compiler/types: Fix glsl_dvec*_type() helpers
+
+Patrick Lerda (2):
+
+- r300: fix constants_remap_table memory leak
+- radeonsi/gfx10: fix main_shader_part_ngg_es memory leak
+
+Pierre-Eric Pelloux-Prayer (1):
+
+- radeonsi: try to disable dcc if compute_blit is the only option
+
+Rhys Perry (1):
+
+- aco: don't combine linear and normal VGPR copies
+
+Robert Beckett (1):
+
+- vulkan/wsi: fix force_bgra8_unorm_first
+
+Rohan Garg (1):
+
+- anv, blorp: Set COMPUTE_WALKER Message SIMD field
+
+Samuel Pitoiset (5):
+
+- radv: fix conditional rendering with direct mesh+task draws and multiview
+- radv: fix conditional rendering on compute queue on GFX6
+- radv: add missing conditional rendering for indirect dispatches on GFX6
+- radv: enable radv_zero_vram for RAGE2
+- util/u_debug: fix parsing of "all" again
+
+Simon Ser (1):
+
+- egl/wayland: ensure wl_drm is available before use
+
+Tapani Pälli (4):
+
+- iris: make sure aux is disabled for external objects
+- anv: make sure aux is disabled for memory objects
+- hasvk: make sure aux is disabled for memory objects
+- crocus: make sure aux is disabled for memory objects
+
+Vasily Khoruzhick (4):
+
+- lima: ppir: always use vec4 for output register
+- lima: ppir: use dummy program if FS has empty body
+- lima: gpir: abort compilation if load_uniform instrinsic src isn't const
+- lima: update expected CI failures
+
+Yiwei Zhang (1):
+
+- venus: fix ffb batch prepare for a corner case and avoid a memcpy UB
+
+qbojj (1):
+
+- vulkan: Fix calculation of flags in vk_graphics_pipeline_state_fill
diff --git a/docs/relnotes/24.0.4.rst b/docs/relnotes/24.0.4.rst
new file mode 100644
index 00000000000..c3482ab86b5
--- /dev/null
+++ b/docs/relnotes/24.0.4.rst
@@ -0,0 +1,220 @@
+Mesa 24.0.4 Release Notes / 2024-03-27
+======================================
+
+Mesa 24.0.4 is a bug fix release which fixes bugs found since the 24.0.3 release.
+
+Mesa 24.0.4 implements the OpenGL 4.6 API, but the version reported by
+glGetString(GL_VERSION) or glGetIntegerv(GL_MAJOR_VERSION) /
+glGetIntegerv(GL_MINOR_VERSION) depends on the particular driver being used.
+Some drivers don't support all the features required in OpenGL 4.6. OpenGL
+4.6 is **only** available if requested at context creation.
+Compatibility contexts may report a lower version depending on each driver.
+
+Mesa 24.0.4 implements the Vulkan 1.3 API, but the version reported by
+the apiVersion property of the VkPhysicalDeviceProperties struct
+depends on the particular driver being used.
+
+SHA256 checksum
+---------------
+
+::
+
+    90febd30a098cbcd97ff62ecc3dcf5c93d76f7fa314de944cfce81951ba745f0  mesa-24.0.4.tar.xz
+
+
+New features
+------------
+
+- None
+
+
+Bug fixes
+---------
+
+- nvk: dota 2 crashes after ~5 seconds in game
+- VAAPI: Incorrect HEVC block size reported with radeonsi
+- radv: WWE 2K24 has very quirky DCC issues on RDNA2
+- RUSTICL creating a shared reference to mutable static is discouraged and will become a hard error
+- KiCAD 3D Viewer - rounded pads rendered incorrectly (texture mapping or stencil test error)
+- OpenSCAD rendering incorrect and inconsistent on radeonsi
+- [radv] Half-Life Alyx renders solid black for reflective surfaces
+- [RX 7900 XTX] Helldivers 2 cause GPU reset
+- radeon: Crash in radeon_bo_can_reclaim_slab
+- RV530 renders improperly at non 4:3 resolutions.
+- anv: new cooperative matrix failures with CTS 1.3.8.0
+- \`[gfxhub0] no-retry page fault` triggered by \`AMD_TEST=testdmaperf` on gfx90c APU
+
+
+Changes
+-------
+
+Boris Brezillon (1):
+
+- panvk: Disable global offset on varying and non-VS attribute descriptors
+
+Caio Oliveira (2):
+
+- intel/brw: Use helper to create accumulator register
+- intel/brw: Fix validation of accumulator register
+
+Charlie Turner (1):
+
+- {vulkan,radv,anv}/video: fix issue in H264 scaling lists derivation
+
+Corentin Noël (2):
+
+- st_pbo/compute: Use the correct structure type when allocating a specialized key
+- zink: Make sure to initialize all the fields of VkMemoryBarrier
+
+Dave Airlie (1):
+
+- radv/video: fix h265 decode with unaligned w/h
+
+David Rosca (1):
+
+- radv/video: Set maxActiveReferencePictures to 16 for H264/5
+
+Eric Engestrom (5):
+
+- docs: add sha256sum for 24.0.3
+- .pick_status.json: Update to 9b6d6c1d2d0c8a517e974abbf7b75a47a607f6ec
+- .pick_status.json: Update to eac703f69128d5aa6879c9becbad627ce08a7920
+- .pick_status.json: Update to 912e203a534be8b70b3ef8bf00294e9c962e385a
+- .pick_status.json: Update to c0875d21563257442fd91aab5740248b0fd96a5c
+
+Faith Ekstrand (2):
+
+- nir/builder: Correctly handle decl_reg or undef as the first instruction
+- nir/gather_types: Support unstructured control-flow
+
+Francisco Jerez (1):
+
+- intel/eu/xe2+: Translate brw_reg fields in REG_SIZE units to physical 512b GRF units during codegen.
+
+Friedrich Vock (2):
+
+- radv: Only enable SEs that the device reports
+- radeonsi: Only enable SEs that the device reports
+
+Gert Wollny (2):
+
+- nir-to-spirv: Cast SSBO input pointer when needed
+- nir_to_spirv: Allow LOD for external images
+
+Hyunjun Ko (1):
+
+- anv/video: fix scan order for scaling lists on H265 decoding.
+
+Iván Briano (2):
+
+- compiler/types: fix serialization of cooperative matrix
+- intel/cmat: fix stride calculation in cmat load/store
+
+Jordan Justen (1):
+
+- intel/compiler/fs: Restore SIMD32 restriction for ray_queries on Xe2
+
+Karol Herbst (2):
+
+- rusticl/kernel: assign sampler locations before DCEing variables
+- nouveau: call glsl_type_singleton_init_or_ref earlier
+
+Kenneth Graunke (1):
+
+- intel/brw: Fix opt_split_sends() to allow for FIXED_GRF send sources
+
+Konstantin Seurer (1):
+
+- zink: Handle aoa derefs of images
+
+Lionel Landwerlin (6):
+
+- intel/fs: fixup sampler header message
+- anv: return unsupported for FSR images on Gfx12.0
+- anv: ignore descriptor alignment for inline uniforms
+- blorp: handle a few allocation failure cases
+- anv: fix block pool allocation failure
+- anv: fix bitfield checks in gfx runtime flushing
+
+Lucas Stach (1):
+
+- etnaviv: fix fixpoint conversion of negative values
+
+Marek Olšák (8):
+
+- amd/registers: add correct gfx11.x enums for BINNING_MODE
+- radeonsi: disable binning correctly on gfx11.5
+- radeonsi/gfx11: fix programming of PA_SC_BINNER_CNTL_1.MAX_ALLOC_COUNT
+- radeonsi/gfx10.3: add a GPU hang workaround for legacy tess+GS
+- radeonsi/gfx11: add missing DCC_RD_POLICY setting
+- ac/llvm: fix SSBO bounds checking by using raw instead of struct opcodes
+- radeonsi: fix the DMA compute shader
+- r300: port scanout pitch alignment from the DDX to fix DRI3
+
+Mary Guillemard (1):
+
+- nvk: Always copy conditional rendering value before compare
+
+Matthew Waters (1):
+
+- teximage: allow glCopyTex{Sub}Image[123]D into R/RG textures with OpenGL ES 2.0
+
+Mike Blumenkrantz (13):
+
+- zink: destroy batch states after copy context
+- mesa: force rendertarget usage on required-renderable formats
+- zink: try getting sparse page size again without storage bit on fail
+- zink: set the sparse format usage flags directly based on queried props
+- zink: rename optimal_key in update_gfx_program_optimal()
+- zink: use the sanitized key in update_gfx_program_optimal()
+- zink: always sync and replace separable progs even with ZINK_DEBUG=noopt
+- zink: add even more strict checks for separate shader usage
+- glx: only print zink failure-to-load messages if explicitly requested
+- zink: iterate all the modes when doing separate shader fixups
+- zink: do io fixup on patch variables too
+- zink: defer present barrier to flush if a clear is pending
+- zink: clamp swapchain renderarea instead of asserting
+
+Patrick Lerda (1):
+
+- ac/llvm,radeonsi: fix memory leaks triggered by ac_nir_translate() errors
+
+Paulo Zanoni (1):
+
+- anv: don't leak device->vma_samplers
+
+Philipp Zabel (1):
+
+- rusticl: work around reference-to-mutable-static warnings
+
+Pierre-Eric Pelloux-Prayer (2):
+
+- winsys/radeon: pass priv instead NULL to radeon_bo_can_reclaim
+- radeonsi: preserve alpha if needed in kill_ps_outputs_cb
+
+Rhys Perry (4):
+
+- aco: don't reuse misaligned attribute destination VGPRs in VS prologs
+- radv: use dual_color_blend_by_location with Half-Life Alyx
+- aco/cssa: reset equal_anc_out if merging fails
+- aco/gfx11: fix scratch ST mode assembly
+
+Ruijing Dong (3):
+
+- radeonsi/vcn: add enc surface alignment caps
+- frontends/va: add surface alignment attribute
+- radeonsi/vcn: update to use correct padding size.
+
+Samuel Pitoiset (7):
+
+- ac/nir: fix exporting NGG streamout outputs with implicit PrimId from VS/TES
+- radv: disable binning correctly on GFX11.5
+- radv: fix programming of PA_SC_BINNER_CNTL_1.MAX_ALLOC_COUNT on GFX11
+- radv: fix occlusion queries with MSAA and no attachments
+- radv: add radv_force_pstate_peak_gfx11_dgpu and enable it for Helldivers 2
+- radv: add a workaround for null IBO on GFX6
+- radv: invalidate L2 metadata for VK_ACCESS_2_MEMORY_READ_BIT
+
+Yusuf Khan (1):
+
+- nvk: fix valve segfault from setting a descriptor set from NULL
diff --git a/docs/relnotes/24.0.5.rst b/docs/relnotes/24.0.5.rst
new file mode 100644
index 00000000000..2670930e574
--- /dev/null
+++ b/docs/relnotes/24.0.5.rst
@@ -0,0 +1,212 @@
+Mesa 24.0.5 Release Notes / 2024-04-10
+======================================
+
+Mesa 24.0.5 is a bug fix release which fixes bugs found since the 24.0.4 release.
+
+Mesa 24.0.5 implements the OpenGL 4.6 API, but the version reported by
+glGetString(GL_VERSION) or glGetIntegerv(GL_MAJOR_VERSION) /
+glGetIntegerv(GL_MINOR_VERSION) depends on the particular driver being used.
+Some drivers don't support all the features required in OpenGL 4.6. OpenGL
+4.6 is **only** available if requested at context creation.
+Compatibility contexts may report a lower version depending on each driver.
+
+Mesa 24.0.5 implements the Vulkan 1.3 API, but the version reported by
+the apiVersion property of the VkPhysicalDeviceProperties struct
+depends on the particular driver being used.
+
+SHA256 checksum
+---------------
+
+::
+
+    38cc245ca8faa3c69da6d2687f8906377001f63365348a62cc6f7fafb1e8c018  mesa-24.0.5.tar.xz
+
+
+New features
+------------
+
+- None
+
+
+Bug fixes
+---------
+
+- anv: vkd3d-proton test_stress_suballocation failure
+- d3d12: Zwift renders with bad textures/lighting
+- NVK: Misrendering with Civilization 6
+- radv: RDR2 might need zerovram
+- Issues rendering gtk4 window decorations on v3d on Fedora-40/mesa-24.0
+- clc: Failure when linking with llvm+clang 18.1 (-Dshared-llvm=disabled)
+- LLVM-18 build issue
+
+
+Changes
+-------
+
+Axel Davy (5):
+
+- frontend/nine: Fix ff ps key
+- frontend/nine: Fix programmable vs check
+- frontend/nine: Fix missing light flag check
+- frontend/nine: Fix destruction race
+- frontend/nine: Reset should EndScene
+
+Connor Abbott (2):
+
+- freedreno/a7xx: Add CP_CCHE_INVALIDATE
+- tu: Implement CCHE invalidation
+
+Dave Airlie (1):
+
+- mesa: reorder st context teardown
+
+David Heidelberg (7):
+
+- r300: add missing licence to the r300_public.h
+- r300: add missing copyright header
+- docs: we support EGL 1.5 for a long time
+- ci/amd: drop old PIGLIT_REPLAY_DESCRIPTION_FILE surpassed by PIGLIT_TRACES_FILE
+- r600: add license header to r600_formats.h
+- r600: add license info to the r600_opcodes.h
+- r600: add license information to the sfn_shader_gs.h
+
+David Stern (1):
+
+- vulkan/wsi/x11: Explicitly discard errors from xcb_present_pixmap.
+
+Eric Engestrom (5):
+
+- docs: add sha256sum for 24.0.4
+- .pick_status.json: Update to 3d68dd78d07b30cefe90d76af681075f4ed6b33d
+- .pick_status.json: Update to fcb568a5d5a52db75fa2f6d04579bb404ca7f597
+- .pick_status.json: Update to 078fe5454e97d073feb18bcdcf7ed1874e8b4835
+- .pick_status.json: Update to 2c1cb65949933a05eedb2eacc15cd893ecaef8aa
+
+Eric R. Smith (2):
+
+- panfrost: mark indirect compute buffer as read
+- gallium: handle copy_image of depth textures
+
+Faith Ekstrand (2):
+
+- nvk: Add a _pad field to nvk_cbuf
+- nvk: Add a _pad field to nvk_fs_key
+
+Georg Lehmann (2):
+
+- aco: don't combine mul+add_clamp to mad_clamp
+- aco/ra: use SDWA for 16bit instructions when the second byte is blocked
+
+Iago Toral Quiroga (2):
+
+- v3d: implement fix for GFXH-1602
+- broadcom/compiler: fix workaround for GFXH-1602
+
+Ian Romanick (3):
+
+- intel/brw: Clear write_accumulator flag when changing the destination
+- intel/brw: Use enums for DPAS source regioning
+- nir: intel/brw: Change the order of sources for nir_dpas_intel
+
+Jesse Natalie (1):
+
+- glsl: Use a stable attr sort for VS in / FS out
+
+Jordan Justen (1):
+
+- intel/dev: Add 0x56be and 0x56bf DG2 PCI IDs
+
+José Roberto de Souza (4):
+
+- anv: Fix calculation of syncs required in Xe KMD
+- iris: Wait for drm_xe_exec_queue to be idle before destroying it
+- anv: Create protected engine context when i915 supports vm control
+- intel: Enable Xe KMD support by default
+
+Juston Li (1):
+
+- Revert "zink: store last pipeline directly for zink_gfx_program::last_pipeline"
+
+Karol Herbst (1):
+
+- meson: fix link failure with llvm-18
+
+Kenneth Graunke (2):
+
+- intel/brw: Fix generate_mov_indirect to check has_64bit_int not float
+- intel/brw: Fix lower_regioning for BROADCAST, MOV_INDIRECT on Q types
+
+Konstantin Seurer (1):
+
+- nir/serialize: Encode data for temporaries
+
+Lionel Landwerlin (7):
+
+- anv: fix protected memory allocations
+- anv: disable protected content around surface state copies
+- anv: disable generated draws in protected command buffers
+- anv: update protection fault property
+- anv: add missing data flush out of L3 for transform feedback writes
+- anv: mark descriptors & pipeline dirty after blorp compute
+- isl: set NullPageCoherencyEnable for depth/stencil sparse surfaces
+
+Lucas Stach (2):
+
+- etnaviv: fix depth writes without testing
+- etnaviv: rs: take src dimensions into account when increasing height alignment
+
+Mike Blumenkrantz (12):
+
+- zink: only check that CUBE_COMPATIBLE for images doesn't subtract flags
+- zink: don't use set_foreach_remove with dmabuf_exports
+- zink: make descriptor pool creation more robust
+- zink: fix shaderdb pipeline compile
+- zink: don't clobber indirect array reads with missing components
+- zink: fix add_derefs case for compact arrays
+- llvmpipe: fix DRAW_USE_LLVM=0
+- glsl: handle xfb resources for spirv before running varying opts
+- mesa: clamp binary pointer in ShaderBinary if length==0
+- glsl: set PSIZ bit in outputs_written when injecting a 1.0 psiz write
+- nir/lower_clamp_color_outputs: fix use with lowered io
+- nir/texcoord_replace: fix scalarized io handling
+
+Nikita Popov (1):
+
+- Pass no-verify-fixpoint option to instcombine in LLVM 18
+
+Patrick Lerda (1):
+
+- r300: fix constants_remap_table memory leak related to the dummy shader path
+
+Paul Gofman (3):
+
+- glsl: allow out arrays in #110 with allow_glsl_120_subset_in_110
+- driconf: add a workaround for Joe Danger 2
+- driconf: add a workaround for Joe Danger
+
+Paulo Zanoni (2):
+
+- anv/xe: don't leak xe_syncs during trtt submission
+- anv, iris: add missing CS_STALL bit for GPGPU texture invalidation
+
+Samuel Pitoiset (3):
+
+- radv: fix conditional rendering with mesh+task and multiview (again)
+- radv: enable radv_zero_vram for Red Dead Redemption 2
+- radv: make sure the heap budget is less than or equal to the heap size
+
+Tapani Pälli (1):
+
+- anv: disable fcv optimization on >= gfx125
+
+Yonggang Luo (1):
+
+- util: Fixes futex_wait on win32
+
+Zack Rusin (1):
+
+- svga: Fix instanced draw detection
+
+Zan Dobersek (1):
+
+- tu: fix memory leaks in tu_shader
diff --git a/docs/relnotes/24.0.6.rst b/docs/relnotes/24.0.6.rst
new file mode 100644
index 00000000000..3f2aec76c08
--- /dev/null
+++ b/docs/relnotes/24.0.6.rst
@@ -0,0 +1,159 @@
+Mesa 24.0.6 Release Notes / 2024-04-24
+======================================
+
+Mesa 24.0.6 is a bug fix release which fixes bugs found since the 24.0.5 release.
+
+Mesa 24.0.6 implements the OpenGL 4.6 API, but the version reported by
+glGetString(GL_VERSION) or glGetIntegerv(GL_MAJOR_VERSION) /
+glGetIntegerv(GL_MINOR_VERSION) depends on the particular driver being used.
+Some drivers don't support all the features required in OpenGL 4.6. OpenGL
+4.6 is **only** available if requested at context creation.
+Compatibility contexts may report a lower version depending on each driver.
+
+Mesa 24.0.6 implements the Vulkan 1.3 API, but the version reported by
+the apiVersion property of the VkPhysicalDeviceProperties struct
+depends on the particular driver being used.
+
+SHA256 checksum
+---------------
+
+::
+
+    8b7a92dbe6468c18f2383700135b5fe9de836cdf0cc8fd7dbae3c7110237d604  mesa-24.0.6.tar.xz
+
+
+New features
+------------
+
+- None
+
+
+Bug fixes
+---------
+
+- radv: mesa-9999/src/amd/vulkan/radv_image_view.c:147: radv_set_mutable_tex_desc_fields: Assertion \`(plane->surface.u.gfx9.surf_pitch * plane->surface.bpe) % 256 == 0' failed.
+- r600: Valheim hangs CAYMAN gpu (regression/bisected)
+- r600: Artifacts in Oxygen Not Included around air ducts and pipes (regression, bisected)
+- RADV, regression : Objects randomly appear/disappear on Unreal Engine 4 titles using D3D12 backend on Polaris
+- mesa 23.1.0-rc3 flickering textures/lighting in Unreal 4 games Polaris10
+- anv: flaky vkd3d-proton test_buffer_feedback_instructions_sm51
+
+
+Changes
+-------
+
+Bas Nieuwenhuizen (1):
+
+- radv: Fix differing aspect masks for multiplane image copies.
+
+Boris Brezillon (1):
+
+- nir/lower_blend: Fix nir_blend_logicop() for 8/16-bit integer formats
+
+Dave Airlie (1):
+
+- egl/dri2: don't bind dri2 for zink
+
+Eric Engestrom (5):
+
+- docs: add sha256sum for 24.0.5
+- .pick_status.json: Update to 2bb102f020b3a5834d219ab474c6bcdd02f88d09
+- .pick_status.json: Update to 7a1779edc7fb82c891e584074b95d1a4801c1782
+- .pick_status.json: Mark 3c673919c348b0611595b32fcc8a3d376868c830 as denominated
+- .pick_status.json: Update to cd5c9870ea1d7e73d05f125b229f34e5749c8345
+
+Eric R. Smith (3):
+
+- panfrost: fix a GPU/CPU synchronization problem
+- panfrost: mark separate_stencil as valid when surface is valid
+- panfrost: fix an incorrect stencil clear optimization
+
+Georg Lehmann (1):
+
+- aco: use v1 definition for v_interp_p1lv_f16
+
+Gert Wollny (4):
+
+- r600/sfn: Add array element parent also to array
+- r600/sfn: Use dependecies to order barriers and LDS/RAT instructions
+- r600/sfn: when emitting fp64 op2 groups pre-load values
+- r600/sfn: Don't put b2f64 conversion into ALU group
+
+Iago Toral Quiroga (1):
+
+- broadcom/compiler: enable perquad with uses_wide_subgroup_intrinsics
+
+Ian Romanick (1):
+
+- intel/brw: Fix handling of cmat_signed_mask
+
+Jonathan Gray (3):
+
+- intel/dev: update DG2 device names
+- intel/dev: update DG2 device names
+- intel/dev: 0x7d45 is mtl-u not mtl-h
+
+Jose Maria Casanova Crespo (1):
+
+- broadcom/compiler: needs_quad_helper_invocation enable PER_QUAD TMU access
+
+Karol Herbst (1):
+
+- rusticl/program: handle -cl-no-subgroup-ifp
+
+Konstantin Seurer (1):
+
+- lavapipe: Handle multiple planes in GetDescriptorEXT
+
+M Henning (1):
+
+- nvk: Don't use a descriptor cbuf if it's too large
+
+Mike Blumenkrantz (13):
+
+- lavapipe: don't clamp index buffer size for null index buffer draws
+- zink: block LA formats with srgb
+- llvmpipe: clamp 32bit query results to low 32 bits rather than MIN
+- lavapipe: clamp 32bit query results to low 32 bits rather than MIN
+- nir/remove_unused_io_vars: check all components to determine variable liveness
+- lavapipe: disable stencil test if no stencil attachment
+- egl: fix defines for zink's dri3 check
+- egl/android: fix zink loading
+- zink: disable buffer reordering correctly on shader image binds
+- zink: destroy shaderdb pipelines
+- zink: add VK_PIPELINE_CREATE_CAPTURE_STATISTICS_BIT_KHR for shaderdb
+- brw/lower_a2c: fix for scalarized fs outputs
+- zink: copy shader name when copying shader info
+
+Patrick Lerda (2):
+
+- r300: fix r300_draw_elements() behavior
+- panfrost: remove panfrost_create_shader_state() related dead code
+
+Paulo Zanoni (1):
+
+- anv/sparse: replace device->using_sparse with device->num_sparse_resources
+
+Sagar Ghuge (3):
+
+- anv: Fix typo in DestinationAlphaBlendFactor value
+- anv: Use appropriate argument format for indirect draw
+- isl: Update isl_swizzle_supports_rendering comment
+
+Samuel Pitoiset (3):
+
+- radv: add missing SQTT markers when an indirect indexed draw is used with DGC
+- radv: use canonicalized VA for VM fault reports
+- radv: fix waiting for occlusion queries on GFX6-8
+
+Stéphane Cerveau (1):
+
+- vulkan/video: hevc: b-frames can be reference or not
+
+Yonggang Luo (1):
+
+- compiler/spirv: vtn_add_printf_string support for handling OpBitcast
+
+nyanmisaka (1):
+
+- radeonsi/uvd_enc: update to use correct padding size
diff --git a/docs/relnotes/24.0.7.rst b/docs/relnotes/24.0.7.rst
new file mode 100644
index 00000000000..0eaecdec76f
--- /dev/null
+++ b/docs/relnotes/24.0.7.rst
@@ -0,0 +1,155 @@
+Mesa 24.0.7 Release Notes / 2024-05-08
+======================================
+
+Mesa 24.0.7 is a bug fix release which fixes bugs found since the 24.0.6 release.
+
+Mesa 24.0.7 implements the OpenGL 4.6 API, but the version reported by
+glGetString(GL_VERSION) or glGetIntegerv(GL_MAJOR_VERSION) /
+glGetIntegerv(GL_MINOR_VERSION) depends on the particular driver being used.
+Some drivers don't support all the features required in OpenGL 4.6. OpenGL
+4.6 is **only** available if requested at context creation.
+Compatibility contexts may report a lower version depending on each driver.
+
+Mesa 24.0.7 implements the Vulkan 1.3 API, but the version reported by
+the apiVersion property of the VkPhysicalDeviceProperties struct
+depends on the particular driver being used.
+
+SHA256 checksum
+---------------
+
+::
+
+    7454425f1ed4a6f1b5b107e1672b30c88b22ea0efea000ae2c7d96db93f6c26a  mesa-24.0.7.tar.xz
+
+
+New features
+------------
+
+- None
+
+
+Bug fixes
+---------
+
+- mesa 24 intel A770 KOTOR black shadow smoke scenes
+- Graphical glitches in RPCS3 after updating Vulkan Intel drivers
+- [R600] OpenGL and VDPAU regression in Mesa 23.3.0 - some bitmaps get distorted.
+- VAAPI radeonsi: VBAQ broken with HEVC
+- radv: vkCmdWaitEvents2 is broken
+- Zink: enabled extensions and features may not match
+
+
+Changes
+-------
+
+Boris Brezillon (3):
+
+- panfrost: do not write outside num_wg_sysval
+- panfrost: Add the BO containing fragment program descriptor to the batch
+- pan/kmod: Make default allocator thread-safe
+
+Constantine Shablia (2):
+
+- pan/bi: fix 1D array tex coord lowering
+- panfrost: report correct MAX_VARYINGS
+
+Daniel Schürmann (1):
+
+- aco/ra: fix kill flags after renaming fixed Operands
+
+David Rosca (5):
+
+- radeonsi/vcn: Allocate session buffer in VRAM
+- radeonsi/vcn: Fix 10bit HEVC VPS general_profile_compatibility_flags
+- radeonsi/vcn: Only enable VBAQ with rate control mode
+- frontends/va: Fix AV1 slice_data_offset with multiple slice data buffers
+- Revert "radeonsi/vcn: AV1 skip the redundant bs resize"
+
+Eric Engestrom (6):
+
+- docs: add sha256sum for 24.0.6
+- .pick_status.json: Update to 86281ef15fca378ef48bcb072a762168e537820d
+- .pick_status.json: Mark 0666a715c7210558017ce717f6b0b947c679a68e as denominated
+- .pick_status.json: Update to 603982ea802b3846e91a943b413a7baf430e875d
+- .pick_status.json: Update to 9666756f603f0285d8a93ef93db1c7ec702b671f
+- .pick_status.json: Update to b8e79d2769b4a4aed7e2103cf0405acc5bdadb86
+
+Erik Faye-Lund (2):
+
+- panfrost: correct first-tracking for signature
+- panvk: avoid dereferencing a null-pointer
+
+Georg Lehmann (1):
+
+- radv, radeonsi: don't use D16 for f2f16_rtz
+
+Gert Wollny (1):
+
+- zink/kopper: Wait for last QueuePresentKHR to finish before acquiring for readback
+
+Ian Romanick (1):
+
+- intel/brw: Fix optimize_extract_to_float for i2f of unsigned extract
+
+Iván Briano (2):
+
+- anv: check requirements for VK_IMAGE_USAGE_FRAGMENT_SHADING_RATE
+- anv: fix casting to graphics_pipeline_base
+
+Karol Herbst (2):
+
+- nir: fix nir_shader_get_function_for_name for functions without names.
+- rusticl: use stream uploader for cb0 if prefered
+
+Kenneth Graunke (1):
+
+- isl: Set MOCS to uncached for Gfx12.0 blitter sources/destinations
+
+Konstantin Seurer (1):
+
+- radv: Handle all dependencies of CmdWaitEvents2
+
+Lionel Landwerlin (2):
+
+- anv: disable dual source blending state if not used in shader
+- intel/brw: fixup wm_prog_data_barycentric_modes()
+
+Mike Blumenkrantz (8):
+
+- zink: reconstruct features pnext after determining extension support
+- glthread: check for invalid primitive modes in DrawElementsBaseVertex
+- zink: prune zink_shader::programs under lock
+- zink: fully wait on all program fences during ctx destroy
+- kopper: fix bufferage/swapinterval handling for non-window swapchains
+- zink: slightly better swapinterval failure handling
+- zink: clean up accidental debug print
+- zink: add a tu flake
+
+Patrick Lerda (1):
+
+- gallium/auxiliary/vl: fix typo which negatively impacts the src_stride initialization
+
+Rohan Garg (1):
+
+- anv: formatting fix when printing pipe controls
+
+Samuel Pitoiset (1):
+
+- radv: fix image format properties with fragment shading rate usage
+
+Sviatoslav Peleshko (1):
+
+- anv: Fix descriptor sampler offsets assignment
+
+Tapani Pälli (1):
+
+- iris: change stream uploader default size to 2MB
+
+Yiwei Zhang (2):
+
+- venus: avoid client allocators for ring internals
+- venus: fix to destroy all pipeline handles on early error paths
+
+Yusuf Khan (1):
+
+- nouveau: Fix crash when destination or source screen fences are null
diff --git a/docs/relnotes/24.0.8.rst b/docs/relnotes/24.0.8.rst
new file mode 100644
index 00000000000..a90b0e7a6e5
--- /dev/null
+++ b/docs/relnotes/24.0.8.rst
@@ -0,0 +1,155 @@
+Mesa 24.0.8 Release Notes / 2024-05-22
+======================================
+
+Mesa 24.0.8 is a bug fix release which fixes bugs found since the 24.0.7 release.
+
+Mesa 24.0.8 implements the OpenGL 4.6 API, but the version reported by
+glGetString(GL_VERSION) or glGetIntegerv(GL_MAJOR_VERSION) /
+glGetIntegerv(GL_MINOR_VERSION) depends on the particular driver being used.
+Some drivers don't support all the features required in OpenGL 4.6. OpenGL
+4.6 is **only** available if requested at context creation.
+Compatibility contexts may report a lower version depending on each driver.
+
+Mesa 24.0.8 implements the Vulkan 1.3 API, but the version reported by
+the apiVersion property of the VkPhysicalDeviceProperties struct
+depends on the particular driver being used.
+
+SHA256 checksum
+---------------
+
+::
+
+    d1ed86a266d5b7b8c136ae587ef5618ed1a9837a43440f3713622bf0123bf5c1  mesa-24.0.8.tar.xz
+
+
+New features
+------------
+
+- None
+
+
+Bug fixes
+---------
+
+- [24.1-rc4] fatal error: intel/dev/intel_wa.h: No such file or directory
+- vcn: rewinding attached video in Totem cause [mmhub] page fault
+- When using amd gpu deinterlace, tv bt709 properties mapping to 2 chroma
+- VCN decoding freezes the whole system
+- [RDNA2 [AV1] [VAAPI] hw decoding glitches in Thorium 123.0.6312.133 after https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/28960
+- WSI: Support VK_IMAGE_ASPECT_MEMORY_PLANE_i_BIT_EXT for DRM Modifiers in Vulkan
+- radv: Enshrouded GPU hang on RX 6800
+- NVK Zink: Wrong color in Unigine Valley benchmark
+- [anv] FINISHME: support YUV colorspace with DRM format modifiers
+- 24.0.6: build fails
+
+
+Changes
+-------
+
+Antoine Coutant (1):
+
+- drisw: fix build without dri3
+
+Bas Nieuwenhuizen (1):
+
+- radv: Use zerovram for Enshrouded.
+
+David Heidelberg (2):
+
+- freedreno/ci: move the disabled jobs from include to the main file
+- winsys/i915: depends on intel_wa.h
+
+David Rosca (6):
+
+- frontends/va: Only increment slice offset after first slice parameters
+- radeonsi: Update buffer for other planes in si_alloc_resource
+- frontends/va: Store slice types for H264 decode
+- radeonsi/vcn: Ensure DPB has as many buffers as references
+- radeonsi/vcn: Allow duplicate buffers in DPB
+- radeonsi/vcn: Ensure at least one reference for H264 P/B frames
+
+Eric Engestrom (5):
+
+- docs: add sha256sum for 24.0.7
+- .pick_status.json: Update to 18c53157318d6c8e572062f6bb768dfb621a55fd
+- .pick_status.json: Update to e154f90aa9e71cc98375866c3ab24c4e08e66cb7
+- .pick_status.json: Mark ae8fbe220ae67ffdce662c26bc4a634d475c0389 as denominated
+- .pick_status.json: Update to a31996ce5a6b7eb3b324b71eb9e9c45173953c50
+
+Faith Ekstrand (6):
+
+- nvk: Re-emit sample locations when rasterization samples changes
+- nvk/meta: Restore set_sizes[0]
+- nouveau/winsys: Take a reference to BOs found in the cache
+- drm-uapi: Sync nouveau_drm.h
+- nouveau/winsys: Add back nouveau_ws_bo_new_tiled()
+- vulkan/wsi: Bind memory planes, not YCbCr planes.
+
+Friedrich Vock (2):
+
+- aco/tests: Insert p_logical_start/end in reduce_temp tests
+- aco/spill: Insert p_start_linear_vgpr right after p_logical_end
+
+Georg Lehmann (1):
+
+- zink: use bitcasts instead of pack/unpack double opcodes
+
+José Expósito (1):
+
+- meson: Update proc_macro2 meson.build patch
+
+Karol Herbst (5):
+
+- rusticl/event: use Weak refs for dependencies
+- Revert "rusticl/event: use Weak refs for dependencies"
+- event: break long dependency chains on drop
+- rusticl/mesa/context: flush context before destruction
+- nir/lower_cl_images: set binding also for samplers
+
+Konstantin Seurer (3):
+
+- radv: Fix radv_shader_arena_block list corruption
+- radv: Remove arenas from capture_replay_arena_vas
+- radv: Zero initialize capture replay group handles
+
+Lionel Landwerlin (3):
+
+- anv: fix ycbcr plane indexing with indirect descriptors
+- anv: fix push constant subgroup_id location
+- nir/divergence: add missing load_printf_buffer_address
+
+Marek Olšák (1):
+
+- util: shift the mask in BITSET_TEST_RANGE_INSIDE_WORD to be relative to b
+
+Mike Blumenkrantz (8):
+
+- egl/x11: disable dri3 with LIBGL_KOPPER_DRI2=1 as expected
+- zink: add a batch ref for committed sparse resources
+- u_blitter: stop leaking saved blitter states on no-op blits
+- frontends/dri: only release pipe when screen init fails
+- frontends/dri: always init opencl_func_mutex in InitScreen hooks
+- zink: clean up semaphore arrays on batch state destroy
+- nir/lower_aaline: fix for scalarized outputs
+- nir/linking: fix nir_assign_io_var_locations for scalarized dual blend
+
+Patrick Lerda (2):
+
+- clover: fix memory leak related to optimize
+- r600: fix vertex state update clover regression
+
+Rhys Perry (1):
+
+- aco/waitcnt: fix DS/VMEM ordered writes when mixed
+
+Romain Naour (1):
+
+- glxext: don't try zink if not enabled in mesa
+
+Yiwei Zhang (5):
+
+- turnip: msm: clean up iova on error path
+- turnip: msm: fix racy gem close for re-imported dma-buf
+- turnip: virtio: fix error path in virtio_bo_init
+- turnip: virtio: fix iova leak upon found already imported dmabuf
+- turnip: virtio: fix racy gem close for re-imported dma-buf
diff --git a/docs/relnotes/24.0.9.rst b/docs/relnotes/24.0.9.rst
new file mode 100644
index 00000000000..fe635e833db
--- /dev/null
+++ b/docs/relnotes/24.0.9.rst
@@ -0,0 +1,155 @@
+Mesa 24.0.9 Release Notes / 2024-06-06
+======================================
+
+Mesa 24.0.9 is a bug fix release which fixes bugs found since the 24.0.8 release.
+
+Mesa 24.0.9 implements the OpenGL 4.6 API, but the version reported by
+glGetString(GL_VERSION) or glGetIntegerv(GL_MAJOR_VERSION) /
+glGetIntegerv(GL_MINOR_VERSION) depends on the particular driver being used.
+Some drivers don't support all the features required in OpenGL 4.6. OpenGL
+4.6 is **only** available if requested at context creation.
+Compatibility contexts may report a lower version depending on each driver.
+
+Mesa 24.0.9 implements the Vulkan 1.3 API, but the version reported by
+the apiVersion property of the VkPhysicalDeviceProperties struct
+depends on the particular driver being used.
+
+SHA256 checksum
+---------------
+
+::
+
+    TBD.
+
+
+New features
+------------
+
+- None
+
+
+Bug fixes
+---------
+
+- RustiCL: deadlock when calling clGetProfilingInfo() on callbacks
+- dEQP-VK.pipeline.pipeline_library.shader_module_identifier.pipeline_from_id.graphics regression
+- anv: unbounded shader cache
+- radv: Crash due to nir validation fail in Enshrouded
+- radv: Crash due to nir validation fail in Enshrouded
+- panforst: T604 issue with using u32 for flat varyings
+
+
+Changes
+-------
+
+Alexandre Marquet (1):
+
+- pan/mdg: quirk to disable auto32
+
+David Heidelberg (2):
+
+- subprojects: uprev perfetto to v45.0
+- ci/nouveau: move disabled jobs back from include into main gitlab-ci.yml
+
+David Rosca (1):
+
+- frontends/va: Fix leak when destroying VAEncCodedBufferType
+
+Eric Engestrom (11):
+
+- docs: add sha256sum for 24.0.8
+- .pick_status.json: Update to 18c736bcfc55b8fa309ede02332b9c7a2ca22e78
+- .pick_status.json: Mark 01bac643f6c088f7537edf18f2d4094881c1ecda as denominated
+- .pick_status.json: Update to 4b6f7613c0bd161548f1bd45d42b65b4841a278a
+- .pick_status.json: Mark eefe34127f8e8ae2ba91a7837b9dfef999dc3f87 as denominated
+- .pick_status.json: Update to a1ea0956b46778d0331e4ef60ebd2be057fd0e9f
+- .pick_status.json: Mark 410ca6a3e99c5c1c9c91f0f79bf43a35103cbd98 as denominated
+- freedreno/a6xx: fix kernel -> compute handling
+- panfrost: mark tests as fixed
+- panfrost/ci: add missing genxml trigger path
+- .pick_status.json: Update to 6f713a764fb412567caaabd9ae574822e79da383
+
+Eric R. Smith (4):
+
+- get_color_read_type: make sure format/type combo is legal for gles
+- glsl: test both inputs when sorting varyings for xfb
+- panfrost: fix some omissions in valhall flow control
+- panfrost: change default rounding mode for samplers
+
+Friedrich Vock (2):
+
+- radv: Use max_se instead of num_se where appropriate
+- radeonsi: Use max_se instead of num_se where appropriate
+
+Iago Toral Quiroga (4):
+
+- broadcom/compiler: make add_node return the node index
+- broadcom/compiler: don't assign payload registers to spilling setup temps
+- broadcom/compiler: apply payload conflict to spill setup before RA
+- v3dv: fix incorrect index buffer size
+
+Iván Briano (1):
+
+- anv: check cmd_buffer is on a transfer queue more properly
+
+Jose Maria Casanova Crespo (8):
+
+- v3d: fix CLE MMU errors avoiding using last bytes of CL BOs.
+- v3dv: fix CLE MMU errors avoiding using last bytes of CL BOs.
+- v3d: Increase alignment to 16k on CL BO on RPi5
+- v3dv: Increase alignment to 16k on CL BO on RPi5
+- v3dv: V3D_CL_MAX_INSTR_SIZE bytes in last CL instruction not needed
+- v3dv: Emit stencil draw clear if needed for GFXH-1461
+- v3dv: really fix CLE MMU errors on 7.1HW Rpi5
+- v3d: really fix CLE MMU errors on 7.1HW Rpi5
+
+Juan A. Suarez Romero (1):
+
+- ci: define SNMP base interface on runner
+
+Karol Herbst (5):
+
+- gallium/vl: stub vl_video_buffer_create_as_resource
+- gallium/vl: remove stubs which are defined in mesa_util
+- meson: centralize galliumvl_stub handling
+- rusticl: link against libgalliumvl_stub
+- rusticl/event: fix deadlock when calling clGetEventProfilingInfo inside callbacks
+
+Kevin Chuang (1):
+
+- anv: Properly fetch partial results in vkGetQueryPoolResults
+
+Lionel Landwerlin (5):
+
+- anv: use weak_ref mode for global pipeline caches
+- anv: fix shader identifier handling
+- intel/brw: ensure find_live_channel don't access arch register without sync
+- anv: fix utrace compute walker timestamp captures
+- anv: fix timestamp copies from secondary buffers
+
+Renato Pereyra (1):
+
+- anv: Attempt to compile all pipelines even after errors
+
+Rhys Perry (3):
+
+- aco: create lcssa phis for continue_or_break loops when necessary
+- aco: create lcssa phis for continue_or_break loops when necessary
+- radv: malloc graphics pipeline stages
+
+Samuel Pitoiset (6):
+
+- radv: allow 3d views with VK_IMAGE_CREATE_2D_VIEW_COMPATIBLE_BIT_EXT
+- radv: set image view descriptors as buffer for non-graphics GPU
+- radv: mark some formats as unsupported on GFX8/CARRIZO
+- radv: only set ALPHA_IS_ON_MSB if the image has DCC on GFX6-9
+- radv: fix setting a custom pitch for CB on GFX10_3+
+- radv: fix flushing DB meta cache on GFX11.5
+
+Tapani Pälli (1):
+
+- anv/android: enable emulated astc for applications
+
+Yusuf Khan (1):
+
+- zink/query: begin time elapsed queries even if we arent in a rp
diff --git a/include/drm-uapi/nouveau_drm.h b/include/drm-uapi/nouveau_drm.h
index 0bade1592f3..dd87f8f3079 100644
--- a/include/drm-uapi/nouveau_drm.h
+++ b/include/drm-uapi/nouveau_drm.h
@@ -54,11 +54,42 @@ extern "C" {
  */
 #define NOUVEAU_GETPARAM_EXEC_PUSH_MAX   17
 
+/*
+ * NOUVEAU_GETPARAM_VRAM_BAR_SIZE - query bar size
+ *
+ * Query the VRAM BAR size.
+ */
+#define NOUVEAU_GETPARAM_VRAM_BAR_SIZE 18
+
+/*
+ * NOUVEAU_GETPARAM_VRAM_USED
+ *
+ * Get remaining VRAM size.
+ */
+#define NOUVEAU_GETPARAM_VRAM_USED 19
+
+/*
+ * NOUVEAU_GETPARAM_HAS_VMA_TILEMODE
+ *
+ * Query whether tile mode and PTE kind are accepted with VM allocs or not.
+ */
+#define NOUVEAU_GETPARAM_HAS_VMA_TILEMODE 20
+
 struct drm_nouveau_getparam {
 	__u64 param;
 	__u64 value;
 };
 
+/*
+ * Those are used to support selecting the main engine used on Kepler.
+ * This goes into drm_nouveau_channel_alloc::tt_ctxdma_handle
+ */
+#define NOUVEAU_FIFO_ENGINE_GR  0x01
+#define NOUVEAU_FIFO_ENGINE_VP  0x02
+#define NOUVEAU_FIFO_ENGINE_PPP 0x04
+#define NOUVEAU_FIFO_ENGINE_BSP 0x08
+#define NOUVEAU_FIFO_ENGINE_CE  0x30
+
 struct drm_nouveau_channel_alloc {
 	__u32     fb_ctxdma_handle;
 	__u32     tt_ctxdma_handle;
@@ -81,6 +112,18 @@ struct drm_nouveau_channel_free {
 	__s32 channel;
 };
 
+struct drm_nouveau_notifierobj_alloc {
+	__u32 channel;
+	__u32 handle;
+	__u32 size;
+	__u32 offset;
+};
+
+struct drm_nouveau_gpuobj_free {
+	__s32 channel;
+	__u32 handle;
+};
+
 #define NOUVEAU_GEM_DOMAIN_CPU       (1 << 0)
 #define NOUVEAU_GEM_DOMAIN_VRAM      (1 << 1)
 #define NOUVEAU_GEM_DOMAIN_GART      (1 << 2)
@@ -238,34 +281,32 @@ struct drm_nouveau_vm_init {
 struct drm_nouveau_vm_bind_op {
 	/**
 	 * @op: the operation type
+	 *
+	 * Supported values:
+	 *
+	 * %DRM_NOUVEAU_VM_BIND_OP_MAP - Map a GEM object to the GPU's VA
+	 * space. Optionally, the &DRM_NOUVEAU_VM_BIND_SPARSE flag can be
+	 * passed to instruct the kernel to create sparse mappings for the
+	 * given range.
+	 *
+	 * %DRM_NOUVEAU_VM_BIND_OP_UNMAP - Unmap an existing mapping in the
+	 * GPU's VA space. If the region the mapping is located in is a
+	 * sparse region, new sparse mappings are created where the unmapped
+	 * (memory backed) mapping was mapped previously. To remove a sparse
+	 * region the &DRM_NOUVEAU_VM_BIND_SPARSE must be set.
 	 */
 	__u32 op;
-/**
- * @DRM_NOUVEAU_VM_BIND_OP_MAP:
- *
- * Map a GEM object to the GPU's VA space. Optionally, the
- * &DRM_NOUVEAU_VM_BIND_SPARSE flag can be passed to instruct the kernel to
- * create sparse mappings for the given range.
- */
 #define DRM_NOUVEAU_VM_BIND_OP_MAP 0x0
-/**
- * @DRM_NOUVEAU_VM_BIND_OP_UNMAP:
- *
- * Unmap an existing mapping in the GPU's VA space. If the region the mapping
- * is located in is a sparse region, new sparse mappings are created where the
- * unmapped (memory backed) mapping was mapped previously. To remove a sparse
- * region the &DRM_NOUVEAU_VM_BIND_SPARSE must be set.
- */
 #define DRM_NOUVEAU_VM_BIND_OP_UNMAP 0x1
 	/**
 	 * @flags: the flags for a &drm_nouveau_vm_bind_op
+	 *
+	 * Supported values:
+	 *
+	 * %DRM_NOUVEAU_VM_BIND_SPARSE - Indicates that an allocated VA
+	 * space region should be sparse.
 	 */
 	__u32 flags;
-/**
- * @DRM_NOUVEAU_VM_BIND_SPARSE:
- *
- * Indicates that an allocated VA space region should be sparse.
- */
 #define DRM_NOUVEAU_VM_BIND_SPARSE (1 << 8)
 	/**
 	 * @handle: the handle of the DRM GEM object to map
@@ -301,17 +342,17 @@ struct drm_nouveau_vm_bind {
 	__u32 op_count;
 	/**
 	 * @flags: the flags for a &drm_nouveau_vm_bind ioctl
+	 *
+	 * Supported values:
+	 *
+	 * %DRM_NOUVEAU_VM_BIND_RUN_ASYNC - Indicates that the given VM_BIND
+	 * operation should be executed asynchronously by the kernel.
+	 *
+	 * If this flag is not supplied the kernel executes the associated
+	 * operations synchronously and doesn't accept any &drm_nouveau_sync
+	 * objects.
 	 */
 	__u32 flags;
-/**
- * @DRM_NOUVEAU_VM_BIND_RUN_ASYNC:
- *
- * Indicates that the given VM_BIND operation should be executed asynchronously
- * by the kernel.
- *
- * If this flag is not supplied the kernel executes the associated operations
- * synchronously and doesn't accept any &drm_nouveau_sync objects.
- */
 #define DRM_NOUVEAU_VM_BIND_RUN_ASYNC 0x1
 	/**
 	 * @wait_count: the number of wait &drm_nouveau_syncs
diff --git a/include/pci_ids/iris_pci_ids.h b/include/pci_ids/iris_pci_ids.h
index 46bbe20f6e8..58ab85b2fb6 100644
--- a/include/pci_ids/iris_pci_ids.h
+++ b/include/pci_ids/iris_pci_ids.h
@@ -187,6 +187,8 @@ CHIPSET(0x46c3, adl_gt2, "ADL GT2", "Intel(R) Graphics")
 CHIPSET(0x46d0, adl_n, "ADL-N", "Intel(R) Graphics")
 CHIPSET(0x46d1, adl_n, "ADL-N", "Intel(R) Graphics")
 CHIPSET(0x46d2, adl_n, "ADL-N", "Intel(R) Graphics")
+CHIPSET(0x46d3, adl_n, "ADL-N", "Intel(R) Graphics")
+CHIPSET(0x46d4, adl_n, "ADL-N", "Intel(R) Graphics")
 
 CHIPSET(0x9a40, tgl_gt2, "TGL GT2", "Intel(R) Xe Graphics")
 CHIPSET(0x9a49, tgl_gt2, "TGL GT2", "Intel(R) Xe Graphics")
@@ -232,8 +234,8 @@ CHIPSET(0x5692, dg2_g10, "DG2", "Intel(R) Arc(tm) A550M Graphics")
 CHIPSET(0x5693, dg2_g11, "DG2", "Intel(R) Arc(tm) A370M Graphics")
 CHIPSET(0x5694, dg2_g11, "DG2", "Intel(R) Arc(tm) A350M Graphics")
 CHIPSET(0x5695, dg2_g11, "DG2", "Intel(R) Graphics")
-CHIPSET(0x5696, dg2_g12, "DG2", "Intel(R) Graphics")
-CHIPSET(0x5697, dg2_g12, "DG2", "Intel(R) Graphics")
+CHIPSET(0x5696, dg2_g12, "DG2", "Intel(R) Arc(tm) A570M Graphics")
+CHIPSET(0x5697, dg2_g12, "DG2", "Intel(R) Arc(tm) A530M Graphics")
 CHIPSET(0x56a0, dg2_g10, "DG2", "Intel(R) Arc(tm) A770 Graphics")
 CHIPSET(0x56a1, dg2_g10, "DG2", "Intel(R) Arc(tm) A750 Graphics")
 CHIPSET(0x56a2, dg2_g10, "DG2", "Intel(R) Arc(tm) A580 Graphics")
@@ -243,17 +245,19 @@ CHIPSET(0x56a5, dg2_g11, "DG2", "Intel(R) Arc(tm) A380 Graphics")
 CHIPSET(0x56a6, dg2_g11, "DG2", "Intel(R) Arc(tm) A310 Graphics")
 CHIPSET(0x56b0, dg2_g11, "DG2", "Intel(R) Arc(tm) Pro A30M Graphics")
 CHIPSET(0x56b1, dg2_g11, "DG2", "Intel(R) Arc(tm) Pro A40/A50 Graphics")
-CHIPSET(0x56b2, dg2_g12, "DG2", "Intel(R) Graphics")
-CHIPSET(0x56b3, dg2_g12, "DG2", "Intel(R) Graphics")
-CHIPSET(0x56ba, dg2_g11, "DG2", "Intel(R) Graphics")
-CHIPSET(0x56bb, dg2_g11, "DG2", "Intel(R) Graphics")
-CHIPSET(0x56bc, dg2_g11, "DG2", "Intel(R) Graphics")
-CHIPSET(0x56bd, dg2_g11, "DG2", "Intel(R) Graphics")
+CHIPSET(0x56b2, dg2_g12, "DG2", "Intel(R) Arc(tm) Pro A60M Graphics")
+CHIPSET(0x56b3, dg2_g12, "DG2", "Intel(R) Arc(tm) Pro A60 Graphics")
+CHIPSET(0x56ba, dg2_g11, "DG2", "Intel(R) Arc(tm) A380E Graphics")
+CHIPSET(0x56bb, dg2_g11, "DG2", "Intel(R) Arc(tm) A310E Graphics")
+CHIPSET(0x56bc, dg2_g11, "DG2", "Intel(R) Arc(tm) A370E Graphics")
+CHIPSET(0x56bd, dg2_g11, "DG2", "Intel(R) Arc(tm) A350E Graphics")
+CHIPSET(0x56be, dg2_g10, "DG2", "Intel(R) Arc(tm) A750E Graphics")
+CHIPSET(0x56bf, dg2_g10, "DG2", "Intel(R) Arc(tm) A580E Graphics")
 CHIPSET(0x56c0, atsm_g10, "ATS-M", "Intel(R) Data Center GPU Flex Series 170 Graphics")
 CHIPSET(0x56c1, atsm_g11, "ATS-M", "Intel(R) Data Center GPU Flex Series 140 Graphics")
 
 CHIPSET(0x7d40, mtl_u, "MTL", "Intel(R) Graphics")
-CHIPSET(0x7d45, mtl_h, "MTL", "Intel(R) Graphics")
+CHIPSET(0x7d45, mtl_u, "MTL", "Intel(R) Graphics")
 CHIPSET(0x7d55, mtl_h, "MTL", "Intel(R) Arc(tm) Graphics")
 CHIPSET(0x7d60, mtl_u, "MTL", "Intel(R) Graphics")
 CHIPSET(0x7dd5, mtl_h, "MTL", "Intel(R) Graphics")
diff --git a/meson.build b/meson.build
index 2f04f423d99..d61f09cbe0f 100644
--- a/meson.build
+++ b/meson.build
@@ -672,7 +672,7 @@ vdpau = get_option('gallium-vdpau') \
   .require(with_platform_x11, error_message : 'VDPAU state tracker requires X11 support.') \
   .require(_vdpau_drivers.contains(true), error_message : 'VDPAU state tracker requires at least one of the following gallium drivers: r600, radeonsi, nouveau, d3d12 (with option gallium-d3d12-video, virgl).') 
 
-dep_vdpau = dependency('vdpau', version : '>= 1.1', required : vdpau)
+dep_vdpau = dependency('vdpau', version : '>= 1.4', required : vdpau)
 if dep_vdpau.found()
   dep_vdpau = dep_vdpau.partial_dependency(compile_args : true)
   pre_args += '-DHAVE_ST_VDPAU'
@@ -882,7 +882,6 @@ if _opencl != 'disabled'
     error('The Clover OpenCL state tracker requires rtti')
   endif
 
-  with_clc = true
   with_gallium_opencl = true
   with_opencl_icd = _opencl == 'icd'
 else
@@ -907,7 +906,7 @@ if with_gallium_rusticl
 endif
 
 dep_clc = null_dep
-if with_clc
+if with_gallium_opencl or with_clc
   dep_clc = dependency('libclc')
 endif
 
@@ -1597,11 +1596,6 @@ if with_any_intel and ['x86', 'x86_64'].contains(host_machine.cpu_family())
   pre_args += '-DSUPPORT_INTEL_INTEGRATED_GPUS'
 endif
 
-if get_option('intel-xe-kmd').enabled()
-  pre_args += '-DINTEL_XE_KMD_SUPPORTED'
-endif
-
-
 if with_gallium_i915 and host_machine.cpu_family().startswith('x86') == false
   error('Intel "i915" Gallium driver requires x86 or x86_64 CPU family')
 endif
@@ -1797,7 +1791,7 @@ if with_clc
   # all-targets is needed to support static linking LLVM build with multiple targets.
   # windowsdriver is needded with LLVM>=15 and frontendhlsl is needed with LLVM>=16,
   # but we don't know what LLVM version we are using yet
-  llvm_optional_modules += ['all-targets', 'windowsdriver', 'frontendhlsl']
+  llvm_optional_modules += ['all-targets', 'windowsdriver', 'frontendhlsl', 'frontenddriver']
 endif
 draw_with_llvm = get_option('draw-use-llvm')
 if draw_with_llvm
@@ -1941,6 +1935,9 @@ if with_clc
     if dep_llvm.version().version_compare('>= 16.0')
       clang_modules += 'clangASTMatchers'
     endif
+    if dep_llvm.version().version_compare('>= 18.0')
+      clang_modules += 'clangAPINotes'
+    endif
 
     dep_clang = []
     foreach m : clang_modules
diff --git a/meson_options.txt b/meson_options.txt
index 1a2b959a3aa..52a5052053b 100644
--- a/meson_options.txt
+++ b/meson_options.txt
@@ -711,11 +711,4 @@ option(
   description : 'Build custom xmlconfig (driconf) support. If disabled, ' +
                 'the default driconf file is hardcoded into Mesa. ' +
                 'Requires expat.'
-)
-
-option (
-  'intel-xe-kmd',
-  type : 'feature',
-  value : 'disabled',
-  description: 'Enable Intel Xe KMD support.'
-)
+)
\ No newline at end of file
diff --git a/src/amd/ci/gitlab-ci.yml b/src/amd/ci/gitlab-ci.yml
index 5ee36998612..bec735a0aa3 100644
--- a/src/amd/ci/gitlab-ci.yml
+++ b/src/amd/ci/gitlab-ci.yml
@@ -46,7 +46,6 @@ radeonsi-stoney-traces:x86_64:
   variables:
     EGL_PLATFORM: surfaceless
     PIGLIT_TRACES_FILE: traces-amd.yml
-    PIGLIT_REPLAY_DESCRIPTION_FILE: "/install/traces-amd.yml"
     PIGLIT_REPLAY_EXTRA_ARGS: --keep-image
 
 radv-raven-vkcts:x86_64:
@@ -163,7 +162,7 @@ radeonsi-raven-va-full:x86_64:
 vkcts-polaris10-valve:
   extends:
     - .vkcts-test-valve
-    - .polaris10-test-valve-mupuf
+    - .polaris10-test-valve-kws
     - .radv-valve-manual-rules
   timeout: 1h 15m
   variables:
diff --git a/src/amd/ci/radv-navi21-aco-flakes.txt b/src/amd/ci/radv-navi21-aco-flakes.txt
index 9e2e41634c1..2ff57de65bf 100644
--- a/src/amd/ci/radv-navi21-aco-flakes.txt
+++ b/src/amd/ci/radv-navi21-aco-flakes.txt
@@ -10,7 +10,4 @@ dEQP-VK.draw.renderpass.multi_draw.mosaic.indexed_mixed.max_draws.stride_extra_1
 dEQP-VK.pipeline.*line_stipple_enable
 dEQP-VK.pipeline.*line_stipple_params
 
-# New CTS flakes in 1.3.6.3
-dEQP-VK.ray_tracing_pipeline.pipeline_library.configurations.(single|multi)threaded_compilation.*_check_(all|capture_replay)_handles
-
 dEQP-VK.query_pool.statistics_query.host_query_reset.geometry_shader_invocations.secondary.32bits_triangle_list_clear_depth
diff --git a/src/amd/ci/radv-navi31-aco-flakes.txt b/src/amd/ci/radv-navi31-aco-flakes.txt
index d1a8efcf746..e69de29bb2d 100644
--- a/src/amd/ci/radv-navi31-aco-flakes.txt
+++ b/src/amd/ci/radv-navi31-aco-flakes.txt
@@ -1,5 +0,0 @@
-# New CTS flakes in 1.3.6.3
-dEQP-VK.ray_tracing_pipeline.pipeline_library.configurations.multithreaded_compilation.*_check_all_handles
-dEQP-VK.ray_tracing_pipeline.pipeline_library.configurations.multithreaded_compilation.*_check_capture_replay_handles
-dEQP-VK.ray_tracing_pipeline.pipeline_library.configurations.singlethreaded_compilation.*_check_all_handles
-dEQP-VK.ray_tracing_pipeline.pipeline_library.configurations.singlethreaded_compilation.*_check_capture_replay_handles
diff --git a/src/amd/common/ac_gpu_info.c b/src/amd/common/ac_gpu_info.c
index 2e12a74f854..e90348f29ba 100644
--- a/src/amd/common/ac_gpu_info.c
+++ b/src/amd/common/ac_gpu_info.c
@@ -1210,6 +1210,11 @@ bool ac_query_gpu_info(int fd, void *dev_p, struct radeon_info *info,
     */
    info->has_pops_missed_overlap_bug = info->family == CHIP_VEGA10 || info->family == CHIP_RAVEN;
 
+   /* GFX6 hw bug when the IBO addr is 0 which causes invalid clamping (underflow).
+    * Setting the IB addr to 2 or higher solves this issue.
+    */
+   info->has_null_index_buffer_clamping_bug = info->gfx_level == GFX6;
+
    /* Drawing from 0-sized index buffers causes hangs on gfx10. */
    info->has_zero_index_buffer_bug = info->gfx_level == GFX10;
 
diff --git a/src/amd/common/ac_gpu_info.h b/src/amd/common/ac_gpu_info.h
index f390e144f69..bc9cd46c941 100644
--- a/src/amd/common/ac_gpu_info.h
+++ b/src/amd/common/ac_gpu_info.h
@@ -96,6 +96,7 @@ struct radeon_info {
    bool has_small_prim_filter_sample_loc_bug;
    bool has_ls_vgpr_init_bug;
    bool has_pops_missed_overlap_bug;
+   bool has_null_index_buffer_clamping_bug;
    bool has_zero_index_buffer_bug;
    bool has_image_load_dcc_bug;
    bool has_two_planes_iterate256_bug;
diff --git a/src/amd/common/ac_nir_lower_ngg.c b/src/amd/common/ac_nir_lower_ngg.c
index 987f5b8fed8..2a9937fe151 100644
--- a/src/amd/common/ac_nir_lower_ngg.c
+++ b/src/amd/common/ac_nir_lower_ngg.c
@@ -71,6 +71,7 @@ typedef struct
    bool early_prim_export;
    bool streamout_enabled;
    bool has_user_edgeflags;
+   bool skip_primitive_id;
    unsigned max_num_waves;
 
    /* LDS params */
@@ -1760,8 +1761,11 @@ ngg_nogs_store_xfb_outputs_to_lds(nir_builder *b, lower_ngg_nogs_state *s)
    nir_def *addr = pervertex_lds_addr(b, tid, s->pervertex_lds_bytes);
 
    u_foreach_bit64(slot, xfb_outputs) {
+      uint64_t outputs_written = b->shader->info.outputs_written;
+      if (s->skip_primitive_id)
+         outputs_written &= ~VARYING_BIT_PRIMITIVE_ID;
       unsigned packed_location =
-         util_bitcount64(b->shader->info.outputs_written & BITFIELD64_MASK(slot));
+         util_bitcount64(outputs_written & BITFIELD64_MASK(slot));
 
       unsigned mask = xfb_mask[slot];
 
@@ -1986,7 +1990,8 @@ ngg_build_streamout_vertex(nir_builder *b, nir_xfb_info *info,
                            unsigned stream, nir_def *so_buffer[4],
                            nir_def *buffer_offsets[4],
                            nir_def *vtx_buffer_idx, nir_def *vtx_lds_addr,
-                           shader_output_types *output_types)
+                           shader_output_types *output_types,
+                           bool skip_primitive_id)
 {
    nir_def *vtx_buffer_offsets[4];
    for (unsigned buffer = 0; buffer < 4; buffer++) {
@@ -2009,8 +2014,12 @@ ngg_build_streamout_vertex(nir_builder *b, nir_xfb_info *info,
             util_bitcount(b->shader->info.outputs_written_16bit &
                           BITFIELD_MASK(out->location - VARYING_SLOT_VAR0_16BIT));
       } else {
+         uint64_t outputs_written = b->shader->info.outputs_written;
+         if (skip_primitive_id)
+            outputs_written &= ~VARYING_BIT_PRIMITIVE_ID;
+
          base =
-            util_bitcount64(b->shader->info.outputs_written &
+            util_bitcount64(outputs_written &
                             BITFIELD64_MASK(out->location));
       }
 
@@ -2099,7 +2108,7 @@ ngg_nogs_build_streamout(nir_builder *b, lower_ngg_nogs_state *s)
             nir_def *vtx_lds_addr = pervertex_lds_addr(b, vtx_lds_idx, vtx_lds_stride);
             ngg_build_streamout_vertex(b, info, 0, so_buffer, buffer_offsets,
                                        nir_iadd_imm(b, vtx_buffer_idx, i),
-                                       vtx_lds_addr, &s->output_types);
+                                       vtx_lds_addr, &s->output_types, s->skip_primitive_id);
          }
          nir_pop_if(b, if_valid_vertex);
       }
@@ -2455,6 +2464,7 @@ ac_nir_lower_ngg_nogs(nir_shader *shader, const ac_nir_lower_ngg_options *option
       .gs_exported_var = gs_exported_var,
       .max_num_waves = DIV_ROUND_UP(options->max_workgroup_size, options->wave_size),
       .has_user_edgeflags = has_user_edgeflags,
+      .skip_primitive_id = streamout_enabled && options->export_primitive_id,
    };
 
    const bool need_prim_id_store_shared =
@@ -3415,7 +3425,7 @@ ngg_gs_build_streamout(nir_builder *b, lower_ngg_gs_state *s)
                                        buffer_offsets,
                                        nir_iadd_imm(b, vtx_buffer_idx, i),
                                        exported_vtx_lds_addr[i],
-                                       &s->output_types);
+                                       &s->output_types, false);
          }
       }
       nir_pop_if(b, if_emit);
diff --git a/src/amd/common/ac_shader_util.c b/src/amd/common/ac_shader_util.c
index 24e53098843..5b062640634 100644
--- a/src/amd/common/ac_shader_util.c
+++ b/src/amd/common/ac_shader_util.c
@@ -1001,7 +1001,7 @@ void ac_get_scratch_tmpring_size(const struct radeon_info *info,
 
    unsigned max_scratch_waves = info->max_scratch_waves;
    if (info->gfx_level >= GFX11)
-      max_scratch_waves /= info->num_se; /* WAVES is per SE */
+      max_scratch_waves /= info->max_se; /* WAVES is per SE */
 
    /* TODO: We could decrease WAVES to make the whole buffer fit into the infinity cache. */
    *tmpring_size = S_0286E8_WAVES(max_scratch_waves) |
diff --git a/src/amd/common/ac_surface.h b/src/amd/common/ac_surface.h
index 11808f79bf1..83338f34134 100644
--- a/src/amd/common/ac_surface.h
+++ b/src/amd/common/ac_surface.h
@@ -76,6 +76,12 @@ enum radeon_micro_mode
 #define RADEON_SURF_NO_TEXTURE            (1ull << 34)
 #define RADEON_SURF_NO_STENCIL_ADJUST     (1ull << 35)
 
+enum radeon_enc_hevc_surface_alignment
+{
+   RADEON_ENC_HEVC_SURFACE_LOG2_WIDTH_ALIGNMENT = 6,
+   RADEON_ENC_HEVC_SURFACE_LOG2_HEIGHT_ALIGNMENT = 4,
+};
+
 struct legacy_surf_level {
    uint32_t offset_256B;   /* divided by 256, the hw can only do 40-bit addresses */
    uint32_t slice_size_dw; /* in dwords; max = 4GB / 4. */
diff --git a/src/amd/compiler/aco_assembler.cpp b/src/amd/compiler/aco_assembler.cpp
index 49508e96137..7cc5178dc34 100644
--- a/src/amd/compiler/aco_assembler.cpp
+++ b/src/amd/compiler/aco_assembler.cpp
@@ -733,10 +733,10 @@ emit_instruction(asm_context& ctx, std::vector<uint32_t>& out, Instruction* inst
       } else if (instr->format != Format::FLAT ||
                  ctx.gfx_level >= GFX10) { /* SADDR is actually used with FLAT on GFX10 */
          /* For GFX10.3 scratch, 0x7F disables both ADDR and SADDR, unlike sgpr_null, which only
-          * disables SADDR.
+          * disables SADDR. On GFX11, this was replaced with SVE.
           */
          if (ctx.gfx_level <= GFX9 ||
-             (instr->format == Format::SCRATCH && instr->operands[0].isUndefined()))
+             (instr->isScratch() && instr->operands[0].isUndefined() && ctx.gfx_level < GFX11))
             encoding |= 0x7F << 16;
          else
             encoding |= reg(ctx, sgpr_null) << 16;
diff --git a/src/amd/compiler/aco_insert_waitcnt.cpp b/src/amd/compiler/aco_insert_waitcnt.cpp
index 0209f44c377..784d992363c 100644
--- a/src/amd/compiler/aco_insert_waitcnt.cpp
+++ b/src/amd/compiler/aco_insert_waitcnt.cpp
@@ -428,18 +428,20 @@ check_instr(wait_ctx& ctx, wait_imm& wait, alu_delay_info& delay, Instruction* i
          if (it == ctx.gpr_map.end())
             continue;
 
+         wait_imm reg_imm = it->second.imm;
+
          /* Vector Memory reads and writes return in the order they were issued */
          uint8_t vmem_type = get_vmem_type(instr);
          if (vmem_type && ((it->second.events & vm_events) == event_vmem) &&
              it->second.vmem_types == vmem_type)
-            continue;
+            reg_imm.vm = wait_imm::unset_counter;
 
          /* LDS reads and writes return in the order they were issued. same for GDS */
          if (instr->isDS() &&
              (it->second.events & lgkm_events) == (instr->ds().gds ? event_gds : event_lds))
-            continue;
+            reg_imm.lgkm = wait_imm::unset_counter;
 
-         wait.combine(it->second.imm);
+         wait.combine(reg_imm);
       }
    }
 }
diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 1fe6918aa51..3203b5ecdee 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -759,7 +759,7 @@ get_alu_src(struct isel_context* ctx, nir_alu_src src, unsigned size = 1)
       vec_instr->definitions[0] = Definition(dst);
       ctx->block->instructions.emplace_back(std::move(vec_instr));
       ctx->allocated_vec.emplace(dst.id(), elems);
-      return vec.type() == RegType::sgpr ? Builder(ctx->program, ctx->block).as_uniform(dst) : dst;
+      return as_uniform ? Builder(ctx->program, ctx->block).as_uniform(dst) : dst;
    }
 }
 
@@ -5490,7 +5490,7 @@ emit_interp_instr(isel_context* ctx, unsigned idx, unsigned component, Temp src,
          Builder::Result interp_p1 =
             bld.vintrp(aco_opcode::v_interp_mov_f32, bld.def(v1), Operand::c32(2u) /* P0 */,
                        bld.m0(prim_mask), idx, component);
-         interp_p1 = bld.vintrp(aco_opcode::v_interp_p1lv_f16, bld.def(v2b), coord1,
+         interp_p1 = bld.vintrp(aco_opcode::v_interp_p1lv_f16, bld.def(v1), coord1,
                                 bld.m0(prim_mask), interp_p1, idx, component);
          bld.vintrp(aco_opcode::v_interp_p2_legacy_f16, Definition(dst), coord2, bld.m0(prim_mask),
                     interp_p1, idx, component);
@@ -10205,6 +10205,29 @@ visit_block(isel_context* ctx, nir_block* block)
       ctx->cf_info.nir_to_aco[block->index] = ctx->block->index;
 }
 
+static bool
+all_uses_inside_loop(nir_def* def, nir_block* block_before_loop, nir_block* block_after_loop)
+{
+   nir_foreach_use_including_if (use, def) {
+      if (nir_src_is_if(use)) {
+         nir_block* branch_block =
+            nir_cf_node_as_block(nir_cf_node_prev(&nir_src_parent_if(use)->cf_node));
+         if (branch_block->index <= block_before_loop->index ||
+             branch_block->index >= block_after_loop->index)
+            return false;
+      } else {
+         nir_instr* instr = nir_src_parent_instr(use);
+         if ((instr->block->index <= block_before_loop->index ||
+              instr->block->index >= block_after_loop->index) &&
+             !(instr->type == nir_instr_type_phi && instr->block == block_after_loop)) {
+            return false;
+         }
+      }
+   }
+
+   return true;
+}
+
 static Operand
 create_continue_phis(isel_context* ctx, unsigned first, unsigned last,
                      aco_ptr<Instruction>& header_phi, Operand* vals)
@@ -10251,6 +10274,65 @@ create_continue_phis(isel_context* ctx, unsigned first, unsigned last,
    return vals[last - first];
 }
 
+Temp
+rename_temp(const std::map<unsigned, unsigned>& renames, Temp tmp)
+{
+   auto it = renames.find(tmp.id());
+   if (it != renames.end())
+      return Temp(it->second, tmp.regClass());
+   return tmp;
+}
+
+static void
+lcssa_workaround(isel_context* ctx, nir_loop* loop)
+{
+   assert(ctx->block->linear_preds.size() == ctx->block->logical_preds.size() + 1);
+
+   nir_block* block_before_loop = nir_cf_node_as_block(nir_cf_node_prev(&loop->cf_node));
+   nir_block* block_after_loop = nir_cf_node_as_block(nir_cf_node_next(&loop->cf_node));
+
+   std::map<unsigned, unsigned> renames;
+   nir_foreach_block_in_cf_node (block, &loop->cf_node) {
+      nir_foreach_instr (instr, block) {
+         nir_def* def = nir_instr_def(instr);
+         if (!def)
+            continue;
+
+         Temp tmp = get_ssa_temp(ctx, def);
+         if (!tmp.is_linear() || all_uses_inside_loop(def, block_before_loop, block_after_loop))
+            continue;
+
+         Temp new_tmp = ctx->program->allocateTmp(tmp.regClass());
+         aco_ptr<Instruction> phi(create_instruction<Pseudo_instruction>(
+            aco_opcode::p_linear_phi, Format::PSEUDO, ctx->block->linear_preds.size(), 1));
+         for (unsigned i = 0; i < ctx->block->logical_preds.size(); i++)
+            phi->operands[i] = Operand(new_tmp);
+         phi->operands.back() = Operand(tmp.regClass());
+         phi->definitions[0] = Definition(tmp);
+         ctx->block->instructions.emplace(ctx->block->instructions.begin(), std::move(phi));
+
+         renames.emplace(tmp.id(), new_tmp.id());
+      }
+   }
+
+   if (renames.empty())
+      return;
+
+   for (unsigned i = ctx->block->index - 1;
+        ctx->program->blocks[i].loop_nest_depth > ctx->block->loop_nest_depth; i--) {
+      for (aco_ptr<Instruction>& instr : ctx->program->blocks[i].instructions) {
+         for (Definition& def : instr->definitions) {
+            if (def.isTemp())
+               def.setTemp(rename_temp(renames, def.getTemp()));
+         }
+         for (Operand& op : instr->operands) {
+            if (op.isTemp())
+               op.setTemp(rename_temp(renames, op.getTemp()));
+         }
+      }
+   }
+}
+
 static void begin_uniform_if_then(isel_context* ctx, if_context* ic, Temp cond);
 static void begin_uniform_if_else(isel_context* ctx, if_context* ic);
 static void end_uniform_if(isel_context* ctx, if_context* ic);
@@ -10317,6 +10399,10 @@ visit_loop(isel_context* ctx, nir_loop* loop)
    }
 
    end_loop(ctx, &lc);
+
+   /* Create extra LCSSA phis for continue_or_break */
+   if (ctx->block->linear_preds.size() > ctx->block->logical_preds.size())
+      lcssa_workaround(ctx, loop);
 }
 
 static void
@@ -12661,6 +12747,20 @@ select_rt_prolog(Program* program, ac_shader_config* config,
    program->config->num_sgprs = get_sgpr_alloc(program, num_sgprs);
 }
 
+PhysReg
+get_next_vgpr(unsigned size, unsigned* num, int *offset = NULL)
+{
+   unsigned reg = *num + (offset ? *offset : 0);
+   if (reg + size >= *num) {
+      *num = reg + size;
+      if (offset)
+         *offset = 0;
+   } else if (offset) {
+      *offset += size;
+   }
+   return PhysReg(256 + reg);
+}
+
 void
 select_vs_prolog(Program* program, const struct aco_vs_prolog_info* pinfo, ac_shader_config* config,
                  const struct aco_compiler_options* options, const struct aco_shader_info* info,
@@ -12702,13 +12802,30 @@ select_vs_prolog(Program* program, const struct aco_vs_prolog_info* pinfo, ac_sh
    Operand start_instance = get_arg_fixed(args, args->start_instance);
    Operand instance_id = get_arg_fixed(args, args->instance_id);
 
-   PhysReg attributes_start(256 + args->num_vgprs_used);
-   /* choose vgprs that won't be used for anything else until the last attribute load */
-   PhysReg vertex_index(attributes_start.reg() + pinfo->num_attributes * 4 - 1);
-   PhysReg instance_index(attributes_start.reg() + pinfo->num_attributes * 4 - 2);
-   PhysReg start_instance_vgpr(attributes_start.reg() + pinfo->num_attributes * 4 - 3);
-   PhysReg nontrivial_tmp_vgpr0(attributes_start.reg() + pinfo->num_attributes * 4 - 4);
-   PhysReg nontrivial_tmp_vgpr1(attributes_start.reg() + pinfo->num_attributes * 4);
+   bool needs_instance_index =
+      pinfo->instance_rate_inputs &
+      ~(pinfo->zero_divisors | pinfo->nontrivial_divisors); /* divisor is 1 */
+   bool needs_start_instance = pinfo->instance_rate_inputs & pinfo->zero_divisors;
+   bool needs_vertex_index = ~pinfo->instance_rate_inputs & attrib_mask;
+   bool needs_tmp_vgpr0 = has_nontrivial_divisors;
+   bool needs_tmp_vgpr1 = has_nontrivial_divisors &&
+                          (program->gfx_level <= GFX8 || program->gfx_level >= GFX11);
+
+   int vgpr_offset = pinfo->misaligned_mask & (1u << (pinfo->num_attributes - 1)) ? 0 : -4;
+
+   unsigned num_vgprs = args->num_vgprs_used;
+   PhysReg attributes_start = get_next_vgpr(pinfo->num_attributes * 4, &num_vgprs);
+   PhysReg vertex_index, instance_index, start_instance_vgpr, nontrivial_tmp_vgpr0, nontrivial_tmp_vgpr1;
+   if (needs_vertex_index)
+      vertex_index = get_next_vgpr(1, &num_vgprs, &vgpr_offset);
+   if (needs_instance_index)
+      instance_index = get_next_vgpr(1, &num_vgprs, &vgpr_offset);
+   if (needs_start_instance)
+      start_instance_vgpr = get_next_vgpr(1, &num_vgprs, &vgpr_offset);
+   if (needs_tmp_vgpr0)
+      nontrivial_tmp_vgpr0 = get_next_vgpr(1, &num_vgprs, &vgpr_offset);
+   if (needs_tmp_vgpr1)
+      nontrivial_tmp_vgpr1 = get_next_vgpr(1, &num_vgprs, &vgpr_offset);
 
    bld.sop1(aco_opcode::s_mov_b32, Definition(vertex_buffers, s1),
             get_arg_fixed(args, args->vertex_buffers));
@@ -12720,16 +12837,10 @@ select_vs_prolog(Program* program, const struct aco_vs_prolog_info* pinfo, ac_sh
                Operand::c32((unsigned)options->address32_hi));
    }
 
-   /* calculate vgpr requirements */
-   unsigned num_vgprs = attributes_start.reg() - 256;
-   num_vgprs += pinfo->num_attributes * 4;
-   if (has_nontrivial_divisors && program->gfx_level <= GFX8)
-      num_vgprs++; /* make space for nontrivial_tmp_vgpr1 */
-   unsigned num_sgprs = 0;
-
    const struct ac_vtx_format_info* vtx_info_table =
       ac_get_vtx_format_info_table(GFX8, CHIP_POLARIS10);
 
+   unsigned num_sgprs = 0;
    for (unsigned loc = 0; loc < pinfo->num_attributes;) {
       unsigned num_descs =
          load_vb_descs(bld, desc, Operand(vertex_buffers, s2), loc, pinfo->num_attributes - loc);
@@ -12769,11 +12880,6 @@ select_vs_prolog(Program* program, const struct aco_vs_prolog_info* pinfo, ac_sh
             }
          }
 
-         bool needs_instance_index =
-            pinfo->instance_rate_inputs &
-            ~(pinfo->zero_divisors | pinfo->nontrivial_divisors); /* divisor is 1 */
-         bool needs_start_instance = pinfo->instance_rate_inputs & pinfo->zero_divisors;
-         bool needs_vertex_index = ~pinfo->instance_rate_inputs & attrib_mask;
          if (needs_vertex_index)
             bld.vadd32(Definition(vertex_index, v1), get_arg_fixed(args, args->base_vertex),
                        get_arg_fixed(args, args->vertex_id), false, Operand(s2), true);
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index cd4ceb221ce..1d48fbf4a8a 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -1613,7 +1613,7 @@ static_assert(sizeof(Export_instruction) == sizeof(Instruction) + 4, "Unexpected
 struct Pseudo_instruction : public Instruction {
    PhysReg scratch_sgpr; /* might not be valid if it's not needed */
    bool tmp_in_scc;
-   uint8_t padding;
+   bool needs_scratch_reg; /* if scratch_sgpr/scc can be written, initialized by RA. */
 };
 static_assert(sizeof(Pseudo_instruction) == sizeof(Instruction) + 4, "Unexpected padding");
 
diff --git a/src/amd/compiler/aco_lower_to_cssa.cpp b/src/amd/compiler/aco_lower_to_cssa.cpp
index 3c509ee2f81..f62c2816e09 100644
--- a/src/amd/compiler/aco_lower_to_cssa.cpp
+++ b/src/amd/compiler/aco_lower_to_cssa.cpp
@@ -305,8 +305,11 @@ try_merge_merge_set(cssa_ctx& ctx, Temp dst, merge_set& set_b)
       while (!dom.empty() && !dominates(ctx, dom.back(), current))
          dom.pop_back(); /* not the desired parent, remove */
 
-      if (!dom.empty() && interference(ctx, current, dom.back()))
+      if (!dom.empty() && interference(ctx, current, dom.back())) {
+         for (Temp t : union_set)
+            ctx.merge_node_table[t.id()].equal_anc_out = Temp();
          return false; /* intersection detected */
+      }
 
       dom.emplace_back(current); /* otherwise, keep checking */
       if (current != dst)
diff --git a/src/amd/compiler/aco_lower_to_hw_instr.cpp b/src/amd/compiler/aco_lower_to_hw_instr.cpp
index 27787c7322f..03282ea2857 100644
--- a/src/amd/compiler/aco_lower_to_hw_instr.cpp
+++ b/src/amd/compiler/aco_lower_to_hw_instr.cpp
@@ -1767,6 +1767,9 @@ try_coalesce_copies(lower_context* ctx, std::map<PhysReg, copy_operation>& copy_
        copy.op.isConstant() != other->second.op.isConstant())
       return;
 
+   if (other->second.def.regClass().is_linear_vgpr() != copy.def.regClass().is_linear_vgpr())
+      return;
+
    /* don't create 64-bit copies before GFX10 */
    if (copy.bytes >= 4 && copy.def.regClass().type() == RegType::vgpr &&
        ctx->program->gfx_level < GFX10)
diff --git a/src/amd/compiler/aco_optimizer.cpp b/src/amd/compiler/aco_optimizer.cpp
index 1ca0fd95dd1..2fcbf4a5406 100644
--- a/src/amd/compiler/aco_optimizer.cpp
+++ b/src/amd/compiler/aco_optimizer.cpp
@@ -3851,6 +3851,8 @@ combine_vop3p(opt_ctx& ctx, aco_ptr<Instruction>& instr)
       bool fadd = instr->opcode == aco_opcode::v_pk_add_f16;
       if (fadd && instr->definitions[0].isPrecise())
          return;
+      if (!fadd && instr->valu().clamp)
+         return;
 
       Instruction* mul_instr = nullptr;
       unsigned add_op_idx = 0;
@@ -4506,20 +4508,20 @@ combine_instruction(opt_ctx& ctx, aco_ptr<Instruction>& instr)
       }
    } else if (instr->opcode == aco_opcode::v_not_b32 && ctx.program->gfx_level >= GFX10) {
       combine_not_xor(ctx, instr);
-   } else if (instr->opcode == aco_opcode::v_add_u16) {
+   } else if (instr->opcode == aco_opcode::v_add_u16 && !instr->valu().clamp) {
       combine_three_valu_op(
          ctx, instr, aco_opcode::v_mul_lo_u16,
          ctx.program->gfx_level == GFX8 ? aco_opcode::v_mad_legacy_u16 : aco_opcode::v_mad_u16,
          "120", 1 | 2);
-   } else if (instr->opcode == aco_opcode::v_add_u16_e64) {
+   } else if (instr->opcode == aco_opcode::v_add_u16_e64 && !instr->valu().clamp) {
       combine_three_valu_op(ctx, instr, aco_opcode::v_mul_lo_u16_e64, aco_opcode::v_mad_u16, "120",
                             1 | 2);
-   } else if (instr->opcode == aco_opcode::v_add_u32) {
+   } else if (instr->opcode == aco_opcode::v_add_u32 && !instr->usesModifiers()) {
       if (combine_add_sub_b2i(ctx, instr, aco_opcode::v_addc_co_u32, 1 | 2)) {
       } else if (combine_add_bcnt(ctx, instr)) {
       } else if (combine_three_valu_op(ctx, instr, aco_opcode::v_mul_u32_u24,
                                        aco_opcode::v_mad_u32_u24, "120", 1 | 2)) {
-      } else if (ctx.program->gfx_level >= GFX9 && !instr->usesModifiers()) {
+      } else if (ctx.program->gfx_level >= GFX9) {
          if (combine_three_valu_op(ctx, instr, aco_opcode::s_xor_b32, aco_opcode::v_xad_u32, "120",
                                    1 | 2)) {
          } else if (combine_three_valu_op(ctx, instr, aco_opcode::v_xor_b32, aco_opcode::v_xad_u32,
@@ -4533,8 +4535,9 @@ combine_instruction(opt_ctx& ctx, aco_ptr<Instruction>& instr)
          } else if (combine_add_or_then_and_lshl(ctx, instr)) {
          }
       }
-   } else if (instr->opcode == aco_opcode::v_add_co_u32 ||
-              instr->opcode == aco_opcode::v_add_co_u32_e64) {
+   } else if ((instr->opcode == aco_opcode::v_add_co_u32 ||
+               instr->opcode == aco_opcode::v_add_co_u32_e64) &&
+              !instr->usesModifiers()) {
       bool carry_out = ctx.uses[instr->definitions[1].tempId()] > 0;
       if (combine_add_sub_b2i(ctx, instr, aco_opcode::v_addc_co_u32, 1 | 2)) {
       } else if (!carry_out && combine_add_bcnt(ctx, instr)) {
diff --git a/src/amd/compiler/aco_optimizer_postRA.cpp b/src/amd/compiler/aco_optimizer_postRA.cpp
index 5978e7c4b46..93fc261a259 100644
--- a/src/amd/compiler/aco_optimizer_postRA.cpp
+++ b/src/amd/compiler/aco_optimizer_postRA.cpp
@@ -57,8 +57,8 @@ Idx const_or_undef{UINT32_MAX, 2};
 /** Indicates that a register was overwritten by different instructions in previous blocks. */
 Idx overwritten_untrackable{UINT32_MAX, 3};
 
-/** Indicates that a register was written by subdword operations. */
-Idx overwritten_subdword{UINT32_MAX, 4};
+/** Indicates that there isn't a clear single writer, for example due to subdword operations. */
+Idx overwritten_unknown_instr{UINT32_MAX, 4};
 
 struct pr_opt_ctx {
    using Idx_array = std::array<Idx, max_reg_cnt>;
@@ -150,13 +150,19 @@ save_reg_writes(pr_opt_ctx& ctx, aco_ptr<Instruction>& instr)
       Idx idx{ctx.current_block->index, ctx.current_instr_idx};
 
       if (def.regClass().is_subdword())
-         idx = overwritten_subdword;
+         idx = overwritten_unknown_instr;
 
       assert((r + dw_size) <= max_reg_cnt);
       assert(def.size() == dw_size || def.regClass().is_subdword());
       std::fill(ctx.instr_idx_by_regs[ctx.current_block->index].begin() + r,
                 ctx.instr_idx_by_regs[ctx.current_block->index].begin() + r + dw_size, idx);
    }
+   if (instr->isPseudo() && instr->pseudo().needs_scratch_reg) {
+      if (!instr->pseudo().tmp_in_scc)
+         ctx.instr_idx_by_regs[ctx.current_block->index][scc] = overwritten_unknown_instr;
+      ctx.instr_idx_by_regs[ctx.current_block->index][instr->pseudo().scratch_sgpr] =
+         overwritten_unknown_instr;
+   }
 }
 
 Idx
@@ -211,7 +217,7 @@ is_overwritten_since(pr_opt_ctx& ctx, PhysReg reg, RegClass rc, const Idx& since
          return true;
       else if (i == overwritten_untrackable || i == not_written_yet)
          continue;
-      else if (i == overwritten_subdword)
+      else if (i == overwritten_unknown_instr)
          return true;
 
       assert(i.found());
@@ -748,6 +754,12 @@ optimize_postRA(Program* program)
 
       for (aco_ptr<Instruction>& instr : block.instructions)
          process_instruction(ctx, instr);
+
+      /* SCC might get overwritten by copies or swaps from parallelcopies
+       * inserted by SSA-elimination for linear phis.
+       */
+      if (!block.scc_live_out)
+         ctx.instr_idx_by_regs[block.index][scc] = overwritten_unknown_instr;
    }
 
    /* Cleanup pass
diff --git a/src/amd/compiler/aco_reduce_assign.cpp b/src/amd/compiler/aco_reduce_assign.cpp
index 83514206d46..2bc5add45ae 100644
--- a/src/amd/compiler/aco_reduce_assign.cpp
+++ b/src/amd/compiler/aco_reduce_assign.cpp
@@ -79,7 +79,7 @@ setup_reduce_temp(Program* program)
           * Here, the linear vgpr is used before any phi copies, so this isn't necessary.
           */
          if (inserted_at >= 0) {
-            aco_ptr<Instruction> end{create_instruction<Instruction>(
+            aco_ptr<Instruction> end{create_instruction<Pseudo_instruction>(
                aco_opcode::p_end_linear_vgpr, Format::PSEUDO, vtmp_inserted_at >= 0 ? 2 : 1, 0)};
             end->operands[0] = Operand(reduceTmp);
             if (vtmp_inserted_at >= 0)
@@ -118,11 +118,16 @@ setup_reduce_temp(Program* program)
                 * would insert at the end instead of using this one. */
             } else {
                assert(last_top_level_block_idx < block.index);
-               /* insert before the branch at last top level block */
+               /* insert after p_logical_end of the last top-level block */
                std::vector<aco_ptr<Instruction>>& instructions =
                   program->blocks[last_top_level_block_idx].instructions;
-               instructions.insert(std::next(instructions.begin(), instructions.size() - 1),
-                                   std::move(create));
+               auto insert_point =
+                  std::find_if(instructions.rbegin(), instructions.rend(),
+                               [](const auto& iter) {
+                                  return iter->opcode == aco_opcode::p_logical_end;
+                               })
+                     .base();
+               instructions.insert(insert_point, std::move(create));
                inserted_at = last_top_level_block_idx;
             }
          }
@@ -161,8 +166,13 @@ setup_reduce_temp(Program* program)
                assert(last_top_level_block_idx < block.index);
                std::vector<aco_ptr<Instruction>>& instructions =
                   program->blocks[last_top_level_block_idx].instructions;
-               instructions.insert(std::next(instructions.begin(), instructions.size() - 1),
-                                   std::move(create));
+               auto insert_point =
+                  std::find_if(instructions.rbegin(), instructions.rend(),
+                               [](const auto& iter) {
+                                  return iter->opcode == aco_opcode::p_logical_end;
+                               })
+                     .base();
+               instructions.insert(insert_point, std::move(create));
                vtmp_inserted_at = last_top_level_block_idx;
             }
          }
diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 83960cd5d77..4596dddf253 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -47,7 +47,8 @@ void add_subdword_operand(ra_ctx& ctx, aco_ptr<Instruction>& instr, unsigned idx
                           RegClass rc);
 std::pair<unsigned, unsigned>
 get_subdword_definition_info(Program* program, const aco_ptr<Instruction>& instr, RegClass rc);
-void add_subdword_definition(Program* program, aco_ptr<Instruction>& instr, PhysReg reg);
+void add_subdword_definition(Program* program, aco_ptr<Instruction>& instr, PhysReg reg,
+                             bool allow_16bit_write);
 
 struct assignment {
    PhysReg reg;
@@ -62,7 +63,7 @@ struct assignment {
    };
    uint32_t affinity = 0;
    assignment() = default;
-   assignment(PhysReg reg_, RegClass rc_) : reg(reg_), rc(rc_), assigned(-1) {}
+   assignment(PhysReg reg_, RegClass rc_) : reg(reg_), rc(rc_) { assigned = true; }
    void set(const Definition& def)
    {
       assigned = true;
@@ -96,9 +97,9 @@ struct ra_ctx {
          renames(program->blocks.size()), policy(policy_)
    {
       pseudo_dummy.reset(
-         create_instruction<Instruction>(aco_opcode::p_parallelcopy, Format::PSEUDO, 0, 0));
+         create_instruction<Pseudo_instruction>(aco_opcode::p_parallelcopy, Format::PSEUDO, 0, 0));
       phi_dummy.reset(
-         create_instruction<Instruction>(aco_opcode::p_linear_phi, Format::PSEUDO, 0, 0));
+         create_instruction<Pseudo_instruction>(aco_opcode::p_linear_phi, Format::PSEUDO, 0, 0));
       sgpr_limit = get_addr_sgpr_from_waves(program, program->min_waves);
       vgpr_limit = get_addr_vgpr_from_waves(program, program->min_waves);
    }
@@ -678,7 +679,8 @@ get_subdword_definition_info(Program* program, const aco_ptr<Instruction>& instr
 }
 
 void
-add_subdword_definition(Program* program, aco_ptr<Instruction>& instr, PhysReg reg)
+add_subdword_definition(Program* program, aco_ptr<Instruction>& instr, PhysReg reg,
+                        bool allow_16bit_write)
 {
    if (instr->isPseudo())
       return;
@@ -687,7 +689,7 @@ add_subdword_definition(Program* program, aco_ptr<Instruction>& instr, PhysReg r
       amd_gfx_level gfx_level = program->gfx_level;
       assert(instr->definitions[0].bytes() <= 2);
 
-      if (reg.byte() == 0 && instr_is_16bit(gfx_level, instr->opcode))
+      if (reg.byte() == 0 && allow_16bit_write && instr_is_16bit(gfx_level, instr->opcode))
          return;
 
       /* use SDWA */
@@ -696,6 +698,8 @@ add_subdword_definition(Program* program, aco_ptr<Instruction>& instr, PhysReg r
          return;
       }
 
+      assert(allow_16bit_write);
+
       if (instr->opcode == aco_opcode::v_fma_mixlo_f16) {
          instr->opcode = aco_opcode::v_fma_mixhi_f16;
          return;
@@ -838,7 +842,7 @@ update_renames(ra_ctx& ctx, RegisterFile& reg_file,
       assert(ctx.assignments.size() == ctx.program->peekAllocationId());
 
       /* check if we moved an operand */
-      bool first = true;
+      bool first[2] = {true, true};
       bool fill = true;
       for (unsigned i = 0; i < instr->operands.size(); i++) {
          Operand& op = instr->operands[i];
@@ -846,25 +850,31 @@ update_renames(ra_ctx& ctx, RegisterFile& reg_file,
             continue;
          if (op.tempId() == copy.first.tempId()) {
             /* only rename precolored operands if the copy-location matches */
-            if ((flags & rename_precolored_ops) && op.isFixed() &&
-                op.physReg() != copy.second.physReg())
-               continue;
-
-            bool omit_renaming = !(flags & rename_not_killed_ops) && !op.isKillBeforeDef();
-            for (std::pair<Operand, Definition>& pc : parallelcopies) {
-               PhysReg def_reg = pc.second.physReg();
-               omit_renaming &= def_reg > copy.first.physReg()
-                                   ? (copy.first.physReg() + copy.first.size() <= def_reg.reg())
-                                   : (def_reg + pc.second.size() <= copy.first.physReg().reg());
+            bool omit_renaming = (flags & rename_precolored_ops) && op.isFixed() &&
+                                 op.physReg() != copy.second.physReg();
+
+            /* Omit renaming in some cases for p_create_vector in order to avoid
+             * unnecessary shuffle code. */
+            if (!(flags & rename_not_killed_ops) && !op.isKillBeforeDef()) {
+               omit_renaming = true;
+               for (std::pair<Operand, Definition>& pc : parallelcopies) {
+                  PhysReg def_reg = pc.second.physReg();
+                  omit_renaming &= def_reg > copy.first.physReg()
+                                      ? (copy.first.physReg() + copy.first.size() <= def_reg.reg())
+                                      : (def_reg + pc.second.size() <= copy.first.physReg().reg());
+               }
             }
-            if (omit_renaming) {
-               if (first)
-                  op.setFirstKill(true);
-               else
-                  op.setKill(true);
-               first = false;
+
+            /* Fix the kill flags */
+            if (first[omit_renaming])
+               op.setFirstKill(omit_renaming || op.isKill());
+            else
+               op.setKill(omit_renaming || op.isKill());
+            first[omit_renaming] = false;
+
+            if (omit_renaming)
                continue;
-            }
+
             op.setTemp(copy.second.getTemp());
             op.setFixed(copy.second.physReg());
 
@@ -1913,6 +1923,7 @@ handle_pseudo(ra_ctx& ctx, const RegisterFile& reg_file, Instruction* instr)
    if (!needs_scratch_reg)
       return;
 
+   instr->pseudo().needs_scratch_reg = true;
    instr->pseudo().tmp_in_scc = reg_file[scc];
 
    int reg = ctx.max_used_sgpr;
@@ -1936,19 +1947,6 @@ bool
 operand_can_use_reg(amd_gfx_level gfx_level, aco_ptr<Instruction>& instr, unsigned idx, PhysReg reg,
                     RegClass rc)
 {
-   bool is_writelane = instr->opcode == aco_opcode::v_writelane_b32 ||
-                       instr->opcode == aco_opcode::v_writelane_b32_e64;
-   if (gfx_level <= GFX9 && is_writelane && idx <= 1) {
-      /* v_writelane_b32 can take two sgprs but only if one is m0. */
-      bool is_other_sgpr =
-         instr->operands[!idx].isTemp() &&
-         (!instr->operands[!idx].isFixed() || instr->operands[!idx].physReg() != m0);
-      if (is_other_sgpr && instr->operands[!idx].tempId() != instr->operands[idx].tempId()) {
-         instr->operands[idx].setFixed(m0);
-         return reg == m0;
-      }
-   }
-
    if (reg.byte()) {
       unsigned stride = get_subdword_operand_stride(gfx_level, instr, idx, rc);
       if (reg.byte() % stride)
@@ -2844,6 +2842,18 @@ register_allocation(Program* program, std::vector<IDSet>& live_out_per_block, ra
                operand.isFixed() && ctx.assignments[operand.tempId()].reg != operand.physReg();
          }
 
+         bool is_writelane = instr->opcode == aco_opcode::v_writelane_b32 ||
+                             instr->opcode == aco_opcode::v_writelane_b32_e64;
+         if (program->gfx_level <= GFX9 && is_writelane && instr->operands[0].isTemp() &&
+             instr->operands[1].isTemp()) {
+            /* v_writelane_b32 can take two sgprs but only if one is m0. */
+            if (ctx.assignments[instr->operands[0].tempId()].reg != m0 &&
+                ctx.assignments[instr->operands[1].tempId()].reg != m0) {
+               instr->operands[0].setFixed(m0);
+               fixed = true;
+            }
+         }
+
          if (fixed)
             handle_fixed_operands(ctx, register_file, parallelcopy, instr);
 
@@ -2979,7 +2989,8 @@ register_allocation(Program* program, std::vector<IDSet>& live_out_per_block, ra
                   PhysReg reg = get_reg(ctx, register_file, tmp, parallelcopy, instr);
                   definition->setFixed(reg);
                   if (reg.byte() || register_file.test(reg, 4)) {
-                     add_subdword_definition(program, instr, reg);
+                     bool allow_16bit_write = reg.byte() % 2 == 0 && !register_file.test(reg, 2);
+                     add_subdword_definition(program, instr, reg, allow_16bit_write);
                      definition = &instr->definitions[i]; /* add_subdword_definition can invalidate
                                                              the reference */
                   }
@@ -3068,6 +3079,7 @@ register_allocation(Program* program, std::vector<IDSet>& live_out_per_block, ra
 
                handle_pseudo(ctx, tmp_file, pc.get());
             } else {
+               pc->needs_scratch_reg = sgpr_operands_alias_defs || linear_vgpr;
                pc->tmp_in_scc = false;
             }
 
diff --git a/src/amd/compiler/aco_spill.cpp b/src/amd/compiler/aco_spill.cpp
index 8da3d49202b..47c4f803834 100644
--- a/src/amd/compiler/aco_spill.cpp
+++ b/src/amd/compiler/aco_spill.cpp
@@ -1845,10 +1845,16 @@ assign_spill_slots(spill_ctx& ctx, unsigned spills_to_vgpr)
                      instructions.emplace_back(std::move(create));
                   } else {
                      assert(last_top_level_block_idx < block.index);
-                     /* insert before the branch at last top level block */
+                     /* insert after p_logical_end of the last top-level block */
                      std::vector<aco_ptr<Instruction>>& block_instrs =
                         ctx.program->blocks[last_top_level_block_idx].instructions;
-                     block_instrs.insert(std::prev(block_instrs.end()), std::move(create));
+                     auto insert_point =
+                        std::find_if(block_instrs.rbegin(), block_instrs.rend(),
+                                     [](const auto& iter) {
+                                        return iter->opcode == aco_opcode::p_logical_end;
+                                     })
+                           .base();
+                     block_instrs.insert(insert_point, std::move(create));
                   }
                }
 
@@ -1885,10 +1891,16 @@ assign_spill_slots(spill_ctx& ctx, unsigned spills_to_vgpr)
                      instructions.emplace_back(std::move(create));
                   } else {
                      assert(last_top_level_block_idx < block.index);
-                     /* insert before the branch at last top level block */
+                     /* insert after p_logical_end of the last top-level block */
                      std::vector<aco_ptr<Instruction>>& block_instrs =
                         ctx.program->blocks[last_top_level_block_idx].instructions;
-                     block_instrs.insert(std::prev(block_instrs.end()), std::move(create));
+                     auto insert_point =
+                        std::find_if(block_instrs.rbegin(), block_instrs.rend(),
+                                     [](const auto& iter) {
+                                        return iter->opcode == aco_opcode::p_logical_end;
+                                     })
+                           .base();
+                     block_instrs.insert(insert_point, std::move(create));
                   }
                }
 
diff --git a/src/amd/compiler/aco_ssa_elimination.cpp b/src/amd/compiler/aco_ssa_elimination.cpp
index 729449fcf7f..477b59ba6be 100644
--- a/src/amd/compiler/aco_ssa_elimination.cpp
+++ b/src/amd/compiler/aco_ssa_elimination.cpp
@@ -133,6 +133,7 @@ insert_parallelcopies(ssa_elimination_ctx& ctx)
       }
       pc->tmp_in_scc = block.scc_live_out;
       pc->scratch_sgpr = scratch_sgpr;
+      pc->needs_scratch_reg = true;
       block.instructions.insert(it, std::move(pc));
    }
 }
@@ -397,6 +398,10 @@ try_optimize_branching_sequence(ssa_elimination_ctx& ctx, Block& block, const in
                                    regs_intersect(Definition(exec, ctx.program->lane_mask), def);
                          }))
             break;
+
+         if (instr->isPseudo() && instr->pseudo().needs_scratch_reg &&
+             regs_intersect(exec_copy_def, Definition(instr->pseudo().scratch_sgpr, s1)))
+            break;
       }
    }
 
@@ -436,6 +441,9 @@ try_optimize_branching_sequence(ssa_elimination_ctx& ctx, Block& block, const in
          for (const Definition& def : instr->definitions)
             if (regs_intersect(exec_copy_def, def))
                return;
+         if (instr->isPseudo() && instr->pseudo().needs_scratch_reg &&
+             regs_intersect(exec_copy_def, Definition(instr->pseudo().scratch_sgpr, s1)))
+            return;
       }
 
       /* Check if the instruction may implicitly read VCC, eg. v_cndmask or add with carry.
diff --git a/src/amd/compiler/tests/test_assembler.cpp b/src/amd/compiler/tests/test_assembler.cpp
index a7106e98686..14a3594829c 100644
--- a/src/amd/compiler/tests/test_assembler.cpp
+++ b/src/amd/compiler/tests/test_assembler.cpp
@@ -741,6 +741,9 @@ BEGIN_TEST(assembler.gfx11.flat)
    //! scratch_load_b32 v42, v10, s32                              ; dc510000 2aa0000a
    bld.scratch(aco_opcode::scratch_load_dword, dst_v1, op_v1, op_s1);
 
+   //! scratch_load_b32 v42, off, off                              ; dc510000 2a7c0080
+   bld.scratch(aco_opcode::scratch_load_dword, dst_v1, Operand(v1), Operand(s1));
+
    //! global_load_b32 v42, v[20:21], off offset:-42               ; dc521fd6 2a7c0014
    bld.global(aco_opcode::global_load_dword, dst_v1, op_v2, Operand(s1), -42);
 
diff --git a/src/amd/compiler/tests/test_insert_waitcnt.cpp b/src/amd/compiler/tests/test_insert_waitcnt.cpp
index 86d81845173..e17a39be195 100644
--- a/src/amd/compiler/tests/test_insert_waitcnt.cpp
+++ b/src/amd/compiler/tests/test_insert_waitcnt.cpp
@@ -53,3 +53,71 @@ BEGIN_TEST(insert_waitcnt.ds_ordered_count)
 
    finish_waitcnt_test();
 END_TEST
+
+BEGIN_TEST(insert_waitcnt.waw.mixed_vmem_lds.vmem)
+   if (!setup_cs(NULL, GFX10))
+      return;
+
+   Definition def_v4(PhysReg(260), v1);
+   Operand op_v0(PhysReg(256), v1);
+   Operand desc0(PhysReg(0), s4);
+
+   //>> BB0
+   //! /* logical preds: / linear preds: / kind: top-level, */
+   //! v1: %0:v[4] = buffer_load_dword %0:s[0-3], %0:v[0], 0
+   bld.mubuf(aco_opcode::buffer_load_dword, def_v4, desc0, op_v0, Operand::zero(), 0, false);
+
+   //>> BB1
+   //! /* logical preds: / linear preds: / kind: */
+   //! v1: %0:v[4] = ds_read_b32 %0:v[0]
+   bld.reset(program->create_and_insert_block());
+   bld.ds(aco_opcode::ds_read_b32, def_v4, op_v0);
+
+   bld.reset(program->create_and_insert_block());
+   program->blocks[2].linear_preds.push_back(0);
+   program->blocks[2].linear_preds.push_back(1);
+   program->blocks[2].logical_preds.push_back(0);
+   program->blocks[2].logical_preds.push_back(1);
+
+   //>> BB2
+   //! /* logical preds: BB0, BB1, / linear preds: BB0, BB1, / kind: uniform, */
+   //! s_waitcnt lgkmcnt(0)
+   //! v1: %0:v[4] = buffer_load_dword %0:s[0-3], %0:v[0], 0
+   bld.mubuf(aco_opcode::buffer_load_dword, def_v4, desc0, op_v0, Operand::zero(), 0, false);
+
+   finish_waitcnt_test();
+END_TEST
+
+BEGIN_TEST(insert_waitcnt.waw.mixed_vmem_lds.lds)
+   if (!setup_cs(NULL, GFX10))
+      return;
+
+   Definition def_v4(PhysReg(260), v1);
+   Operand op_v0(PhysReg(256), v1);
+   Operand desc0(PhysReg(0), s4);
+
+   //>> BB0
+   //! /* logical preds: / linear preds: / kind: top-level, */
+   //! v1: %0:v[4] = buffer_load_dword %0:s[0-3], %0:v[0], 0
+   bld.mubuf(aco_opcode::buffer_load_dword, def_v4, desc0, op_v0, Operand::zero(), 0, false);
+
+   //>> BB1
+   //! /* logical preds: / linear preds: / kind: */
+   //! v1: %0:v[4] = ds_read_b32 %0:v[0]
+   bld.reset(program->create_and_insert_block());
+   bld.ds(aco_opcode::ds_read_b32, def_v4, op_v0);
+
+   bld.reset(program->create_and_insert_block());
+   program->blocks[2].linear_preds.push_back(0);
+   program->blocks[2].linear_preds.push_back(1);
+   program->blocks[2].logical_preds.push_back(0);
+   program->blocks[2].logical_preds.push_back(1);
+
+   //>> BB2
+   //! /* logical preds: BB0, BB1, / linear preds: BB0, BB1, / kind: uniform, */
+   //! s_waitcnt vmcnt(0)
+   //! v1: %0:v[4] = ds_read_b32 %0:v[0]
+   bld.ds(aco_opcode::ds_read_b32, def_v4, op_v0);
+
+   finish_waitcnt_test();
+END_TEST
diff --git a/src/amd/compiler/tests/test_reduce_assign.cpp b/src/amd/compiler/tests/test_reduce_assign.cpp
index 7f44e55486f..8aae778bd98 100644
--- a/src/amd/compiler/tests/test_reduce_assign.cpp
+++ b/src/amd/compiler/tests/test_reduce_assign.cpp
@@ -41,6 +41,11 @@ BEGIN_TEST(setup_reduce_temp.divergent_if_phi)
    if (!setup_cs("s2 v1", GFX9))
       return;
 
+   //>> p_logical_start
+   //>> p_logical_end
+   bld.pseudo(aco_opcode::p_logical_start);
+   bld.pseudo(aco_opcode::p_logical_end);
+
    //>> lv1: %lv = p_start_linear_vgpr
    emit_divergent_if_else(
       program.get(), bld, Operand(inputs[0]),
diff --git a/src/amd/compiler/tests/test_regalloc.cpp b/src/amd/compiler/tests/test_regalloc.cpp
index 456c42359d4..2a8ac922fc6 100644
--- a/src/amd/compiler/tests/test_regalloc.cpp
+++ b/src/amd/compiler/tests/test_regalloc.cpp
@@ -410,3 +410,21 @@ BEGIN_TEST(regalloc.vinterp_fp16)
 
    finish_ra_test(ra_test_policy());
 END_TEST
+
+BEGIN_TEST(regalloc.writelane)
+   //>> v1: %in0:v[0], s1: %in1:s[0], s1: %in2:s[1], s1: %in3:s[2] = p_startpgm
+   if (!setup_cs("v1 s1 s1 s1", GFX8))
+      return;
+
+   //! s1: %tmp:m0 = p_parallelcopy %int3:s[2]
+   Temp tmp = bld.copy(bld.def(s1, m0), inputs[3]);
+
+   //! s1: %in1_2:m0,  s1: %tmp_2:s[0] = p_parallelcopy %in1:s[0], %tmp:m0
+   //! v1: %tmp2:v[0] = v_writelane_b32_e64 %in1_2:m0, %in2:s[1], %in0:v[0]
+   Temp tmp2 = bld.writelane(bld.def(v1), inputs[1], inputs[2], inputs[0]);
+
+   //! p_unit_test %tmp_2:s[0], %tmp2:v[0]
+   bld.pseudo(aco_opcode::p_unit_test, tmp, tmp2);
+
+   finish_ra_test(ra_test_policy());
+END_TEST
diff --git a/src/amd/compiler/tests/test_to_hw_instr.cpp b/src/amd/compiler/tests/test_to_hw_instr.cpp
index 73084fa6652..f16993f9aaf 100644
--- a/src/amd/compiler/tests/test_to_hw_instr.cpp
+++ b/src/amd/compiler/tests/test_to_hw_instr.cpp
@@ -835,6 +835,56 @@ BEGIN_TEST(to_hw_instr.copy_linear_vgpr_v3)
    finish_to_hw_instr_test();
 END_TEST
 
+BEGIN_TEST(to_hw_instr.copy_linear_vgpr_coalesce)
+   if (!setup_cs(NULL, GFX10))
+      return;
+
+   PhysReg reg_v0{256};
+   PhysReg reg_v1{256 + 1};
+   PhysReg reg_v4{256 + 4};
+   PhysReg reg_v5{256 + 5};
+   RegClass v1_linear = v1.as_linear();
+
+   //>> p_unit_test 0
+   //! lv2: %0:v[0-1] = v_lshrrev_b64 0, %0:v[4-5]
+   //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
+   //! lv2: %0:v[0-1] = v_lshrrev_b64 0, %0:v[4-5]
+   //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
+   bld.pseudo(aco_opcode::p_unit_test, Operand::zero());
+
+   Instruction* instr = bld.pseudo(aco_opcode::p_parallelcopy, Definition(reg_v0, v1_linear),
+                                   Definition(reg_v1, v1_linear), Operand(reg_v4, v1_linear),
+                                   Operand(reg_v5, v1_linear));
+   instr->pseudo().scratch_sgpr = m0;
+
+   //! p_unit_test 1
+   //! lv1: %0:v[0] = v_mov_b32 %0:v[4]
+   //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
+   //! lv1: %0:v[0] = v_mov_b32 %0:v[4]
+   //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
+   //! v1: %0:v[1] = v_mov_b32 %0:v[5]
+   bld.pseudo(aco_opcode::p_unit_test, Operand::c32(1));
+
+   instr = bld.pseudo(aco_opcode::p_parallelcopy, Definition(reg_v0, v1_linear),
+                      Definition(reg_v1, v1), Operand(reg_v4, v1_linear), Operand(reg_v5, v1));
+   instr->pseudo().scratch_sgpr = m0;
+
+   //! p_unit_test 2
+   //! v1: %0:v[0] = v_mov_b32 %0:v[4]
+   //! lv1: %0:v[1] = v_mov_b32 %0:v[5]
+   //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
+   //! lv1: %0:v[1] = v_mov_b32 %0:v[5]
+   //! s2: %0:exec,  s1: %0:scc = s_not_b64 %0:exec
+   bld.pseudo(aco_opcode::p_unit_test, Operand::c32(2));
+
+   instr =
+      bld.pseudo(aco_opcode::p_parallelcopy, Definition(reg_v0, v1), Definition(reg_v1, v1_linear),
+                 Operand(reg_v4, v1), Operand(reg_v5, v1_linear));
+   instr->pseudo().scratch_sgpr = m0;
+
+   finish_to_hw_instr_test();
+END_TEST
+
 BEGIN_TEST(to_hw_instr.pack2x16_constant)
    PhysReg v0_lo{256};
    PhysReg v0_hi{256};
diff --git a/src/amd/llvm/ac_llvm_build.c b/src/amd/llvm/ac_llvm_build.c
index 25013b7667c..5d8da90b6f1 100644
--- a/src/amd/llvm/ac_llvm_build.c
+++ b/src/amd/llvm/ac_llvm_build.c
@@ -119,6 +119,8 @@ void ac_llvm_context_dispose(struct ac_llvm_context *ctx)
    free(ctx->flow->stack);
    free(ctx->flow);
    ctx->flow = NULL;
+
+   LLVMDisposeBuilder(ctx->builder);
 }
 
 int ac_get_llvm_num_components(LLVMValueRef value)
diff --git a/src/amd/llvm/ac_nir_to_llvm.c b/src/amd/llvm/ac_nir_to_llvm.c
index 1d7db8580b0..5dd3ddabe40 100644
--- a/src/amd/llvm/ac_nir_to_llvm.c
+++ b/src/amd/llvm/ac_nir_to_llvm.c
@@ -1935,7 +1935,6 @@ static LLVMValueRef visit_load_buffer(struct ac_nir_context *ctx, nir_intrinsic_
    LLVMValueRef offset = get_src(ctx, instr->src[1]);
    LLVMValueRef rsrc = ctx->abi->load_ssbo ?
       ctx->abi->load_ssbo(ctx->abi, rsrc_base, false, false) : rsrc_base;
-   LLVMValueRef vindex = ctx->ac.i32_0;
 
    LLVMTypeRef def_type = get_def_type(ctx, &instr->def);
    LLVMTypeRef def_elem_type = num_components > 1 ? LLVMGetElementType(def_type) : def_type;
@@ -1964,7 +1963,7 @@ static LLVMValueRef visit_load_buffer(struct ac_nir_context *ctx, nir_intrinsic_
          int num_channels = util_next_power_of_two(load_bytes) / 4;
          bool can_speculate = access & ACCESS_CAN_REORDER;
 
-         ret = ac_build_buffer_load(&ctx->ac, rsrc, num_channels, vindex, voffset, ctx->ac.i32_0,
+         ret = ac_build_buffer_load(&ctx->ac, rsrc, num_channels, NULL, voffset, ctx->ac.i32_0,
                                     ctx->ac.f32, access, can_speculate, false);
       }
 
@@ -4366,6 +4365,7 @@ bool ac_nir_translate(struct ac_llvm_context *ac, struct ac_shader_abi *abi,
 {
    struct ac_nir_context ctx = {0};
    struct nir_function *func;
+   bool ret;
 
    ctx.ac = *ac;
    ctx.abi = abi;
@@ -4395,10 +4395,8 @@ bool ac_nir_translate(struct ac_llvm_context *ac, struct ac_shader_abi *abi,
    if (gl_shader_stage_is_compute(nir->info.stage))
       setup_shared(&ctx, nir);
 
-   if (!visit_cf_list(&ctx, &func->impl->body))
-      return false;
-
-   phi_post_pass(&ctx);
+   if ((ret = visit_cf_list(&ctx, &func->impl->body)))
+      phi_post_pass(&ctx);
 
    free(ctx.ssa_defs);
    ralloc_free(ctx.defs);
@@ -4406,7 +4404,7 @@ bool ac_nir_translate(struct ac_llvm_context *ac, struct ac_shader_abi *abi,
    if (ctx.abi->kill_ps_if_inf_interp)
       ralloc_free(ctx.verified_interp);
 
-   return true;
+   return ret;
 }
 
 /* Fixup the HW not emitting the TCS regs if there are no HS threads. */
diff --git a/src/amd/registers/gfx11.json b/src/amd/registers/gfx11.json
index 5f8fc95d029..23d5d82d007 100644
--- a/src/amd/registers/gfx11.json
+++ b/src/amd/registers/gfx11.json
@@ -20,8 +20,8 @@
    "entries": [
     {"name": "BINNING_ALLOWED", "value": 0},
     {"name": "FORCE_BINNING_ON", "value": 1},
-    {"name": "DISABLE_BINNING_USE_NEW_SC", "value": 2},
-    {"name": "DISABLE_BINNING_USE_LEGACY_SC", "value": 3}
+    {"name": "BINNING_ONE_PRIM_PER_BATCH", "value": 2},
+    {"name": "BINNING_DISABLED", "value": 3}
    ]
   },
   "BlendOp": {
diff --git a/src/amd/registers/gfx115.json b/src/amd/registers/gfx115.json
index 03f320a7fa6..20e0f2c64f2 100644
--- a/src/amd/registers/gfx115.json
+++ b/src/amd/registers/gfx115.json
@@ -20,8 +20,7 @@
    "entries": [
     {"name": "BINNING_ALLOWED", "value": 0},
     {"name": "FORCE_BINNING_ON", "value": 1},
-    {"name": "DISABLE_BINNING_USE_NEW_SC", "value": 2},
-    {"name": "DISABLE_BINNING_USE_LEGACY_SC", "value": 3}
+    {"name": "BINNING_DISABLED", "value": 3}
    ]
   },
   "BlendOp": {
diff --git a/src/amd/registers/parse_kernel_headers.py b/src/amd/registers/parse_kernel_headers.py
index 79e0f00fd0f..2da746c8589 100644
--- a/src/amd/registers/parse_kernel_headers.py
+++ b/src/amd/registers/parse_kernel_headers.py
@@ -444,6 +444,23 @@ VRSHtileEncoding = {
  ]
 }
 
+BinningModeGfx11 = {
+ "entries": [
+  {"name": "BINNING_ALLOWED", "value": 0},
+  {"name": "FORCE_BINNING_ON", "value": 1},
+  {"name": "BINNING_ONE_PRIM_PER_BATCH", "value": 2},
+  {"name": "BINNING_DISABLED", "value": 3}
+ ]
+}
+
+BinningModeGfx115Plus = {
+ "entries": [
+  {"name": "BINNING_ALLOWED", "value": 0},
+  {"name": "FORCE_BINNING_ON", "value": 1},
+  {"name": "BINNING_DISABLED", "value": 3}
+ ]
+}
+
 missing_enums_all = {
   'FLOAT_MODE': {
     "entries": [
@@ -669,6 +686,11 @@ missing_enums_gfx11plus = {
   },
 }
 
+missing_enums_gfx115plus = {
+  **missing_enums_gfx11plus,
+  "BinningMode": BinningModeGfx115Plus,
+}
+
 enums_missing = {
   'gfx6': {
     **missing_enums_all,
@@ -704,9 +726,10 @@ enums_missing = {
   },
   'gfx11': {
     **missing_enums_gfx11plus,
+    "BinningMode": BinningModeGfx11,
   },
   'gfx115': {
-    **missing_enums_gfx11plus,
+    **missing_enums_gfx115plus,
   },
 }
 
diff --git a/src/amd/vulkan/meta/radv_meta_bufimage.c b/src/amd/vulkan/meta/radv_meta_bufimage.c
index 743b405507b..c581440806c 100644
--- a/src/amd/vulkan/meta/radv_meta_bufimage.c
+++ b/src/amd/vulkan/meta/radv_meta_bufimage.c
@@ -1585,15 +1585,26 @@ radv_meta_image_to_image_cs(struct radv_cmd_buffer *cmd_buffer, struct radv_meta
    }
 
    u_foreach_bit (i, dst->aspect_mask) {
-      unsigned aspect_mask = 1u << i;
+      unsigned dst_aspect_mask = 1u << i;
+      unsigned src_aspect_mask = dst_aspect_mask;
       VkFormat depth_format = 0;
-      if (aspect_mask == VK_IMAGE_ASPECT_STENCIL_BIT)
+      if (dst_aspect_mask == VK_IMAGE_ASPECT_STENCIL_BIT)
          depth_format = vk_format_stencil_only(dst->image->vk.format);
-      else if (aspect_mask == VK_IMAGE_ASPECT_DEPTH_BIT)
+      else if (dst_aspect_mask == VK_IMAGE_ASPECT_DEPTH_BIT)
          depth_format = vk_format_depth_only(dst->image->vk.format);
+      else {
+         /*
+          * "Multi-planar images can only be copied on a per-plane basis, and the subresources used in each region when
+          * copying to or from such images must specify only one plane, though different regions can specify different
+          * planes."
+          */
+         assert((dst->aspect_mask & (dst->aspect_mask - 1)) == 0);
+         assert((src->aspect_mask & (src->aspect_mask - 1)) == 0);
+         src_aspect_mask = src->aspect_mask;
+      }
 
-      create_iview(cmd_buffer, src, &src_view, depth_format, aspect_mask);
-      create_iview(cmd_buffer, dst, &dst_view, depth_format, aspect_mask);
+      create_iview(cmd_buffer, src, &src_view, depth_format, src_aspect_mask);
+      create_iview(cmd_buffer, dst, &dst_view, depth_format, dst_aspect_mask);
 
       itoi_bind_descriptors(cmd_buffer, &src_view, &dst_view);
 
diff --git a/src/amd/vulkan/nir/radv_nir_rt_common.c b/src/amd/vulkan/nir/radv_nir_rt_common.c
index 9ae88c44ca2..9c681b7e103 100644
--- a/src/amd/vulkan/nir/radv_nir_rt_common.c
+++ b/src/amd/vulkan/nir/radv_nir_rt_common.c
@@ -227,6 +227,13 @@ intersect_ray_amd_software_tri(struct radv_device *device, nir_builder *b, nir_d
    nir_def *cx = nir_fsub(b, nir_vector_extract(b, v_c, kx), nir_fmul(b, sx, nir_vector_extract(b, v_c, kz)));
    nir_def *cy = nir_fsub(b, nir_vector_extract(b, v_c, ky), nir_fmul(b, sy, nir_vector_extract(b, v_c, kz)));
 
+   ax = nir_f2f64(b, ax);
+   ay = nir_f2f64(b, ay);
+   bx = nir_f2f64(b, bx);
+   by = nir_f2f64(b, by);
+   cx = nir_f2f64(b, cx);
+   cy = nir_f2f64(b, cy);
+
    nir_def *u = nir_fsub(b, nir_fmul(b, cx, by), nir_fmul(b, cy, bx));
    nir_def *v = nir_fsub(b, nir_fmul(b, ax, cy), nir_fmul(b, ay, cx));
    nir_def *w = nir_fsub(b, nir_fmul(b, bx, ay), nir_fmul(b, by, ax));
@@ -244,6 +251,12 @@ intersect_ray_amd_software_tri(struct radv_device *device, nir_builder *b, nir_d
    {
       nir_def *det = nir_fadd(b, u, nir_fadd(b, v, w));
 
+      sz = nir_f2f64(b, sz);
+
+      v_a = nir_f2f64(b, v_a);
+      v_b = nir_f2f64(b, v_b);
+      v_c = nir_f2f64(b, v_c);
+
       nir_def *az = nir_fmul(b, sz, nir_vector_extract(b, v_a, kz));
       nir_def *bz = nir_fmul(b, sz, nir_vector_extract(b, v_b, kz));
       nir_def *cz = nir_fmul(b, sz, nir_vector_extract(b, v_c, kz));
@@ -256,7 +269,13 @@ intersect_ray_amd_software_tri(struct radv_device *device, nir_builder *b, nir_d
 
       nir_push_if(b, det_cond_front);
       {
-         nir_def *indices[4] = {t, det, v, w};
+         nir_def *det_abs = nir_fabs(b, det);
+
+         t = nir_f2f32(b, nir_fdiv(b, t, det_abs));
+         v = nir_f2f32(b, nir_fdiv(b, v, det_abs));
+         w = nir_f2f32(b, nir_fdiv(b, w, det_abs));
+
+         nir_def *indices[4] = {t, nir_f2f32(b, nir_fsign(b, det)), v, w};
          nir_store_var(b, result, nir_vec(b, indices, 4), 0xf);
       }
       nir_pop_if(b, NULL);
@@ -399,20 +418,6 @@ insert_traversal_triangle_case(struct radv_device *device, nir_builder *b, const
             nir_def *divs[2] = {div, div};
             intersection.barycentrics = nir_fdiv(b, nir_channels(b, result, 0xc), nir_vec(b, divs, 2));
 
-            nir_def *hit_t = intersection.t;
-            /* t values within 10 ULP of the current hit t are most likely duplicate hits along shared edges, which
-             * might occur with emulated RT. The Vulkan spec discourages double-hits along shared-edges, so reject them
-             * here by subtracting 10 ULP from t.
-             */
-            if (radv_emulate_rt(device->physical_device)) {
-               nir_def *abs_t = nir_fabs(b, hit_t);
-               nir_def *sign_t = nir_fsign(b, hit_t);
-
-               nir_def *tm1 = nir_iadd(b, hit_t, nir_imul_imm(b, nir_f2i32(b, sign_t), -10));
-               nir_def *tm2 = nir_fmul(b, nir_isub_imm(b, 10, abs_t), nir_fneg(b, sign_t));
-               intersection.t = nir_bcsel(b, nir_ige_imm(b, abs_t, 10), tm1, tm2);
-            }
-
             args->triangle_cb(b, &intersection, args, ray_flags);
          }
          nir_pop_if(b, NULL);
diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 5c6cb7f76d7..fce095af99b 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -1551,6 +1551,8 @@ radv_get_disabled_binning_state(struct radv_cmd_buffer *cmd_buffer)
    uint32_t pa_sc_binner_cntl_0;
 
    if (pdevice->rad_info.gfx_level >= GFX10) {
+      const unsigned binning_disabled =
+         pdevice->rad_info.gfx_level >= GFX11_5 ? V_028C44_BINNING_DISABLED : V_028C44_DISABLE_BINNING_USE_NEW_SC;
       unsigned min_bytes_per_pixel = 0;
 
       for (unsigned i = 0; i < render->color_att_count; ++i) {
@@ -1567,8 +1569,8 @@ radv_get_disabled_binning_state(struct radv_cmd_buffer *cmd_buffer)
             min_bytes_per_pixel = bytes;
       }
 
-      pa_sc_binner_cntl_0 = S_028C44_BINNING_MODE(V_028C44_DISABLE_BINNING_USE_NEW_SC) | S_028C44_BIN_SIZE_X(0) |
-                            S_028C44_BIN_SIZE_Y(0) | S_028C44_BIN_SIZE_X_EXTEND(2) |       /* 128 */
+      pa_sc_binner_cntl_0 = S_028C44_BINNING_MODE(binning_disabled) | S_028C44_BIN_SIZE_X(0) | S_028C44_BIN_SIZE_Y(0) |
+                            S_028C44_BIN_SIZE_X_EXTEND(2) |                                /* 128 */
                             S_028C44_BIN_SIZE_Y_EXTEND(min_bytes_per_pixel <= 4 ? 2 : 1) | /* 128 or 64 */
                             S_028C44_DISABLE_START_OF_PRIM(1) | S_028C44_FLUSH_ON_BINNING_TRANSITION(1);
    } else {
@@ -3743,7 +3745,8 @@ radv_flush_occlusion_query_state(struct radv_cmd_buffer *cmd_buffer)
    if (!enable_occlusion_queries) {
       db_count_control = S_028004_ZPASS_INCREMENT_DISABLE(gfx_level < GFX11);
    } else {
-      uint32_t sample_rate = util_logbase2(cmd_buffer->state.render.max_samples);
+      const uint32_t rasterization_samples = radv_get_rasterization_samples(cmd_buffer);
+      const uint32_t sample_rate = util_logbase2(rasterization_samples);
       bool gfx10_perfect =
          gfx_level >= GFX10 && (cmd_buffer->state.perfect_occlusion_queries_enabled ||
                                 cmd_buffer->state.inherited_query_control_flags & VK_QUERY_CONTROL_PRECISE_BIT);
@@ -5609,6 +5612,9 @@ radv_dst_access_flush(struct radv_cmd_buffer *cmd_buffer, VkAccessFlags2 dst_fla
             flush_bits |= RADV_CMD_FLAG_FLUSH_AND_INV_DB_META;
          break;
       case VK_ACCESS_2_MEMORY_READ_BIT:
+         if (has_CB_meta || has_DB_meta)
+            flush_bits |= RADV_CMD_FLAG_INV_L2_METADATA;
+         FALLTHROUGH;
       case VK_ACCESS_2_MEMORY_WRITE_BIT:
          flush_bits |= RADV_CMD_FLAG_INV_VCACHE | RADV_CMD_FLAG_INV_SCACHE;
          if (!image_is_coherent)
@@ -5758,7 +5764,7 @@ radv_BeginCommandBuffer(VkCommandBuffer commandBuffer, const VkCommandBufferBegi
    cmd_buffer->state.dirty |= RADV_CMD_DIRTY_DYNAMIC_ALL | RADV_CMD_DIRTY_GUARDBAND | RADV_CMD_DIRTY_OCCLUSION_QUERY |
                               RADV_CMD_DIRTY_DB_SHADER_CONTROL;
 
-   if (cmd_buffer->device->physical_device->rad_info.gfx_level >= GFX7) {
+   if (cmd_buffer->qf == RADV_QUEUE_COMPUTE || cmd_buffer->device->vk.enabled_features.taskShader) {
       uint32_t pred_value = 0;
       uint32_t pred_offset;
       if (!radv_cmd_buffer_upload_data(cmd_buffer, 4, &pred_value, &pred_offset))
@@ -5955,6 +5961,9 @@ radv_CmdBindIndexBuffer2KHR(VkCommandBuffer commandBuffer, VkBuffer buffer, VkDe
    } else {
       cmd_buffer->state.index_va = 0;
       cmd_buffer->state.max_index_count = 0;
+
+      if (cmd_buffer->device->physical_device->rad_info.has_null_index_buffer_clamping_bug)
+         cmd_buffer->state.index_va = 0x2;
    }
 
    cmd_buffer->state.dirty |= RADV_CMD_DIRTY_INDEX_BUFFER;
@@ -7937,8 +7946,8 @@ radv_emit_view_index(struct radv_cmd_buffer *cmd_buffer, unsigned index)
  * space in the upload BO and emit some packets to invert the condition.
  */
 static void
-radv_cs_emit_compute_predication(struct radv_cmd_state *state, struct radeon_cmdbuf *cs, uint64_t inv_va,
-                                 bool *inv_emitted, unsigned dwords)
+radv_cs_emit_compute_predication(const struct radv_device *device, struct radv_cmd_state *state,
+                                 struct radeon_cmdbuf *cs, uint64_t inv_va, bool *inv_emitted, unsigned dwords)
 {
    if (!state->predicating)
       return;
@@ -7948,28 +7957,37 @@ radv_cs_emit_compute_predication(struct radv_cmd_state *state, struct radeon_cmd
    if (!state->predication_type) {
       /* Invert the condition the first time it is needed. */
       if (!*inv_emitted) {
+         const enum amd_gfx_level gfx_level = device->physical_device->rad_info.gfx_level;
+
          *inv_emitted = true;
 
          /* Write 1 to the inverted predication VA. */
          radeon_emit(cs, PKT3(PKT3_COPY_DATA, 4, 0));
-         radeon_emit(cs,
-                     COPY_DATA_SRC_SEL(COPY_DATA_IMM) | COPY_DATA_DST_SEL(COPY_DATA_DST_MEM) | COPY_DATA_WR_CONFIRM);
+         radeon_emit(cs, COPY_DATA_SRC_SEL(COPY_DATA_IMM) | COPY_DATA_DST_SEL(COPY_DATA_DST_MEM) |
+                            COPY_DATA_WR_CONFIRM | (gfx_level == GFX6 ? COPY_DATA_ENGINE_PFP : 0));
          radeon_emit(cs, 1);
          radeon_emit(cs, 0);
          radeon_emit(cs, inv_va);
          radeon_emit(cs, inv_va >> 32);
 
          /* If the API predication VA == 0, skip next command. */
-         radeon_emit(cs, PKT3(PKT3_COND_EXEC, 3, 0));
-         radeon_emit(cs, va);
-         radeon_emit(cs, va >> 32);
-         radeon_emit(cs, 0);
-         radeon_emit(cs, 6); /* 1x COPY_DATA size */
+         if (device->physical_device->rad_info.gfx_level >= GFX7) {
+            radeon_emit(cs, PKT3(PKT3_COND_EXEC, 3, 0));
+            radeon_emit(cs, va);
+            radeon_emit(cs, va >> 32);
+            radeon_emit(cs, 0);
+            radeon_emit(cs, 6); /* 1x COPY_DATA size */
+         } else {
+            radeon_emit(cs, PKT3(PKT3_COND_EXEC, 2, 0));
+            radeon_emit(cs, va);
+            radeon_emit(cs, va >> 32);
+            radeon_emit(cs, 6); /* 1x COPY_DATA size */
+         }
 
          /* Write 0 to the new predication VA (when the API condition != 0) */
          radeon_emit(cs, PKT3(PKT3_COPY_DATA, 4, 0));
-         radeon_emit(cs,
-                     COPY_DATA_SRC_SEL(COPY_DATA_IMM) | COPY_DATA_DST_SEL(COPY_DATA_DST_MEM) | COPY_DATA_WR_CONFIRM);
+         radeon_emit(cs, COPY_DATA_SRC_SEL(COPY_DATA_IMM) | COPY_DATA_DST_SEL(COPY_DATA_DST_MEM) |
+                            COPY_DATA_WR_CONFIRM | (gfx_level == GFX6 ? COPY_DATA_ENGINE_PFP : 0));
          radeon_emit(cs, 0);
          radeon_emit(cs, 0);
          radeon_emit(cs, inv_va);
@@ -7979,11 +7997,18 @@ radv_cs_emit_compute_predication(struct radv_cmd_state *state, struct radeon_cmd
       va = inv_va;
    }
 
-   radeon_emit(cs, PKT3(PKT3_COND_EXEC, 3, 0));
-   radeon_emit(cs, va);
-   radeon_emit(cs, va >> 32);
-   radeon_emit(cs, 0);      /* Cache policy */
-   radeon_emit(cs, dwords); /* Size of the predicated packet(s) in DWORDs. */
+   if (device->physical_device->rad_info.gfx_level >= GFX7) {
+      radeon_emit(cs, PKT3(PKT3_COND_EXEC, 3, 0));
+      radeon_emit(cs, va);
+      radeon_emit(cs, va >> 32);
+      radeon_emit(cs, 0);      /* Cache policy */
+      radeon_emit(cs, dwords); /* Size of the predicated packet(s) in DWORDs. */
+   } else {
+      radeon_emit(cs, PKT3(PKT3_COND_EXEC, 2, 0));
+      radeon_emit(cs, va);
+      radeon_emit(cs, va >> 32);
+      radeon_emit(cs, dwords); /* Size of the predicated packet(s) in DWORDs. */
+   }
 }
 
 static void
@@ -8555,11 +8580,12 @@ radv_emit_direct_taskmesh_draw_packets(struct radv_cmd_buffer *cmd_buffer, uint3
 {
    const uint32_t view_mask = cmd_buffer->state.render.view_mask;
    const unsigned num_views = MAX2(1, util_bitcount(view_mask));
-   unsigned ace_predication_size = num_views * 6; /* DISPATCH_TASKMESH_DIRECT_ACE size */
+   const unsigned ace_predication_size = num_views * 6; /* DISPATCH_TASKMESH_DIRECT_ACE size */
 
    radv_emit_userdata_task(cmd_buffer, x, y, z, 0);
-   radv_cs_emit_compute_predication(&cmd_buffer->state, cmd_buffer->gang.cs, cmd_buffer->mec_inv_pred_va,
-                                    &cmd_buffer->mec_inv_pred_emitted, ace_predication_size);
+   radv_cs_emit_compute_predication(cmd_buffer->device, &cmd_buffer->state, cmd_buffer->gang.cs,
+                                    cmd_buffer->mec_inv_pred_va, &cmd_buffer->mec_inv_pred_emitted,
+                                    ace_predication_size);
 
    if (!view_mask) {
       radv_cs_emit_dispatch_taskmesh_direct_ace_packet(cmd_buffer, x, y, z);
@@ -8588,9 +8614,6 @@ radv_emit_indirect_taskmesh_draw_packets(struct radv_cmd_buffer *cmd_buffer, con
                                                       info->count_buffer->offset + info->count_buffer_offset;
    uint64_t workaround_cond_va = 0;
 
-   if (num_views > 1)
-      ace_predication_size += num_views * 3; /* SET_SH_REG size (view index SGPR) */
-
    if (count_va)
       radv_cs_add_buffer(ws, cmd_buffer->gang.cs, info->count_buffer->bo);
 
@@ -8628,8 +8651,9 @@ radv_emit_indirect_taskmesh_draw_packets(struct radv_cmd_buffer *cmd_buffer, con
    }
 
    radv_cs_add_buffer(ws, cmd_buffer->gang.cs, info->indirect->bo);
-   radv_cs_emit_compute_predication(&cmd_buffer->state, cmd_buffer->gang.cs, cmd_buffer->mec_inv_pred_va,
-                                    &cmd_buffer->mec_inv_pred_emitted, ace_predication_size);
+   radv_cs_emit_compute_predication(cmd_buffer->device, &cmd_buffer->state, cmd_buffer->gang.cs,
+                                    cmd_buffer->mec_inv_pred_va, &cmd_buffer->mec_inv_pred_emitted,
+                                    ace_predication_size);
 
    if (workaround_cond_va) {
       radeon_emit(ace_cs, PKT3(PKT3_COND_EXEC, 3, 0));
@@ -8985,7 +9009,8 @@ radv_emit_all_graphics_states(struct radv_cmd_buffer *cmd_buffer, const struct r
    if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_SHADER_QUERY)
       radv_flush_shader_query_state(cmd_buffer);
 
-   if (cmd_buffer->state.dirty & RADV_CMD_DIRTY_OCCLUSION_QUERY)
+   if (cmd_buffer->state.dirty & (RADV_CMD_DIRTY_OCCLUSION_QUERY | RADV_CMD_DIRTY_DYNAMIC_RASTERIZATION_SAMPLES |
+                                  RADV_CMD_DIRTY_DYNAMIC_PRIMITIVE_TOPOLOGY))
       radv_flush_occlusion_query_state(cmd_buffer);
 
    if ((cmd_buffer->state.dirty &
@@ -9665,12 +9690,16 @@ radv_emit_dispatch_packets(struct radv_cmd_buffer *cmd_buffer, const struct radv
 
       if (radv_cmd_buffer_uses_mec(cmd_buffer)) {
          uint64_t indirect_va = info->va;
+         const bool needs_align32_workaround =
+            cmd_buffer->device->physical_device->rad_info.has_async_compute_align32_bug &&
+            cmd_buffer->qf == RADV_QUEUE_COMPUTE && !radv_is_aligned(indirect_va, 32);
+         const unsigned ace_predication_size =
+            4 /* DISPATCH_INDIRECT */ + (needs_align32_workaround ? 6 * 3 /* 3x COPY_DATA */ : 0);
 
-         radv_cs_emit_compute_predication(&cmd_buffer->state, cs, cmd_buffer->mec_inv_pred_va,
-                                          &cmd_buffer->mec_inv_pred_emitted, 4 /* DISPATCH_INDIRECT size */);
+         radv_cs_emit_compute_predication(cmd_buffer->device, &cmd_buffer->state, cs, cmd_buffer->mec_inv_pred_va,
+                                          &cmd_buffer->mec_inv_pred_emitted, ace_predication_size);
 
-         if (cmd_buffer->device->physical_device->rad_info.has_async_compute_align32_bug &&
-             cmd_buffer->qf == RADV_QUEUE_COMPUTE && !radv_is_aligned(indirect_va, 32)) {
+         if (needs_align32_workaround) {
             const uint64_t unaligned_va = indirect_va;
             UNUSED void *ptr;
             uint32_t offset;
@@ -9704,6 +9733,13 @@ radv_emit_dispatch_packets(struct radv_cmd_buffer *cmd_buffer, const struct radv
          radeon_emit(cs, info->va);
          radeon_emit(cs, info->va >> 32);
 
+         if (cmd_buffer->qf == RADV_QUEUE_COMPUTE) {
+            radv_cs_emit_compute_predication(cmd_buffer->device, &cmd_buffer->state, cs,
+                                             cmd_buffer->mec_inv_pred_va, &cmd_buffer->mec_inv_pred_emitted,
+                                             3 /* PKT3_DISPATCH_INDIRECT */);
+            predicating = false;
+         }
+
          radeon_emit(cs, PKT3(PKT3_DISPATCH_INDIRECT, 1, predicating) | PKT3_SHADER_TYPE_S(1));
          radeon_emit(cs, 0);
          radeon_emit(cs, dispatch_initiator);
@@ -9772,8 +9808,8 @@ radv_emit_dispatch_packets(struct radv_cmd_buffer *cmd_buffer, const struct radv
          dispatch_initiator |= S_00B800_FORCE_START_AT_000(1);
       }
 
-      if (radv_cmd_buffer_uses_mec(cmd_buffer)) {
-         radv_cs_emit_compute_predication(&cmd_buffer->state, cs, cmd_buffer->mec_inv_pred_va,
+      if (cmd_buffer->qf == RADV_QUEUE_COMPUTE) {
+         radv_cs_emit_compute_predication(cmd_buffer->device, &cmd_buffer->state, cs, cmd_buffer->mec_inv_pred_va,
                                           &cmd_buffer->mec_inv_pred_emitted, 5 /* DISPATCH_DIRECT size */);
          predicating = false;
       }
@@ -9814,6 +9850,17 @@ radv_upload_compute_shader_descriptors(struct radv_cmd_buffer *cmd_buffer, VkPip
       radv_flush_constants(cmd_buffer, pc_stages, bind_point);
 }
 
+static void
+radv_emit_rt_stack_size(struct radv_cmd_buffer *cmd_buffer)
+{
+   unsigned rsrc2 = cmd_buffer->state.rt_prolog->config.rsrc2;
+   if (cmd_buffer->state.rt_stack_size)
+      rsrc2 |= S_00B12C_SCRATCH_EN(1);
+
+   radeon_check_space(cmd_buffer->device->ws, cmd_buffer->cs, 3);
+   radeon_set_sh_reg(cmd_buffer->cs, R_00B84C_COMPUTE_PGM_RSRC2, rsrc2);
+}
+
 static void
 radv_dispatch(struct radv_cmd_buffer *cmd_buffer, const struct radv_dispatch_info *info,
               struct radv_compute_pipeline *pipeline, struct radv_shader *compute_shader,
@@ -9835,6 +9882,8 @@ radv_dispatch(struct radv_cmd_buffer *cmd_buffer, const struct radv_dispatch_inf
        * packets between the wait and the draw)
        */
       radv_emit_compute_pipeline(cmd_buffer, pipeline);
+      if (bind_point == VK_PIPELINE_BIND_POINT_RAY_TRACING_KHR)
+         radv_emit_rt_stack_size(cmd_buffer);
       radv_emit_cache_flush(cmd_buffer);
       /* <-- CUs are idle here --> */
 
@@ -9863,6 +9912,8 @@ radv_dispatch(struct radv_cmd_buffer *cmd_buffer, const struct radv_dispatch_inf
       radv_upload_compute_shader_descriptors(cmd_buffer, bind_point);
 
       radv_emit_compute_pipeline(cmd_buffer, pipeline);
+      if (bind_point == VK_PIPELINE_BIND_POINT_RAY_TRACING_KHR)
+         radv_emit_rt_stack_size(cmd_buffer);
       radv_emit_dispatch_packets(cmd_buffer, compute_shader, info);
    }
 
@@ -10544,7 +10595,8 @@ radv_cp_dma_wait_for_stages(struct radv_cmd_buffer *cmd_buffer, VkPipelineStageF
 }
 
 static void
-radv_barrier(struct radv_cmd_buffer *cmd_buffer, const VkDependencyInfo *dep_info, enum rgp_barrier_reason reason)
+radv_barrier(struct radv_cmd_buffer *cmd_buffer, uint32_t dep_count, const VkDependencyInfo *dep_infos,
+             enum rgp_barrier_reason reason)
 {
    enum radv_cmd_flush_bits src_flush_bits = 0;
    enum radv_cmd_flush_bits dst_flush_bits = 0;
@@ -10556,27 +10608,31 @@ radv_barrier(struct radv_cmd_buffer *cmd_buffer, const VkDependencyInfo *dep_inf
 
    radv_describe_barrier_start(cmd_buffer, reason);
 
-   for (uint32_t i = 0; i < dep_info->memoryBarrierCount; i++) {
-      src_stage_mask |= dep_info->pMemoryBarriers[i].srcStageMask;
-      src_flush_bits |= radv_src_access_flush(cmd_buffer, dep_info->pMemoryBarriers[i].srcAccessMask, NULL);
-      dst_stage_mask |= dep_info->pMemoryBarriers[i].dstStageMask;
-      dst_flush_bits |= radv_dst_access_flush(cmd_buffer, dep_info->pMemoryBarriers[i].dstAccessMask, NULL);
-   }
+   for (uint32_t dep_idx = 0; dep_idx < dep_count; dep_idx++) {
+      const VkDependencyInfo *dep_info = &dep_infos[dep_idx];
 
-   for (uint32_t i = 0; i < dep_info->bufferMemoryBarrierCount; i++) {
-      src_stage_mask |= dep_info->pBufferMemoryBarriers[i].srcStageMask;
-      src_flush_bits |= radv_src_access_flush(cmd_buffer, dep_info->pBufferMemoryBarriers[i].srcAccessMask, NULL);
-      dst_stage_mask |= dep_info->pBufferMemoryBarriers[i].dstStageMask;
-      dst_flush_bits |= radv_dst_access_flush(cmd_buffer, dep_info->pBufferMemoryBarriers[i].dstAccessMask, NULL);
-   }
+      for (uint32_t i = 0; i < dep_info->memoryBarrierCount; i++) {
+         src_stage_mask |= dep_info->pMemoryBarriers[i].srcStageMask;
+         src_flush_bits |= radv_src_access_flush(cmd_buffer, dep_info->pMemoryBarriers[i].srcAccessMask, NULL);
+         dst_stage_mask |= dep_info->pMemoryBarriers[i].dstStageMask;
+         dst_flush_bits |= radv_dst_access_flush(cmd_buffer, dep_info->pMemoryBarriers[i].dstAccessMask, NULL);
+      }
 
-   for (uint32_t i = 0; i < dep_info->imageMemoryBarrierCount; i++) {
-      RADV_FROM_HANDLE(radv_image, image, dep_info->pImageMemoryBarriers[i].image);
+      for (uint32_t i = 0; i < dep_info->bufferMemoryBarrierCount; i++) {
+         src_stage_mask |= dep_info->pBufferMemoryBarriers[i].srcStageMask;
+         src_flush_bits |= radv_src_access_flush(cmd_buffer, dep_info->pBufferMemoryBarriers[i].srcAccessMask, NULL);
+         dst_stage_mask |= dep_info->pBufferMemoryBarriers[i].dstStageMask;
+         dst_flush_bits |= radv_dst_access_flush(cmd_buffer, dep_info->pBufferMemoryBarriers[i].dstAccessMask, NULL);
+      }
 
-      src_stage_mask |= dep_info->pImageMemoryBarriers[i].srcStageMask;
-      src_flush_bits |= radv_src_access_flush(cmd_buffer, dep_info->pImageMemoryBarriers[i].srcAccessMask, image);
-      dst_stage_mask |= dep_info->pImageMemoryBarriers[i].dstStageMask;
-      dst_flush_bits |= radv_dst_access_flush(cmd_buffer, dep_info->pImageMemoryBarriers[i].dstAccessMask, image);
+      for (uint32_t i = 0; i < dep_info->imageMemoryBarrierCount; i++) {
+         VK_FROM_HANDLE(radv_image, image, dep_info->pImageMemoryBarriers[i].image);
+
+         src_stage_mask |= dep_info->pImageMemoryBarriers[i].srcStageMask;
+         src_flush_bits |= radv_src_access_flush(cmd_buffer, dep_info->pImageMemoryBarriers[i].srcAccessMask, image);
+         dst_stage_mask |= dep_info->pImageMemoryBarriers[i].dstStageMask;
+         dst_flush_bits |= radv_dst_access_flush(cmd_buffer, dep_info->pImageMemoryBarriers[i].dstAccessMask, image);
+      }
    }
 
    /* The Vulkan spec 1.1.98 says:
@@ -10596,26 +10652,31 @@ radv_barrier(struct radv_cmd_buffer *cmd_buffer, const VkDependencyInfo *dep_inf
 
    radv_gang_barrier(cmd_buffer, src_stage_mask, 0);
 
-   for (uint32_t i = 0; i < dep_info->imageMemoryBarrierCount; i++) {
-      RADV_FROM_HANDLE(radv_image, image, dep_info->pImageMemoryBarriers[i].image);
+   for (uint32_t dep_idx = 0; dep_idx < dep_count; dep_idx++) {
+      const VkDependencyInfo *dep_info = &dep_infos[dep_idx];
 
-      const struct VkSampleLocationsInfoEXT *sample_locs_info =
-         vk_find_struct_const(dep_info->pImageMemoryBarriers[i].pNext, SAMPLE_LOCATIONS_INFO_EXT);
-      struct radv_sample_locations_state sample_locations;
+      for (uint32_t i = 0; i < dep_info->imageMemoryBarrierCount; i++) {
+         VK_FROM_HANDLE(radv_image, image, dep_info->pImageMemoryBarriers[i].image);
 
-      if (sample_locs_info) {
-         assert(image->vk.create_flags & VK_IMAGE_CREATE_SAMPLE_LOCATIONS_COMPATIBLE_DEPTH_BIT_EXT);
-         sample_locations.per_pixel = sample_locs_info->sampleLocationsPerPixel;
-         sample_locations.grid_size = sample_locs_info->sampleLocationGridSize;
-         sample_locations.count = sample_locs_info->sampleLocationsCount;
-         typed_memcpy(&sample_locations.locations[0], sample_locs_info->pSampleLocations,
-                      sample_locs_info->sampleLocationsCount);
-      }
+         const struct VkSampleLocationsInfoEXT *sample_locs_info =
+            vk_find_struct_const(dep_info->pImageMemoryBarriers[i].pNext, SAMPLE_LOCATIONS_INFO_EXT);
+         struct radv_sample_locations_state sample_locations;
+
+         if (sample_locs_info) {
+            assert(image->vk.create_flags & VK_IMAGE_CREATE_SAMPLE_LOCATIONS_COMPATIBLE_DEPTH_BIT_EXT);
+            sample_locations.per_pixel = sample_locs_info->sampleLocationsPerPixel;
+            sample_locations.grid_size = sample_locs_info->sampleLocationGridSize;
+            sample_locations.count = sample_locs_info->sampleLocationsCount;
+            typed_memcpy(&sample_locations.locations[0], sample_locs_info->pSampleLocations,
+                         sample_locs_info->sampleLocationsCount);
+         }
 
-      radv_handle_image_transition(
-         cmd_buffer, image, dep_info->pImageMemoryBarriers[i].oldLayout, dep_info->pImageMemoryBarriers[i].newLayout,
-         dep_info->pImageMemoryBarriers[i].srcQueueFamilyIndex, dep_info->pImageMemoryBarriers[i].dstQueueFamilyIndex,
-         &dep_info->pImageMemoryBarriers[i].subresourceRange, sample_locs_info ? &sample_locations : NULL);
+         radv_handle_image_transition(
+            cmd_buffer, image, dep_info->pImageMemoryBarriers[i].oldLayout, dep_info->pImageMemoryBarriers[i].newLayout,
+            dep_info->pImageMemoryBarriers[i].srcQueueFamilyIndex,
+            dep_info->pImageMemoryBarriers[i].dstQueueFamilyIndex, &dep_info->pImageMemoryBarriers[i].subresourceRange,
+            sample_locs_info ? &sample_locations : NULL);
+      }
    }
 
    radv_gang_barrier(cmd_buffer, 0, dst_stage_mask);
@@ -10642,8 +10703,15 @@ VKAPI_ATTR void VKAPI_CALL
 radv_CmdPipelineBarrier2(VkCommandBuffer commandBuffer, const VkDependencyInfo *pDependencyInfo)
 {
    RADV_FROM_HANDLE(radv_cmd_buffer, cmd_buffer, commandBuffer);
+   enum rgp_barrier_reason barrier_reason;
 
-   radv_barrier(cmd_buffer, pDependencyInfo, RGP_BARRIER_EXTERNAL_CMD_PIPELINE_BARRIER);
+   if (cmd_buffer->vk.runtime_rp_barrier) {
+      barrier_reason = RGP_BARRIER_EXTERNAL_RENDER_PASS_SYNC;
+   } else {
+      barrier_reason = RGP_BARRIER_EXTERNAL_CMD_PIPELINE_BARRIER;
+   }
+
+   radv_barrier(cmd_buffer, 1, pDependencyInfo, barrier_reason);
 }
 
 static void
@@ -10765,7 +10833,7 @@ radv_CmdWaitEvents2(VkCommandBuffer commandBuffer, uint32_t eventCount, const Vk
       assert(cmd_buffer->cs->cdw <= cdw_max);
    }
 
-   radv_barrier(cmd_buffer, pDependencyInfos, RGP_BARRIER_EXTERNAL_CMD_WAIT_EVENTS);
+   radv_barrier(cmd_buffer, eventCount, pDependencyInfos, RGP_BARRIER_EXTERNAL_CMD_WAIT_EVENTS);
 }
 
 void
@@ -10776,69 +10844,71 @@ radv_begin_conditional_rendering(struct radv_cmd_buffer *cmd_buffer, uint64_t va
 
    radv_emit_cache_flush(cmd_buffer);
 
-   if (cmd_buffer->qf == RADV_QUEUE_GENERAL && !cmd_buffer->device->physical_device->rad_info.has_32bit_predication) {
-      uint64_t pred_value = 0, pred_va;
-      unsigned pred_offset;
-
-      /* From the Vulkan spec 1.1.107:
-       *
-       * "If the 32-bit value at offset in buffer memory is zero,
-       *  then the rendering commands are discarded, otherwise they
-       *  are executed as normal. If the value of the predicate in
-       *  buffer memory changes while conditional rendering is
-       *  active, the rendering commands may be discarded in an
-       *  implementation-dependent way. Some implementations may
-       *  latch the value of the predicate upon beginning conditional
-       *  rendering while others may read it before every rendering
-       *  command."
-       *
-       * But, the AMD hardware treats the predicate as a 64-bit
-       * value which means we need a workaround in the driver.
-       * Luckily, it's not required to support if the value changes
-       * when predication is active.
-       *
-       * The workaround is as follows:
-       * 1) allocate a 64-value in the upload BO and initialize it
-       *    to 0
-       * 2) copy the 32-bit predicate value to the upload BO
-       * 3) use the new allocated VA address for predication
-       *
-       * Based on the conditionalrender demo, it's faster to do the
-       * COPY_DATA in ME  (+ sync PFP) instead of PFP.
-       */
-      radv_cmd_buffer_upload_data(cmd_buffer, 8, &pred_value, &pred_offset);
+   if (cmd_buffer->qf == RADV_QUEUE_GENERAL) {
+      if (!cmd_buffer->device->physical_device->rad_info.has_32bit_predication) {
+         uint64_t pred_value = 0, pred_va;
+         unsigned pred_offset;
+
+         /* From the Vulkan spec 1.1.107:
+          *
+          * "If the 32-bit value at offset in buffer memory is zero,
+          *  then the rendering commands are discarded, otherwise they
+          *  are executed as normal. If the value of the predicate in
+          *  buffer memory changes while conditional rendering is
+          *  active, the rendering commands may be discarded in an
+          *  implementation-dependent way. Some implementations may
+          *  latch the value of the predicate upon beginning conditional
+          *  rendering while others may read it before every rendering
+          *  command."
+          *
+          * But, the AMD hardware treats the predicate as a 64-bit
+          * value which means we need a workaround in the driver.
+          * Luckily, it's not required to support if the value changes
+          * when predication is active.
+          *
+          * The workaround is as follows:
+          * 1) allocate a 64-value in the upload BO and initialize it
+          *    to 0
+          * 2) copy the 32-bit predicate value to the upload BO
+          * 3) use the new allocated VA address for predication
+          *
+          * Based on the conditionalrender demo, it's faster to do the
+          * COPY_DATA in ME  (+ sync PFP) instead of PFP.
+          */
+         radv_cmd_buffer_upload_data(cmd_buffer, 8, &pred_value, &pred_offset);
 
-      pred_va = radv_buffer_get_va(cmd_buffer->upload.upload_bo) + pred_offset;
+         pred_va = radv_buffer_get_va(cmd_buffer->upload.upload_bo) + pred_offset;
 
-      radeon_check_space(cmd_buffer->device->ws, cmd_buffer->cs, 8);
+         radeon_check_space(cmd_buffer->device->ws, cmd_buffer->cs, 8);
 
-      radeon_emit(cs, PKT3(PKT3_COPY_DATA, 4, 0));
-      radeon_emit(cs,
-                  COPY_DATA_SRC_SEL(COPY_DATA_SRC_MEM) | COPY_DATA_DST_SEL(COPY_DATA_DST_MEM) | COPY_DATA_WR_CONFIRM);
-      radeon_emit(cs, va);
-      radeon_emit(cs, va >> 32);
-      radeon_emit(cs, pred_va);
-      radeon_emit(cs, pred_va >> 32);
+         radeon_emit(cs, PKT3(PKT3_COPY_DATA, 4, 0));
+         radeon_emit(
+            cs, COPY_DATA_SRC_SEL(COPY_DATA_SRC_MEM) | COPY_DATA_DST_SEL(COPY_DATA_DST_MEM) | COPY_DATA_WR_CONFIRM);
+         radeon_emit(cs, va);
+         radeon_emit(cs, va >> 32);
+         radeon_emit(cs, pred_va);
+         radeon_emit(cs, pred_va >> 32);
 
-      radeon_emit(cs, PKT3(PKT3_PFP_SYNC_ME, 0, 0));
-      radeon_emit(cs, 0);
+         radeon_emit(cs, PKT3(PKT3_PFP_SYNC_ME, 0, 0));
+         radeon_emit(cs, 0);
 
-      va = pred_va;
-      pred_op = PREDICATION_OP_BOOL64;
-   }
+         va = pred_va;
+         pred_op = PREDICATION_OP_BOOL64;
+      }
 
-   /* MEC doesn't support predication, we emulate it elsewhere. */
-   if (!radv_cmd_buffer_uses_mec(cmd_buffer)) {
       radv_emit_set_predication_state(cmd_buffer, draw_visible, pred_op, va);
+   } else {
+      /* Compute queue doesn't support predication and it's emulated elsewhere. */
    }
 }
 
 void
 radv_end_conditional_rendering(struct radv_cmd_buffer *cmd_buffer)
 {
-   /* MEC doesn't support predication, no need to emit anything here. */
-   if (!radv_cmd_buffer_uses_mec(cmd_buffer)) {
+   if (cmd_buffer->qf == RADV_QUEUE_GENERAL) {
       radv_emit_set_predication_state(cmd_buffer, false, 0, 0);
+   } else {
+      /* Compute queue doesn't support predication, no need to emit anything here. */
    }
 }
 
diff --git a/src/amd/vulkan/radv_debug.c b/src/amd/vulkan/radv_debug.c
index 34433384062..97322bbd5e9 100644
--- a/src/amd/vulkan/radv_debug.c
+++ b/src/amd/vulkan/radv_debug.c
@@ -1081,7 +1081,7 @@ radv_GetDeviceFaultInfoEXT(VkDevice _device, VkDeviceFaultCountsEXT *pFaultCount
 
    if (vm_fault_occurred) {
       VkDeviceFaultAddressInfoEXT addr_fault_info = {
-         .reportedAddress = fault_info.addr,
+         .reportedAddress = ((int64_t)fault_info.addr << 16) >> 16,
          .addressPrecision = 4096, /* 4K page granularity */
       };
 
diff --git a/src/amd/vulkan/radv_device.c b/src/amd/vulkan/radv_device.c
index 261cf4f6856..5c3b12bb3c1 100644
--- a/src/amd/vulkan/radv_device.c
+++ b/src/amd/vulkan/radv_device.c
@@ -1001,8 +1001,17 @@ radv_CreateDevice(VkPhysicalDevice physicalDevice, const VkDeviceCreateInfo *pCr
    device->pbb_allowed =
       device->physical_device->rad_info.gfx_level >= GFX9 && !(device->instance->debug_flags & RADV_DEBUG_NOBINNING);
 
-   device->mesh_fast_launch_2 = (device->instance->perftest_flags & RADV_PERFTEST_GS_FAST_LAUNCH_2) &&
-                                device->physical_device->rad_info.gfx_level >= GFX11;
+   /* GS_FAST_LAUNCH=2 mode is supposed to be used on GFX11 but it turns
+    * out it has severe impact on performance for unknown reasons (tested on
+    * NAVI31 dGPU). It's disabled by default.
+    *
+    * On RDNA3 APUs (Phoenix) it turns GS_FAST_LAUNCH=1 doesn't work at all,
+    * and using mode2 fixes everything without any performance impact.
+    */
+   device->mesh_fast_launch_2 = ((device->instance->perftest_flags & RADV_PERFTEST_GS_FAST_LAUNCH_2) &&
+                                 device->physical_device->rad_info.gfx_level >= GFX11) ||
+                                device->physical_device->rad_info.family == CHIP_GFX1103_R1 ||
+                                device->physical_device->rad_info.family == CHIP_GFX1103_R2;
 
    device->disable_trunc_coord = device->instance->drirc.disable_trunc_coord;
 
@@ -1239,6 +1248,12 @@ radv_CreateDevice(VkPhysicalDevice physicalDevice, const VkDeviceCreateInfo *pCr
       device->capture_replay_arena_vas = _mesa_hash_table_u64_create(NULL);
    }
 
+   if (device->physical_device->rad_info.gfx_level == GFX11 && device->physical_device->rad_info.has_dedicated_vram &&
+       device->instance->drirc.force_pstate_peak_gfx11_dgpu) {
+      if (!radv_device_acquire_performance_counters(device))
+         fprintf(stderr, "radv: failed to set pstate to profile_peak.\n");
+   }
+
    *pDevice = radv_device_to_handle(device);
    return VK_SUCCESS;
 
@@ -1305,9 +1320,6 @@ radv_DestroyDevice(VkDevice _device, const VkAllocationCallbacks *pAllocator)
    if (!device)
       return;
 
-   if (device->capture_replay_arena_vas)
-      _mesa_hash_table_u64_destroy(device->capture_replay_arena_vas);
-
    radv_device_finish_perf_counter_lock_cs(device);
    if (device->perf_counter_bo)
       device->ws->buffer_destroy(device->ws, device->perf_counter_bo);
@@ -1357,6 +1369,8 @@ radv_DestroyDevice(VkDevice _device, const VkAllocationCallbacks *pAllocator)
    radv_finish_trace(device);
 
    radv_destroy_shader_arenas(device);
+   if (device->capture_replay_arena_vas)
+      _mesa_hash_table_u64_destroy(device->capture_replay_arena_vas);
 
    radv_sqtt_finish(device);
 
@@ -1763,7 +1777,7 @@ radv_initialise_color_surface(struct radv_device *device, struct radv_color_buff
        *
        * We set the pitch in MIP0_WIDTH.
        */
-      if (device->physical_device->rad_info.gfx_level && iview->image->vk.image_type == VK_IMAGE_TYPE_2D &&
+      if (device->physical_device->rad_info.gfx_level >= GFX10_3 && iview->image->vk.image_type == VK_IMAGE_TYPE_2D &&
           iview->image->vk.array_layers == 1 && plane->surface.is_linear) {
          assert((plane->surface.u.gfx9.surf_pitch * plane->surface.bpe) % 256 == 0);
 
diff --git a/src/amd/vulkan/radv_device_generated_commands.c b/src/amd/vulkan/radv_device_generated_commands.c
index 486be831fa8..4b8b3d11d68 100644
--- a/src/amd/vulkan/radv_device_generated_commands.c
+++ b/src/amd/vulkan/radv_device_generated_commands.c
@@ -589,7 +589,7 @@ dgc_emit_pkt3_draw_indirect(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *vtx_
 
 static void
 dgc_emit_draw_indirect(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *stream_base, nir_def *draw_params_offset,
-                       bool indexed)
+                       nir_def *sequence_id, bool indexed)
 {
    nir_def *vtx_base_sgpr = load_param16(b, vtx_base_sgpr);
    nir_def *stream_offset = nir_iadd(b, draw_params_offset, stream_base);
@@ -597,8 +597,14 @@ dgc_emit_draw_indirect(nir_builder *b, struct dgc_cmdbuf *cs, nir_def *stream_ba
    nir_def *stream_addr = load_param64(b, stream_addr);
    nir_def *va = nir_iadd(b, stream_addr, nir_u2u64(b, stream_offset));
 
+   dgc_emit_sqtt_begin_api_marker(b, cs, indexed ? ApiCmdDrawIndexedIndirect : ApiCmdDrawIndirect);
+   dgc_emit_sqtt_marker_event(b, cs, sequence_id, indexed ? EventCmdDrawIndexedIndirect : EventCmdDrawIndirect);
+
    dgc_emit_pkt3_set_base(b, cs, va);
    dgc_emit_pkt3_draw_indirect(b, cs, vtx_base_sgpr, indexed);
+
+   dgc_emit_sqtt_thread_trace_marker(b, cs);
+   dgc_emit_sqtt_end_api_marker(b, cs, indexed ? ApiCmdDrawIndexedIndirect : ApiCmdDrawIndirect);
 }
 
 static nir_def *
@@ -1307,7 +1313,8 @@ build_dgc_prepare_shader(struct radv_device *dev)
             }
             nir_push_else(&b, NULL);
             {
-               dgc_emit_draw_indirect(&b, &cmd_buf, stream_base, load_param16(&b, draw_params_offset), true);
+               dgc_emit_draw_indirect(&b, &cmd_buf, stream_base, load_param16(&b, draw_params_offset), sequence_id,
+                                      true);
             }
 
             nir_pop_if(&b, NULL);
diff --git a/src/amd/vulkan/radv_formats.c b/src/amd/vulkan/radv_formats.c
index 7ed01f2aae0..d0ebd5dd2f4 100644
--- a/src/amd/vulkan/radv_formats.c
+++ b/src/amd/vulkan/radv_formats.c
@@ -181,7 +181,8 @@ radv_is_vertex_buffer_format_supported(VkFormat format)
 }
 
 uint32_t
-radv_translate_tex_dataformat(VkFormat format, const struct util_format_description *desc, int first_non_void)
+radv_translate_tex_dataformat(const struct radv_physical_device *pdev, VkFormat format,
+                              const struct util_format_description *desc, int first_non_void)
 {
    bool uniform = true;
    int i;
@@ -311,6 +312,9 @@ radv_translate_tex_dataformat(VkFormat format, const struct util_format_descript
       uniform = uniform && desc->channel[0].size == desc->channel[i].size;
    }
 
+   if (first_non_void < 0 || first_non_void > 3)
+      goto out_unknown;
+
    /* Non-uniform formats. */
    if (!uniform) {
       switch (desc->nr_channels) {
@@ -320,6 +324,12 @@ radv_translate_tex_dataformat(VkFormat format, const struct util_format_descript
          }
          goto out_unknown;
       case 4:
+         /* 5551 and 1555 UINT formats fail on Gfx8/Carrizo´. */
+         if (pdev->rad_info.family == CHIP_CARRIZO && desc->channel[1].size == 5 && desc->channel[2].size == 5 &&
+             desc->channel[first_non_void].type == UTIL_FORMAT_TYPE_UNSIGNED &&
+             desc->channel[first_non_void].pure_integer)
+            goto out_unknown;
+
          if (desc->channel[0].size == 5 && desc->channel[1].size == 5 && desc->channel[2].size == 5 &&
              desc->channel[3].size == 1) {
             return V_008F14_IMG_DATA_FORMAT_1_5_5_5;
@@ -340,9 +350,6 @@ radv_translate_tex_dataformat(VkFormat format, const struct util_format_descript
       goto out_unknown;
    }
 
-   if (first_non_void < 0 || first_non_void > 3)
-      goto out_unknown;
-
    /* uniform formats */
    switch (desc->channel[first_non_void].size) {
    case 4:
@@ -352,6 +359,11 @@ radv_translate_tex_dataformat(VkFormat format, const struct util_format_descript
 			return V_008F14_IMG_DATA_FORMAT_4_4;
 #endif
       case 4:
+         /* 4444 UINT formats fail on Gfx8/Carrizo´. */
+         if (pdev->rad_info.family == CHIP_CARRIZO && desc->channel[first_non_void].type == UTIL_FORMAT_TYPE_UNSIGNED &&
+             desc->channel[first_non_void].pure_integer)
+            goto out_unknown;
+
          return V_008F14_IMG_DATA_FORMAT_4_4_4_4;
       }
       break;
@@ -461,7 +473,7 @@ radv_translate_tex_numformat(VkFormat format, const struct util_format_descripti
 }
 
 static bool
-radv_is_sampler_format_supported(VkFormat format, bool *linear_sampling)
+radv_is_sampler_format_supported(const struct radv_physical_device *pdev, VkFormat format, bool *linear_sampling)
 {
    const struct util_format_description *desc = vk_format_description(format);
    uint32_t num_format;
@@ -477,7 +489,7 @@ radv_is_sampler_format_supported(VkFormat format, bool *linear_sampling)
       *linear_sampling = true;
    else
       *linear_sampling = false;
-   return radv_translate_tex_dataformat(format, vk_format_description(format),
+   return radv_translate_tex_dataformat(pdev, format, vk_format_description(format),
                                         vk_format_get_first_non_void_channel(format)) != ~0U;
 }
 
@@ -499,7 +511,7 @@ radv_is_storage_image_format_supported(const struct radv_physical_device *physic
    if (vk_format_is_depth_or_stencil(format))
       return false;
 
-   data_format = radv_translate_tex_dataformat(format, desc, vk_format_get_first_non_void_channel(format));
+   data_format = radv_translate_tex_dataformat(physical_device, format, desc, vk_format_get_first_non_void_channel(format));
    num_format = radv_translate_tex_numformat(format, desc, vk_format_get_first_non_void_channel(format));
 
    if (data_format == ~0 || num_format == ~0)
@@ -736,7 +748,7 @@ radv_physical_device_get_format_properties(struct radv_physical_device *physical
       }
    } else {
       bool linear_sampling;
-      if (radv_is_sampler_format_supported(format, &linear_sampling)) {
+      if (radv_is_sampler_format_supported(physical_device, format, &linear_sampling)) {
          linear |= VK_FORMAT_FEATURE_2_SAMPLED_IMAGE_BIT | VK_FORMAT_FEATURE_2_BLIT_SRC_BIT;
          tiled |= VK_FORMAT_FEATURE_2_SAMPLED_IMAGE_BIT | VK_FORMAT_FEATURE_2_BLIT_SRC_BIT;
 
@@ -1480,6 +1492,11 @@ radv_get_image_format_properties(struct radv_physical_device *physical_device,
       }
    }
 
+   if (image_usage & VK_IMAGE_USAGE_FRAGMENT_SHADING_RATE_ATTACHMENT_BIT_KHR) {
+      if (!(format_feature_flags & VK_FORMAT_FEATURE_2_FRAGMENT_SHADING_RATE_ATTACHMENT_BIT_KHR))
+         goto unsupported;
+   }
+
    /* Sparse resources with multi-planar formats are unsupported. */
    if (info->flags & VK_IMAGE_CREATE_SPARSE_BINDING_BIT) {
       if (vk_format_get_plane_count(format) > 1)
diff --git a/src/amd/vulkan/radv_image_view.c b/src/amd/vulkan/radv_image_view.c
index a8e67016ec8..5ec9a92d0af 100644
--- a/src/amd/vulkan/radv_image_view.c
+++ b/src/amd/vulkan/radv_image_view.c
@@ -119,6 +119,13 @@ radv_set_mutable_tex_desc_fields(struct radv_device *device, struct radv_image *
    } else
       va += (uint64_t)base_level_info->offset_256B * 256;
 
+   if (!device->physical_device->rad_info.has_image_opcodes) {
+      /* Set it as a buffer descriptor. */
+      state[0] = va;
+      state[1] |= S_008F04_BASE_ADDRESS_HI(va >> 32);
+      return;
+   }
+
    state[0] = va >> 8;
    if (gfx_level >= GFX9 || base_level_info->mode == RADEON_SURF_MODE_2D)
       state[0] |= swizzle;
@@ -244,6 +251,8 @@ gfx10_make_texture_descriptor(struct radv_device *device, struct radv_image *ima
                               uint32_t *fmask_state, VkImageCreateFlags img_create_flags,
                               const struct ac_surf_nbc_view *nbc_view, const VkImageViewSlicedCreateInfoEXT *sliced_3d)
 {
+   const bool create_2d_view_of_3d =
+      (img_create_flags & VK_IMAGE_CREATE_2D_VIEW_COMPATIBLE_BIT_EXT) && view_type == VK_IMAGE_VIEW_TYPE_2D;
    const struct util_format_description *desc;
    enum pipe_swizzle swizzle[4];
    unsigned img_format;
@@ -265,7 +274,7 @@ gfx10_make_texture_descriptor(struct radv_device *device, struct radv_image *ima
 
    radv_compose_swizzle(desc, mapping, swizzle);
 
-   if (img_create_flags & VK_IMAGE_CREATE_2D_VIEW_COMPATIBLE_BIT_EXT) {
+   if (create_2d_view_of_3d) {
       assert(image->vk.image_type == VK_IMAGE_TYPE_3D);
       type = V_008F1C_SQ_RSRC_IMG_3D;
    } else {
@@ -300,7 +309,7 @@ gfx10_make_texture_descriptor(struct radv_device *device, struct radv_image *ima
    state[6] = 0;
    state[7] = 0;
 
-   if (img_create_flags & VK_IMAGE_CREATE_2D_VIEW_COMPATIBLE_BIT_EXT) {
+   if (create_2d_view_of_3d) {
       assert(type == V_008F1C_SQ_RSRC_IMG_3D);
 
       /* ARRAY_PITCH is only meaningful for 3D images, 0 means SRV, 1 means UAV.
@@ -412,6 +421,8 @@ gfx6_make_texture_descriptor(struct radv_device *device, struct radv_image *imag
                              unsigned width, unsigned height, unsigned depth, float min_lod, uint32_t *state,
                              uint32_t *fmask_state, VkImageCreateFlags img_create_flags)
 {
+   const bool create_2d_view_of_3d =
+      (img_create_flags & VK_IMAGE_CREATE_2D_VIEW_COMPATIBLE_BIT_EXT) && view_type == VK_IMAGE_VIEW_TYPE_2D;
    const struct util_format_description *desc;
    enum pipe_swizzle swizzle[4];
    int first_non_void;
@@ -437,7 +448,7 @@ gfx6_make_texture_descriptor(struct radv_device *device, struct radv_image *imag
       num_format = 0;
    }
 
-   data_format = radv_translate_tex_dataformat(vk_format, desc, first_non_void);
+   data_format = radv_translate_tex_dataformat(device->physical_device, vk_format, desc, first_non_void);
    if (data_format == ~0) {
       data_format = 0;
    }
@@ -451,8 +462,7 @@ gfx6_make_texture_descriptor(struct radv_device *device, struct radv_image *imag
          data_format = V_008F14_IMG_DATA_FORMAT_S8_16;
    }
 
-   if (device->physical_device->rad_info.gfx_level == GFX9 &&
-       img_create_flags & VK_IMAGE_CREATE_2D_VIEW_COMPATIBLE_BIT_EXT) {
+   if (device->physical_device->rad_info.gfx_level == GFX9 && create_2d_view_of_3d) {
       assert(image->vk.image_type == VK_IMAGE_TYPE_3D);
       type = V_008F1C_SQ_RSRC_IMG_3D;
    } else {
@@ -501,7 +511,8 @@ gfx6_make_texture_descriptor(struct radv_device *device, struct radv_image *imag
       state[4] |= S_008F20_DEPTH(depth - 1);
       state[5] |= S_008F24_LAST_ARRAY(last_layer);
    }
-   if (!(image->planes[0].surface.flags & RADEON_SURF_Z_OR_SBUFFER) && image->planes[0].surface.meta_offset) {
+
+   if (radv_dcc_enabled(image, first_level)) {
       state[6] = S_008F28_ALPHA_IS_ON_MSB(vi_alpha_is_on_msb(device, vk_format));
    } else {
       if (device->instance->drirc.disable_aniso_single_level) {
diff --git a/src/amd/vulkan/radv_instance.c b/src/amd/vulkan/radv_instance.c
index 6d0ba625385..652013bd250 100644
--- a/src/amd/vulkan/radv_instance.c
+++ b/src/amd/vulkan/radv_instance.c
@@ -155,6 +155,7 @@ static const driOptionDescription radv_dri_options[] = {
       DRI_CONF_RADV_FLUSH_BEFORE_TIMESTAMP_WRITE(false)
       DRI_CONF_RADV_RT_WAVE64(false)
       DRI_CONF_RADV_LEGACY_SPARSE_BINDING(false)
+      DRI_CONF_RADV_FORCE_PSTATE_PEAK_GFX11_DGPU(false)
       DRI_CONF_DUAL_COLOR_BLEND_BY_LOCATION(false)
       DRI_CONF_RADV_OVERRIDE_GRAPHICS_SHADER_VERSION(0)
       DRI_CONF_RADV_OVERRIDE_COMPUTE_SHADER_VERSION(0)
@@ -231,6 +232,9 @@ radv_init_dri_options(struct radv_instance *instance)
 
    instance->drirc.legacy_sparse_binding = driQueryOptionb(&instance->drirc.options, "radv_legacy_sparse_binding");
 
+   instance->drirc.force_pstate_peak_gfx11_dgpu =
+      driQueryOptionb(&instance->drirc.options, "radv_force_pstate_peak_gfx11_dgpu");
+
    instance->drirc.override_graphics_shader_version =
       driQueryOptioni(&instance->drirc.options, "radv_override_graphics_shader_version");
    instance->drirc.override_compute_shader_version =
diff --git a/src/amd/vulkan/radv_nir_to_llvm.c b/src/amd/vulkan/radv_nir_to_llvm.c
index 155fd45a0a7..9422dae3840 100644
--- a/src/amd/vulkan/radv_nir_to_llvm.c
+++ b/src/amd/vulkan/radv_nir_to_llvm.c
@@ -262,7 +262,6 @@ static void
 ac_llvm_finalize_module(struct radv_shader_context *ctx, LLVMPassManagerRef passmgr)
 {
    LLVMRunPassManager(passmgr, ctx->ac.module);
-   LLVMDisposeBuilder(ctx->ac.builder);
 
    ac_llvm_context_dispose(&ctx->ac);
 }
diff --git a/src/amd/vulkan/radv_physical_device.c b/src/amd/vulkan/radv_physical_device.c
index c9e276efa71..793bb09bb30 100644
--- a/src/amd/vulkan/radv_physical_device.c
+++ b/src/amd/vulkan/radv_physical_device.c
@@ -2380,6 +2380,11 @@ radv_get_memory_budget_properties(VkPhysicalDevice physicalDevice,
       assert(heap == memory_properties->memoryHeapCount);
    }
 
+   /* The heapBudget value must be less than or equal to VkMemoryHeap::size for each heap. */
+   for (uint32_t i = 0; i < memory_properties->memoryHeapCount; i++) {
+      memoryBudget->heapBudget[i] = MIN2(memory_properties->memoryHeaps[i].size, memoryBudget->heapBudget[i]);
+   }
+
    /* The heapBudget and heapUsage values must be zero for array elements
     * greater than or equal to
     * VkPhysicalDeviceMemoryProperties::memoryHeapCount.
diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index 2699eaa792f..027e73c44aa 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -265,11 +265,8 @@ radv_shader_layout_init(const struct radv_pipeline_layout *pipeline_layout, gl_s
    }
 
    layout->push_constant_size = pipeline_layout->push_constant_size;
-
-   if (pipeline_layout->dynamic_offset_count &&
-       (pipeline_layout->dynamic_shader_stages & mesa_to_vk_shader_stage(stage))) {
-      layout->use_dynamic_descriptors = true;
-   }
+   layout->use_dynamic_descriptors = pipeline_layout->dynamic_offset_count &&
+                                     (pipeline_layout->dynamic_shader_stages & mesa_to_vk_shader_stage(stage));
 }
 
 static const struct vk_ycbcr_conversion_state *
@@ -704,7 +701,7 @@ radv_postprocess_nir(struct radv_device *device, const struct radv_pipeline_key
          },
       };
       struct nir_fold_16bit_tex_image_options fold_16bit_options = {
-         .rounding_mode = nir_rounding_mode_rtz,
+         .rounding_mode = nir_rounding_mode_undef,
          .fold_tex_dest_types = nir_type_float,
          .fold_image_dest_types = nir_type_float,
          .fold_image_store_data = true,
diff --git a/src/amd/vulkan/radv_pipeline_graphics.c b/src/amd/vulkan/radv_pipeline_graphics.c
index bdb02754d46..205ec375b1c 100644
--- a/src/amd/vulkan/radv_pipeline_graphics.c
+++ b/src/amd/vulkan/radv_pipeline_graphics.c
@@ -1178,11 +1178,12 @@ get_vs_output_info(const struct radv_graphics_pipeline *pipeline)
 static bool
 radv_should_export_multiview(const struct radv_shader_stage *stage, const struct radv_pipeline_key *pipeline_key)
 {
-   /* Export the layer in the last VGT stage if multiview is used. When the next stage is unknown
-    * (with graphics pipeline library), the layer is exported unconditionally.
+   /* Export the layer in the last VGT stage if multiview is used.
+    * Also checks for NONE stage, which happens when we have depth-only rendering.
+    * When the next stage is unknown (with graphics pipeline library), the layer is exported unconditionally.
     */
    return pipeline_key->has_multiview_view_index &&
-          (stage->info.next_stage == MESA_SHADER_FRAGMENT ||
+          (stage->info.next_stage == MESA_SHADER_FRAGMENT || stage->info.next_stage == MESA_SHADER_NONE ||
            !(pipeline_key->lib_flags & VK_GRAPHICS_PIPELINE_LIBRARY_FRAGMENT_SHADER_BIT_EXT)) &&
           !(stage->nir->info.outputs_written & VARYING_BIT_LAYER);
 }
@@ -2596,7 +2597,7 @@ radv_graphics_pipeline_compile(struct radv_graphics_pipeline *pipeline, const Vk
    unsigned char hash[20];
    bool keep_executable_info = radv_pipeline_capture_shaders(device, pipeline->base.create_flags);
    bool keep_statistic_info = radv_pipeline_capture_shader_stats(device, pipeline->base.create_flags);
-   struct radv_shader_stage stages[MESA_VULKAN_SHADER_STAGES];
+   struct radv_shader_stage *stages = malloc(sizeof(struct radv_shader_stage) * MESA_VULKAN_SHADER_STAGES);
    const VkPipelineCreationFeedbackCreateInfo *creation_feedback =
       vk_find_struct_const(pCreateInfo->pNext, PIPELINE_CREATION_FEEDBACK_CREATE_INFO);
    VkPipelineCreationFeedback pipeline_feedback = {
@@ -2608,6 +2609,9 @@ radv_graphics_pipeline_compile(struct radv_graphics_pipeline *pipeline, const Vk
       !!(pipeline->base.create_flags & VK_PIPELINE_CREATE_2_RETAIN_LINK_TIME_OPTIMIZATION_INFO_BIT_EXT);
    struct radv_retained_shaders *retained_shaders = NULL;
 
+   if (!stages)
+      return VK_ERROR_OUT_OF_HOST_MEMORY;
+
    int64_t pipeline_start = os_time_get_nano();
 
    for (unsigned i = 0; i < MESA_VULKAN_SHADER_STAGES; i++) {
@@ -2668,8 +2672,10 @@ radv_graphics_pipeline_compile(struct radv_graphics_pipeline *pipeline, const Vk
 
          gfx_pipeline_lib->stages = radv_copy_shader_stage_create_info(device, pCreateInfo->stageCount,
                                                                        pCreateInfo->pStages, gfx_pipeline_lib->mem_ctx);
-         if (!gfx_pipeline_lib->stages)
+         if (!gfx_pipeline_lib->stages) {
+            free(stages);
             return VK_ERROR_OUT_OF_HOST_MEMORY;
+         }
 
          gfx_pipeline_lib->stage_count = pCreateInfo->stageCount;
       }
@@ -2678,8 +2684,10 @@ radv_graphics_pipeline_compile(struct radv_graphics_pipeline *pipeline, const Vk
       goto done;
    }
 
-   if (pipeline->base.create_flags & VK_PIPELINE_CREATE_2_FAIL_ON_PIPELINE_COMPILE_REQUIRED_BIT_KHR)
+   if (pipeline->base.create_flags & VK_PIPELINE_CREATE_2_FAIL_ON_PIPELINE_COMPILE_REQUIRED_BIT_KHR) {
+      free(stages);
       return VK_PIPELINE_COMPILE_REQUIRED;
+   }
 
    if (retain_shaders) {
       struct radv_graphics_lib_pipeline *gfx_pipeline_lib = radv_pipeline_to_graphics_lib(&pipeline->base);
@@ -2747,6 +2755,7 @@ done:
       }
    }
 
+   free(stages);
    return result;
 }
 
diff --git a/src/amd/vulkan/radv_pipeline_rt.c b/src/amd/vulkan/radv_pipeline_rt.c
index b2f6ccf2a54..36fce6d53b3 100644
--- a/src/amd/vulkan/radv_pipeline_rt.c
+++ b/src/amd/vulkan/radv_pipeline_rt.c
@@ -193,7 +193,8 @@ radv_rt_fill_group_info(struct radv_device *device, const struct radv_ray_tracin
          } else if (groups[idx].recursive_shader != VK_SHADER_UNUSED_KHR) {
             struct radv_shader *library_shader = stages[groups[idx].recursive_shader].shader;
             simple_mtx_lock(&library_shader->replay_mtx);
-            if (!library_shader->has_replay_alloc) {
+            /* If arena_va is 0, the pipeline is monolithic and the shader was inlined into raygen */
+            if (!library_shader->has_replay_alloc && handle->recursive_shader_alloc.arena_va) {
                union radv_shader_arena_block *new_block =
                   radv_replay_shader_arena_block(device, &handle->recursive_shader_alloc, library_shader);
                if (!new_block) {
@@ -535,7 +536,7 @@ radv_rt_compile_shaders(struct radv_device *device, struct vk_pipeline_cache *ca
       has_callable |= rt_stages[i].stage == MESA_SHADER_CALLABLE;
       monolithic &= rt_stages[i].can_inline;
 
-      if (i > pCreateInfo->stageCount)
+      if (i >= pCreateInfo->stageCount)
          raygen_imported |= rt_stages[i].stage == MESA_SHADER_RAYGEN;
    }
 
@@ -942,7 +943,12 @@ radv_GetRayTracingCaptureReplayShaderGroupHandlesKHR(VkDevice device, VkPipeline
       uint32_t recursive_shader = rt_pipeline->groups[firstGroup + i].recursive_shader;
       if (recursive_shader != VK_SHADER_UNUSED_KHR) {
          struct radv_shader *shader = rt_pipeline->stages[recursive_shader].shader;
-         data[i].recursive_shader_alloc = radv_serialize_shader_arena_block(shader->alloc);
+         if (shader) {
+            data[i].recursive_shader_alloc.offset = shader->alloc->offset;
+            data[i].recursive_shader_alloc.size = shader->alloc->size;
+            data[i].recursive_shader_alloc.arena_va = shader->alloc->arena->bo->va;
+            data[i].recursive_shader_alloc.arena_size = shader->alloc->arena->size;
+         }
       }
       data[i].non_recursive_idx = rt_pipeline->groups[firstGroup + i].handle.any_hit_index;
    }
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 3e31701abc2..310091406b9 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -380,6 +380,7 @@ struct radv_instance {
       bool force_rt_wave64;
       bool dual_color_blend_by_location;
       bool legacy_sparse_binding;
+      bool force_pstate_peak_gfx11_dgpu;
       bool clear_lds;
       bool enable_dgc;
       bool enable_khr_present_wait;
@@ -2353,7 +2354,8 @@ bool radv_is_buffer_format_supported(VkFormat format, bool *scaled);
 uint32_t radv_colorformat_endian_swap(uint32_t colorformat);
 unsigned radv_translate_colorswap(VkFormat format, bool do_endian_swap);
 uint32_t radv_translate_dbformat(VkFormat format);
-uint32_t radv_translate_tex_dataformat(VkFormat format, const struct util_format_description *desc, int first_non_void);
+uint32_t radv_translate_tex_dataformat(const struct radv_physical_device *pdev, VkFormat format,
+                                       const struct util_format_description *desc, int first_non_void);
 uint32_t radv_translate_tex_numformat(VkFormat format, const struct util_format_description *desc, int first_non_void);
 bool radv_format_pack_clear_color(VkFormat format, uint32_t clear_vals[2], VkClearColorValue *value);
 bool radv_is_storage_image_format_supported(const struct radv_physical_device *physical_device, VkFormat format);
diff --git a/src/amd/vulkan/radv_query.c b/src/amd/vulkan/radv_query.c
index 0a215ba761d..bbd3ba6dbce 100644
--- a/src/amd/vulkan/radv_query.c
+++ b/src/amd/vulkan/radv_query.c
@@ -61,6 +61,16 @@ radv_get_pipelinestat_query_size(struct radv_device *device)
    return num_results * 8;
 }
 
+static bool
+radv_occlusion_query_use_l2(const struct radv_physical_device *pdev)
+{
+   /* Occlusion query writes don't go through L2 on GFX6-8 which means the driver would need to
+    * flush caches before every read in shaders or use MTYPE=3 (ie. uncached) in the buffer
+    * descriptor to bypass L2. Use the WAIT_REG_MEM logic instead which is easier to implement.
+    */
+   return pdev->rad_info.gfx_level >= GFX9;
+}
+
 static void
 radv_store_availability(nir_builder *b, nir_def *flags, nir_def *dst_buf, nir_def *offset, nir_def *value32)
 {
@@ -148,29 +158,31 @@ build_occlusion_query_shader(struct radv_device *device)
    nir_store_var(&b, outer_counter, nir_imm_int(&b, 0), 0x1);
    nir_store_var(&b, available, nir_imm_true(&b), 0x1);
 
-   nir_def *query_result_wait = nir_test_mask(&b, flags, VK_QUERY_RESULT_WAIT_BIT);
-   nir_push_if(&b, query_result_wait);
-   {
-      /* Wait on the upper word of the last DB entry. */
-      nir_push_loop(&b);
+   if (radv_occlusion_query_use_l2(device->physical_device)) {
+      nir_def *query_result_wait = nir_test_mask(&b, flags, VK_QUERY_RESULT_WAIT_BIT);
+      nir_push_if(&b, query_result_wait);
       {
-         const uint32_t rb_avail_offset = 16 * util_last_bit64(enabled_rb_mask) - 4;
+         /* Wait on the upper word of the last DB entry. */
+         nir_push_loop(&b);
+         {
+            const uint32_t rb_avail_offset = 16 * util_last_bit64(enabled_rb_mask) - 4;
 
-         /* Prevent the SSBO load to be moved out of the loop. */
-         nir_scoped_memory_barrier(&b, SCOPE_INVOCATION, NIR_MEMORY_ACQUIRE, nir_var_mem_ssbo);
+            /* Prevent the SSBO load to be moved out of the loop. */
+            nir_scoped_memory_barrier(&b, SCOPE_INVOCATION, NIR_MEMORY_ACQUIRE, nir_var_mem_ssbo);
 
-         nir_def *load_offset = nir_iadd_imm(&b, input_base, rb_avail_offset);
-         nir_def *load = nir_load_ssbo(&b, 1, 32, src_buf, load_offset, .align_mul = 4, .access = ACCESS_COHERENT);
+            nir_def *load_offset = nir_iadd_imm(&b, input_base, rb_avail_offset);
+            nir_def *load = nir_load_ssbo(&b, 1, 32, src_buf, load_offset, .align_mul = 4, .access = ACCESS_COHERENT);
 
-         nir_push_if(&b, nir_ige_imm(&b, load, 0x80000000));
-         {
-            nir_jump(&b, nir_jump_break);
+            nir_push_if(&b, nir_ige_imm(&b, load, 0x80000000));
+            {
+               nir_jump(&b, nir_jump_break);
+            }
+            nir_pop_if(&b, NULL);
          }
-         nir_pop_if(&b, NULL);
+         nir_pop_loop(&b, NULL);
       }
-      nir_pop_loop(&b, NULL);
+      nir_pop_if(&b, NULL);
    }
-   nir_pop_if(&b, NULL);
 
    nir_push_loop(&b);
 
@@ -1155,7 +1167,7 @@ radv_query_shader(struct radv_cmd_buffer *cmd_buffer, VkPipeline *pipeline, stru
                                                                          .range = VK_WHOLE_SIZE}}});
 
    /* Encode the number of elements for easy access by the shader. */
-   pipeline_stats_mask &= (1 << radv_get_pipelinestat_query_size(device)) - 1;
+   pipeline_stats_mask &= (1 << (radv_get_pipelinestat_query_size(device) / 8)) - 1;
    pipeline_stats_mask |= util_bitcount(pipeline_stats_mask) << 16;
 
    avail_offset -= src_offset;
@@ -1667,6 +1679,8 @@ radv_CmdCopyQueryPoolResults(VkCommandBuffer commandBuffer, VkQueryPool queryPoo
    RADV_FROM_HANDLE(radv_cmd_buffer, cmd_buffer, commandBuffer);
    RADV_FROM_HANDLE(radv_query_pool, pool, queryPool);
    RADV_FROM_HANDLE(radv_buffer, dst_buffer, dstBuffer);
+   struct radv_device *device = cmd_buffer->device;
+   struct radv_physical_device *pdev = device->physical_device;
    struct radeon_cmdbuf *cs = cmd_buffer->cs;
    uint64_t va = radv_buffer_get_va(pool->bo);
    uint64_t dest_va = radv_buffer_get_va(dst_buffer->bo);
@@ -1697,6 +1711,22 @@ radv_CmdCopyQueryPoolResults(VkCommandBuffer commandBuffer, VkQueryPool queryPoo
 
    switch (pool->vk.query_type) {
    case VK_QUERY_TYPE_OCCLUSION:
+      if (!radv_occlusion_query_use_l2(pdev)) {
+         if (flags & VK_QUERY_RESULT_WAIT_BIT) {
+            uint64_t enabled_rb_mask = pdev->rad_info.enabled_rb_mask;
+            uint32_t rb_avail_offset = 16 * util_last_bit64(enabled_rb_mask) - 4;
+            for (unsigned i = 0; i < queryCount; ++i, dest_va += stride) {
+               unsigned query = firstQuery + i;
+               uint64_t src_va = va + query * pool->stride + rb_avail_offset;
+
+               radeon_check_space(device->ws, cs, 7);
+
+               /* Waits on the upper word of the last DB entry */
+               radv_cp_wait_mem(cs, cmd_buffer->qf, WAIT_REG_MEM_GREATER_OR_EQUAL, src_va, 0x80000000, 0xffffffff);
+            }
+         }
+      }
+
       radv_query_shader(cmd_buffer, &cmd_buffer->device->meta_state.query.occlusion_query_pipeline, pool->bo,
                         dst_buffer->bo, firstQuery * pool->stride, dst_buffer->offset + dstOffset, pool->stride, stride,
                         dst_size, queryCount, flags, 0, 0, false);
diff --git a/src/amd/vulkan/radv_queue.c b/src/amd/vulkan/radv_queue.c
index 129b611a7fb..b143ca13d4d 100644
--- a/src/amd/vulkan/radv_queue.c
+++ b/src/amd/vulkan/radv_queue.c
@@ -637,7 +637,7 @@ radv_emit_graphics_scratch(struct radv_device *device, struct radeon_cmdbuf *cs,
       uint64_t va = radv_buffer_get_va(scratch_bo);
 
       /* WAVES is per SE for SPI_TMPRING_SIZE. */
-      waves /= info->num_se;
+      waves /= info->max_se;
 
       radeon_set_context_reg_seq(cs, R_0286E8_SPI_TMPRING_SIZE, 3);
       radeon_emit(cs, S_0286E8_WAVES(waves) | S_0286E8_WAVESIZE(DIV_ROUND_UP(size_per_wave, 256)));
@@ -675,7 +675,7 @@ radv_emit_compute_scratch(struct radv_device *device, struct radeon_cmdbuf *cs,
       radeon_emit(cs, scratch_va >> 8);
       radeon_emit(cs, scratch_va >> 40);
 
-      waves /= info->num_se;
+      waves /= info->max_se;
    }
 
    radeon_set_sh_reg_seq(cs, R_00B900_COMPUTE_USER_DATA_0, 2);
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 0798b73b3dc..09b45ac715a 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -982,6 +982,7 @@ alloc_block_obj(struct radv_device *device)
 static void
 free_block_obj(struct radv_device *device, union radv_shader_arena_block *block)
 {
+   list_del(&block->pool);
    list_add(&block->pool, &device->shader_block_obj_pool);
 }
 
@@ -1267,7 +1268,6 @@ radv_free_shader_memory(struct radv_device *device, union radv_shader_arena_bloc
          remove_hole(free_list, hole_prev);
 
       hole_prev->size += hole->size;
-      list_del(&hole->list);
       free_block_obj(device, hole);
 
       hole = hole_prev;
@@ -1280,7 +1280,6 @@ radv_free_shader_memory(struct radv_device *device, union radv_shader_arena_bloc
 
       hole_next->offset -= hole->size;
       hole_next->size += hole->size;
-      list_del(&hole->list);
       free_block_obj(device, hole);
 
       hole = hole_next;
@@ -1293,6 +1292,18 @@ radv_free_shader_memory(struct radv_device *device, union radv_shader_arena_bloc
       radv_rmv_log_bo_destroy(device, arena->bo);
       device->ws->buffer_destroy(device->ws, arena->bo);
       list_del(&arena->list);
+
+      if (device->capture_replay_arena_vas) {
+         struct hash_entry *arena_entry = NULL;
+         hash_table_foreach (device->capture_replay_arena_vas->table, entry) {
+            if (entry->data == arena) {
+               arena_entry = entry;
+               break;
+            }
+         }
+         _mesa_hash_table_remove(device->capture_replay_arena_vas->table, arena_entry);
+      }
+
       free(arena);
    } else if (free_list) {
       add_hole(free_list, hole);
@@ -1301,18 +1312,6 @@ radv_free_shader_memory(struct radv_device *device, union radv_shader_arena_bloc
    mtx_unlock(&device->shader_arena_mutex);
 }
 
-struct radv_serialized_shader_arena_block
-radv_serialize_shader_arena_block(union radv_shader_arena_block *block)
-{
-   struct radv_serialized_shader_arena_block serialized_block = {
-      .offset = block->offset,
-      .size = block->size,
-      .arena_va = block->arena->bo->va,
-      .arena_size = block->arena->size,
-   };
-   return serialized_block;
-}
-
 union radv_shader_arena_block *
 radv_replay_shader_arena_block(struct radv_device *device, const struct radv_serialized_shader_arena_block *src,
                                void *ptr)
@@ -1571,7 +1570,7 @@ radv_postprocess_binary_config(struct radv_device *device, struct radv_shader_bi
    const struct radv_shader_info *info = &binary->info;
    gl_shader_stage stage = binary->info.stage;
    const struct radv_physical_device *pdevice = device->physical_device;
-   bool scratch_enabled = config->scratch_bytes_per_wave > 0 || info->cs.is_rt_shader;
+   bool scratch_enabled = config->scratch_bytes_per_wave > 0;
    bool trap_enabled = !!device->trap_handler_shader;
    unsigned vgpr_comp_cnt = 0;
    unsigned num_input_vgprs = args->ac.num_vgprs_used;
@@ -1717,8 +1716,6 @@ radv_postprocess_binary_config(struct radv_device *device, struct radv_shader_bi
    case MESA_SHADER_CALLABLE:
    case MESA_SHADER_INTERSECTION:
    case MESA_SHADER_ANY_HIT:
-      config->rsrc2 |= S_00B12C_SCRATCH_EN(1);
-      FALLTHROUGH;
    case MESA_SHADER_COMPUTE:
    case MESA_SHADER_TASK:
       config->rsrc1 |= S_00B848_MEM_ORDERED(pdevice->rad_info.gfx_level >= GFX10) | S_00B848_WGP_MODE(wgp_mode);
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index c4ceed01d1d..dbf942e28c3 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -775,8 +775,6 @@ union radv_shader_arena_block *radv_replay_shader_arena_block(struct radv_device
                                                               const struct radv_serialized_shader_arena_block *src,
                                                               void *ptr);
 
-struct radv_serialized_shader_arena_block radv_serialize_shader_arena_block(union radv_shader_arena_block *block);
-
 void radv_free_shader_memory(struct radv_device *device, union radv_shader_arena_block *alloc);
 
 struct radv_shader *radv_create_trap_handler_shader(struct radv_device *device);
diff --git a/src/amd/vulkan/radv_video.c b/src/amd/vulkan/radv_video.c
index 04d6d538a29..c9728ccec41 100644
--- a/src/amd/vulkan/radv_video.c
+++ b/src/amd/vulkan/radv_video.c
@@ -39,8 +39,7 @@
 #include "radv_cs.h"
 #include "radv_debug.h"
 
-#define NUM_H264_REFS                17
-#define NUM_H265_REFS                8
+#define NUM_H2645_REFS               16
 #define FB_BUFFER_OFFSET             0x1000
 #define FB_BUFFER_SIZE               2048
 #define FB_BUFFER_SIZE_TONGA         (2048 * 64)
@@ -444,8 +443,8 @@ radv_GetPhysicalDeviceVideoCapabilitiesKHR(VkPhysicalDevice physicalDevice, cons
       if (pVideoProfile->lumaBitDepth != VK_VIDEO_COMPONENT_BIT_DEPTH_8_BIT_KHR)
          return VK_ERROR_VIDEO_PROFILE_FORMAT_NOT_SUPPORTED_KHR;
 
-      pCapabilities->maxDpbSlots = NUM_H264_REFS;
-      pCapabilities->maxActiveReferencePictures = NUM_H264_REFS;
+      pCapabilities->maxDpbSlots = NUM_H2645_REFS + 1;
+      pCapabilities->maxActiveReferencePictures = NUM_H2645_REFS;
 
       /* for h264 on navi21+ separate dpb images should work */
       if (radv_enable_tier2(pdevice))
@@ -473,8 +472,8 @@ radv_GetPhysicalDeviceVideoCapabilitiesKHR(VkPhysicalDevice physicalDevice, cons
           pVideoProfile->lumaBitDepth != VK_VIDEO_COMPONENT_BIT_DEPTH_10_BIT_KHR)
          return VK_ERROR_VIDEO_PROFILE_FORMAT_NOT_SUPPORTED_KHR;
 
-      pCapabilities->maxDpbSlots = NUM_H264_REFS;
-      pCapabilities->maxActiveReferencePictures = NUM_H265_REFS;
+      pCapabilities->maxDpbSlots = NUM_H2645_REFS + 1;
+      pCapabilities->maxActiveReferencePictures = NUM_H2645_REFS;
       /* for h265 on navi21+ separate dpb images should work */
       if (radv_enable_tier2(pdevice))
          pCapabilities->flags |= VK_VIDEO_CAPABILITY_SEPARATE_REFERENCE_IMAGES_BIT_KHR;
@@ -935,7 +934,10 @@ update_h265_scaling(void *it_ptr, const StdVideoH265ScalingLists *scaling_lists)
 
 static rvcn_dec_message_hevc_t
 get_h265_msg(struct radv_device *device, struct radv_video_session *vid, struct radv_video_session_params *params,
-             const struct VkVideoDecodeInfoKHR *frame_info, void *it_ptr)
+             const struct VkVideoDecodeInfoKHR *frame_info,
+             uint32_t *width_in_samples,
+             uint32_t *height_in_samples,
+             void *it_ptr)
 {
    rvcn_dec_message_hevc_t result;
    int i, j;
@@ -967,6 +969,8 @@ get_h265_msg(struct radv_device *device, struct radv_video_session *vid, struct
    }
    result.st_rps_bits = h265_pic_info->pStdPictureInfo->NumBitsForSTRefPicSetInSlice;
 
+   *width_in_samples = sps->pic_width_in_luma_samples;
+   *height_in_samples = sps->pic_height_in_luma_samples;
    result.chroma_format = sps->chroma_format_idc;
    result.bit_depth_luma_minus8 = sps->bit_depth_luma_minus8;
    result.bit_depth_chroma_minus8 = sps->bit_depth_chroma_minus8;
@@ -1221,7 +1225,8 @@ rvcn_dec_message_decode(struct radv_cmd_buffer *cmd_buffer, struct radv_video_se
       break;
    }
    case VK_VIDEO_CODEC_OPERATION_DECODE_H265_BIT_KHR: {
-      rvcn_dec_message_hevc_t hevc = get_h265_msg(device, vid, params, frame_info, it_ptr);
+      rvcn_dec_message_hevc_t hevc =
+         get_h265_msg(device, vid, params, frame_info, &decode->width_in_samples, &decode->height_in_samples, it_ptr);
       memcpy(codec, (void *)&hevc, sizeof(rvcn_dec_message_hevc_t));
       index_codec->message_id = RDECODE_MESSAGE_HEVC;
       break;
@@ -1378,7 +1383,8 @@ get_uvd_h264_msg(struct radv_video_session *vid, struct radv_video_session_param
 
 static struct ruvd_h265
 get_uvd_h265_msg(struct radv_device *device, struct radv_video_session *vid, struct radv_video_session_params *params,
-                 const struct VkVideoDecodeInfoKHR *frame_info, void *it_ptr)
+                 const struct VkVideoDecodeInfoKHR *frame_info, uint32_t *width_in_samples,
+                 uint32_t *height_in_samples, void *it_ptr)
 {
    struct ruvd_h265 result;
    int i, j;
@@ -1406,6 +1412,8 @@ get_uvd_h265_msg(struct radv_device *device, struct radv_video_session *vid, str
    if (device->physical_device->rad_info.family == CHIP_CARRIZO)
       result.sps_info_flags |= 1 << 9;
 
+   *width_in_samples = sps->pic_width_in_luma_samples;
+   *height_in_samples = sps->pic_height_in_luma_samples;
    result.chroma_format = sps->chroma_format_idc;
    result.bit_depth_luma_minus8 = sps->bit_depth_luma_minus8;
    result.bit_depth_chroma_minus8 = sps->bit_depth_chroma_minus8;
@@ -1579,7 +1587,10 @@ ruvd_dec_message_decode(struct radv_device *device, struct radv_video_session *v
       break;
    }
    case VK_VIDEO_CODEC_OPERATION_DECODE_H265_BIT_KHR: {
-      msg->body.decode.codec.h265 = get_uvd_h265_msg(device, vid, params, frame_info, it_ptr);
+      msg->body.decode.codec.h265 = get_uvd_h265_msg(device, vid, params, frame_info,
+                                                     &msg->body.decode.width_in_samples,
+                                                     &msg->body.decode.height_in_samples,
+                                                     it_ptr);
 
       if (vid->ctx.mem)
          msg->body.decode.dpb_reserved = vid->ctx.size;
diff --git a/src/amd/vulkan/si_cmd_buffer.c b/src/amd/vulkan/si_cmd_buffer.c
index 73a8b847a5c..2ca766340a6 100644
--- a/src/amd/vulkan/si_cmd_buffer.c
+++ b/src/amd/vulkan/si_cmd_buffer.c
@@ -84,14 +84,18 @@ radv_emit_compute(struct radv_device *device, struct radeon_cmdbuf *cs)
    radeon_set_sh_reg_seq(cs, R_00B858_COMPUTE_STATIC_THREAD_MGMT_SE0, 2);
    /* R_00B858_COMPUTE_STATIC_THREAD_MGMT_SE0 / SE1,
     * renamed COMPUTE_DESTINATION_EN_SEn on gfx10. */
-   radeon_emit(cs, S_00B858_SH0_CU_EN(info->spi_cu_en) | S_00B858_SH1_CU_EN(info->spi_cu_en));
-   radeon_emit(cs, S_00B858_SH0_CU_EN(info->spi_cu_en) | S_00B858_SH1_CU_EN(info->spi_cu_en));
+   for (unsigned i = 0; i < 2; ++i) {
+      unsigned cu_mask = i < info->num_se ? info->spi_cu_en : 0x0;
+      radeon_emit(cs, S_00B8AC_SA0_CU_EN(cu_mask) | S_00B8AC_SA1_CU_EN(cu_mask));
+   }
 
    if (device->physical_device->rad_info.gfx_level >= GFX7) {
       /* Also set R_00B858_COMPUTE_STATIC_THREAD_MGMT_SE2 / SE3 */
       radeon_set_sh_reg_seq(cs, R_00B864_COMPUTE_STATIC_THREAD_MGMT_SE2, 2);
-      radeon_emit(cs, S_00B858_SH0_CU_EN(info->spi_cu_en) | S_00B858_SH1_CU_EN(info->spi_cu_en));
-      radeon_emit(cs, S_00B858_SH0_CU_EN(info->spi_cu_en) | S_00B858_SH1_CU_EN(info->spi_cu_en));
+      for (unsigned i = 2; i < 4; ++i) {
+         unsigned cu_mask = i < info->num_se ? info->spi_cu_en : 0x0;
+         radeon_emit(cs, S_00B8AC_SA0_CU_EN(cu_mask) | S_00B8AC_SA1_CU_EN(cu_mask));
+      }
 
       if (device->border_color_data.bo) {
          uint64_t bc_va = radv_buffer_get_va(device->border_color_data.bo);
@@ -140,13 +144,12 @@ radv_emit_compute(struct radv_device *device, struct radeon_cmdbuf *cs)
    }
 
    if (device->physical_device->rad_info.gfx_level >= GFX11) {
-      uint32_t spi_cu_en = device->physical_device->rad_info.spi_cu_en;
-
       radeon_set_sh_reg_seq(cs, R_00B8AC_COMPUTE_STATIC_THREAD_MGMT_SE4, 4);
-      radeon_emit(cs, S_00B8AC_SA0_CU_EN(spi_cu_en) | S_00B8AC_SA1_CU_EN(spi_cu_en)); /* SE4 */
-      radeon_emit(cs, S_00B8AC_SA0_CU_EN(spi_cu_en) | S_00B8AC_SA1_CU_EN(spi_cu_en)); /* SE5 */
-      radeon_emit(cs, S_00B8AC_SA0_CU_EN(spi_cu_en) | S_00B8AC_SA1_CU_EN(spi_cu_en)); /* SE6 */
-      radeon_emit(cs, S_00B8AC_SA0_CU_EN(spi_cu_en) | S_00B8AC_SA1_CU_EN(spi_cu_en)); /* SE7 */
+      /* SE4-SE7 */
+      for (unsigned i = 4; i < 8; ++i) {
+         unsigned cu_mask = i < info->num_se ? info->spi_cu_en : 0x0;
+         radeon_emit(cs, S_00B8AC_SA0_CU_EN(cu_mask) | S_00B8AC_SA1_CU_EN(cu_mask));
+      }
 
       radeon_set_sh_reg(cs, R_00B8BC_COMPUTE_DISPATCH_INTERLEAVE, 64);
    }
@@ -511,9 +514,14 @@ radv_emit_graphics(struct radv_device *device, struct radeon_cmdbuf *cs)
    }
 
    if (physical_device->rad_info.gfx_level >= GFX9) {
+      unsigned max_alloc_count = physical_device->rad_info.pbb_max_alloc_count;
+
+      /* GFX11+ shouldn't subtract 1 from pbb_max_alloc_count.  */
+      if (physical_device->rad_info.gfx_level < GFX11)
+         max_alloc_count -= 1;
+
       radeon_set_context_reg(cs, R_028C48_PA_SC_BINNER_CNTL_1,
-                             S_028C48_MAX_ALLOC_COUNT(physical_device->rad_info.pbb_max_alloc_count - 1) |
-                                S_028C48_MAX_PRIM_PER_BATCH(1023));
+                             S_028C48_MAX_ALLOC_COUNT(max_alloc_count) | S_028C48_MAX_PRIM_PER_BATCH(1023));
       radeon_set_context_reg(cs, R_028C4C_PA_SC_CONSERVATIVE_RASTERIZATION_CNTL, S_028C4C_NULL_SQUAD_AA_MASK_ENABLE(1));
       radeon_set_uconfig_reg(cs, R_030968_VGT_INSTANCE_BASE_ID, 0);
    }
@@ -1109,8 +1117,9 @@ gfx10_cs_emit_cache_flush(struct radeon_cmdbuf *cs, enum amd_gfx_level gfx_level
          *sqtt_flush_bits |= RGP_FLUSH_FLUSH_CB | RGP_FLUSH_INVAL_CB;
       }
 
+      /* GFX11 can't flush DB_META and should use a TS event instead. */
       /* TODO: trigger on RADV_CMD_FLAG_FLUSH_AND_INV_DB_META ? */
-      if (gfx_level < GFX11 && (flush_bits & RADV_CMD_FLAG_FLUSH_AND_INV_DB)) {
+      if (gfx_level != GFX11 && (flush_bits & RADV_CMD_FLAG_FLUSH_AND_INV_DB)) {
          /* Flush HTILE. Will wait for idle later. */
          radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
          radeon_emit(cs, EVENT_TYPE(V_028A90_FLUSH_AND_INV_DB_META) | EVENT_INDEX(0));
diff --git a/src/broadcom/common/v3d_limits.h b/src/broadcom/common/v3d_limits.h
index 354c8784914..da3e3b8a834 100644
--- a/src/broadcom/common/v3d_limits.h
+++ b/src/broadcom/common/v3d_limits.h
@@ -24,8 +24,6 @@
 #ifndef V3D_LIMITS_H
 #define V3D_LIMITS_H
 
-#define V3D_CL_MAX_INSTR_SIZE 25
-
 /* Number of channels a QPU thread executes in parallel.  Also known as
  * gl_SubGroupSizeARB.
  */
diff --git a/src/broadcom/compiler/nir_to_vir.c b/src/broadcom/compiler/nir_to_vir.c
index 966adbbdc48..0d886017774 100644
--- a/src/broadcom/compiler/nir_to_vir.c
+++ b/src/broadcom/compiler/nir_to_vir.c
@@ -656,7 +656,10 @@ ntq_emit_tmu_general(struct v3d_compile *c, nir_intrinsic_instr *instr,
                          */
                         uint32_t perquad =
                                 is_load && !vir_in_nonuniform_control_flow(c) &&
-                                !c->emitted_discard ?
+                                ((c->s->info.stage == MESA_SHADER_FRAGMENT &&
+                                  c->s->info.fs.needs_quad_helper_invocations &&
+                                  !c->emitted_discard) ||
+                                 c->s->info.uses_wide_subgroup_intrinsics) ?
                                 GENERAL_TMU_LOOKUP_PER_QUAD :
                                 GENERAL_TMU_LOOKUP_PER_PIXEL;
                         config = 0xffffff00 | tmu_op << 3 | perquad;
@@ -2745,8 +2748,21 @@ ntq_emit_load_input(struct v3d_compile *c, nir_intrinsic_instr *instr)
                                SYSTEM_VALUE_VERTEX_ID)) {
                       index++;
                }
-               for (int i = 0; i < offset; i++)
-                      index += c->vattr_sizes[i];
+
+               for (int i = 0; i < offset; i++) {
+                      /* GFXH-1602: if any builtins (vid, iid, etc) are read then
+                       * attribute 0 must be active (size > 0). When we hit this,
+                       * the driver is expected to program attribute 0 to have a
+                       * size of 1, so here we need to add that.
+                       */
+                      if (i == 0 && c->vs_key->is_coord &&
+                          c->vattr_sizes[i] == 0 && index > 0) {
+                         index++;
+                      } else {
+                         index += c->vattr_sizes[i];
+                      }
+               }
+
                index += nir_intrinsic_component(instr);
                for (int i = 0; i < instr->num_components; i++) {
                       struct qreg vpm_offset = vir_uniform_ui(c, index++);
@@ -3444,6 +3460,10 @@ ntq_emit_intrinsic(struct v3d_compile *c, nir_intrinsic_instr *instr)
                 vir_emit_tlb_color_read(c, instr);
                 break;
 
+        case nir_intrinsic_load_fep_w_v3d:
+                ntq_store_def(c, &instr->def, 0, vir_MOV(c, c->payload_w));
+                break;
+
         case nir_intrinsic_load_input:
                 ntq_emit_load_input(c, instr);
                 break;
diff --git a/src/broadcom/compiler/v3d_compiler.h b/src/broadcom/compiler/v3d_compiler.h
index 18281e42b12..8ba341bf635 100644
--- a/src/broadcom/compiler/v3d_compiler.h
+++ b/src/broadcom/compiler/v3d_compiler.h
@@ -603,6 +603,11 @@ struct v3d_ra_node_info {
                 bool is_program_end;
                 bool unused;
 
+                /* If this node may have an allocation conflict with a
+                 * payload register.
+                 */
+                bool payload_conflict;
+
                 /* V3D 7.x */
                 bool is_ldunif_dst;
         } *info;
diff --git a/src/broadcom/compiler/v3d_nir_lower_line_smooth.c b/src/broadcom/compiler/v3d_nir_lower_line_smooth.c
index 12602486cce..05b5224bc52 100644
--- a/src/broadcom/compiler/v3d_nir_lower_line_smooth.c
+++ b/src/broadcom/compiler/v3d_nir_lower_line_smooth.c
@@ -91,13 +91,23 @@ initialise_coverage_var(struct lower_line_smooth_state *state,
 
         nir_def *real_line_width = nir_load_aa_line_width(&b);
 
-        /* The line coord varies from 0.0 to 1.0 across the width of the line */
+        /* According to the PRM, the line coord varies from 0.0 to 1.0 across
+         * the width of the line. But actually, when a perspective projection
+         * is used, it is also applied to the line coords, so the values end
+         * up being between [min_coord, 1], based on the Wc coordinate.  We
+         * need to re-map the values to be between [0.0, 1.0].
+         */
         nir_def *line_coord = nir_load_line_coord(&b);
+        nir_def *wc = nir_load_fep_w_v3d(&b, 32);
+        nir_def *min_coord_val = nir_fsub(&b, nir_imm_float(&b, 1.0f), wc);
+        nir_def *normalized_line_coord = nir_fdiv(&b,
+                                                  nir_fsub(&b, line_coord, min_coord_val),
+                                                  nir_fsub_imm(&b, 1.0, min_coord_val));;
 
         /* fabs(line_coord - 0.5) * real_line_width */
         nir_def *pixels_from_center =
                 nir_fmul(&b, real_line_width,
-                         nir_fabs(&b, nir_fsub(&b, line_coord,
+                         nir_fabs(&b, nir_fsub(&b, normalized_line_coord,
                                                nir_imm_float(&b, 0.5f))));
 
         /* 0.5 - 1/√2 * (pixels_from_center - line_width * 0.5) */
diff --git a/src/broadcom/compiler/vir_register_allocate.c b/src/broadcom/compiler/vir_register_allocate.c
index 53e84840899..f407eff8b3e 100644
--- a/src/broadcom/compiler/vir_register_allocate.c
+++ b/src/broadcom/compiler/vir_register_allocate.c
@@ -48,6 +48,13 @@ get_phys_index(const struct v3d_device_info *devinfo)
 #define CLASS_BITS_ACC    (1 << 1)
 #define CLASS_BITS_R5     (1 << 4)
 
+static inline bool
+stage_has_payload(struct v3d_compile *c)
+{
+        return c->s->info.stage == MESA_SHADER_FRAGMENT ||
+               c->s->info.stage == MESA_SHADER_COMPUTE;
+}
+
 static uint8_t
 get_class_bit_any(const struct v3d_device_info *devinfo)
 {
@@ -372,7 +379,7 @@ ensure_nodes(struct v3d_compile *c)
 /* Creates the interference node for a new temp. We use this to keep the node
  * list updated during the spilling process, which generates new temps/nodes.
  */
-static void
+static int
 add_node(struct v3d_compile *c, uint32_t temp, uint8_t class_bits)
 {
         ensure_nodes(c);
@@ -387,6 +394,9 @@ add_node(struct v3d_compile *c, uint32_t temp, uint8_t class_bits)
         c->nodes.info[node].is_ldunif_dst = false;
         c->nodes.info[node].is_program_end = false;
         c->nodes.info[node].unused = false;
+        c->nodes.info[node].payload_conflict = false;
+
+        return node;
 }
 
 /* The spill offset for this thread takes a bit of setup, so do it once at
@@ -438,7 +448,9 @@ v3d_setup_spill_base(struct v3d_compile *c)
                             i != c->spill_base.index) {
                                 temp_class |= CLASS_BITS_ACC;
                         }
-                        add_node(c, i, temp_class);
+                        int node = add_node(c, i, temp_class);
+                        c->nodes.info[node].payload_conflict =
+                                stage_has_payload(c);
                 }
         }
 
@@ -940,10 +952,12 @@ v3d_ra_select_rf(struct v3d_ra_select_callback_data *v3d_ra,
         /* The last 3 instructions in a shader can't use some specific registers
          * (usually early rf registers, depends on v3d version) so try to
          * avoid allocating these to registers used by the last instructions
-         * in the shader.
+         * in the shader. Do the same for spilling setup instructions that
+         * may conflict with payload registers.
          */
         const uint32_t safe_rf_start = v3d_ra->devinfo->ver == 42 ? 3 : 4;
-        if (v3d_ra->nodes->info[node].is_program_end &&
+        if ((v3d_ra->nodes->info[node].is_program_end ||
+             v3d_ra->nodes->info[node].payload_conflict) &&
             v3d_ra->next_phys < safe_rf_start) {
                 v3d_ra->next_phys = safe_rf_start;
         }
@@ -1060,11 +1074,52 @@ tmu_spilling_allowed(struct v3d_compile *c)
         return c->spills + c->fills < c->max_tmu_spills;
 }
 
+static bool
+reg_is_payload(struct v3d_compile *c, struct qreg reg)
+{
+   if (reg.file != QFILE_REG)
+      return false;
+
+   if (c->devinfo->ver >= 71) {
+           if (c->s->info.stage == MESA_SHADER_FRAGMENT)
+                   return reg.index >= 1 && reg.index <= 3;
+           if (c->s->info.stage == MESA_SHADER_COMPUTE)
+                   return reg.index == 2 || reg.index == 3;
+           return false;
+   }
+
+   assert(c->devinfo->ver == 42);
+   if (c->s->info.stage == MESA_SHADER_FRAGMENT)
+           return reg.index <= 2;
+   if (c->s->info.stage == MESA_SHADER_COMPUTE)
+           return reg.index == 0 || reg.index == 2;
+   return false;
+}
+
+static bool
+inst_reads_payload(struct v3d_compile *c, struct qinst *inst)
+{
+        if (inst->qpu.type != V3D_QPU_INSTR_TYPE_ALU)
+                return false;
+
+        if (reg_is_payload(c, inst->dst))
+                return true;
+
+        if (reg_is_payload(c, inst->src[0]))
+                return true;
+
+        if (vir_get_nsrc(inst) > 1 && reg_is_payload(c, inst->src[1]))
+                return true;
+
+        return false;
+}
+
 static void
 update_graph_and_reg_classes_for_inst(struct v3d_compile *c,
                                       int *acc_nodes,
                                       int *implicit_rf_nodes,
                                       int last_ldvary_ip,
+                                      bool has_payload,
                                       struct qinst *inst)
 {
         int32_t ip = inst->ip;
@@ -1180,6 +1235,33 @@ update_graph_and_reg_classes_for_inst(struct v3d_compile *c,
                 }
         }
 
+        /* Spill setup instructions are the only ones that we emit before
+         * reading payload registers so we want to flag their temps so we
+         * don't assign them to payload registers and stomp them before we
+         * can read them. For the case where we may have emitted spill setup
+         * before RA (i.e. for scratch), we need to do this now.
+         */
+        if (c->spill_size > 0 && has_payload && inst_reads_payload(c, inst)) {
+                struct qblock *first_block = vir_entry_block(c);
+                list_for_each_entry_from_rev(struct qinst, _i, inst->link.prev,
+                                             &first_block->instructions, link) {
+                        if (_i->qpu.type != V3D_QPU_INSTR_TYPE_ALU)
+                                continue;
+                        if (_i->dst.file == QFILE_TEMP) {
+                                int node = temp_to_node(c, _i->dst.index);
+                                c->nodes.info[node].payload_conflict = true;
+                        }
+                        if (_i->src[0].file == QFILE_TEMP) {
+                                int node = temp_to_node(c, _i->src[0].index);
+                                c->nodes.info[node].payload_conflict = true;
+                        }
+                        if (vir_get_nsrc(_i) > 1 && _i->src[1].file == QFILE_TEMP) {
+                                int node = temp_to_node(c, _i->src[1].index);
+                                c->nodes.info[node].payload_conflict = true;
+                        }
+                }
+        }
+
         if (inst->dst.file == QFILE_TEMP) {
                 /* Only a ldunif gets to write to R5, which only has a
                  * single 32-bit channel of storage.
@@ -1354,6 +1436,7 @@ v3d_register_allocate(struct v3d_compile *c)
          */
         int ip = 0;
         int last_ldvary_ip = -1;
+        bool has_payload = stage_has_payload(c);
         vir_for_each_inst_inorder(inst, c) {
                 inst->ip = ip++;
 
@@ -1373,7 +1456,9 @@ v3d_register_allocate(struct v3d_compile *c)
 
                 update_graph_and_reg_classes_for_inst(c, acc_nodes,
                                                       implicit_rf_nodes,
-                                                      last_ldvary_ip, inst);
+                                                      last_ldvary_ip,
+                                                      has_payload,
+                                                      inst);
         }
 
         /* Flag the nodes that are used in the last instructions of the program
diff --git a/src/broadcom/vulkan/v3dv_cl.c b/src/broadcom/vulkan/v3dv_cl.c
index 851e1388a8d..af4066c7849 100644
--- a/src/broadcom/vulkan/v3dv_cl.c
+++ b/src/broadcom/vulkan/v3dv_cl.c
@@ -31,6 +31,16 @@
 #include "broadcom/common/v3d_macros.h"
 #include "broadcom/cle/v3dx_pack.h"
 
+/* The Control List Executor (CLE) pre-fetches V3D_CLE_READAHEAD bytes from
+ * the Control List buffer. The usage of these last bytes should be avoided or
+ * the CLE would pre-fetch the data after the end of the CL buffer, reporting
+ * the kernel "MMU error from client CLE".
+ */
+#define V3D42_CLE_READAHEAD 256u
+#define V3D42_CLE_BUFFER_MIN_SIZE 4096u
+#define V3D71_CLE_READAHEAD 1024u
+#define V3D71_CLE_BUFFER_MIN_SIZE 16384u
+
 void
 v3dv_cl_init(struct v3dv_job *job, struct v3dv_cl *cl)
 {
@@ -55,14 +65,42 @@ v3dv_cl_destroy(struct v3dv_cl *cl)
    v3dv_cl_init(NULL, cl);
 }
 
+enum v3dv_cl_chain_type {
+   V3D_CL_BO_CHAIN_NONE = 0,
+   V3D_CL_BO_CHAIN_WITH_BRANCH,
+   V3D_CL_BO_CHAIN_WITH_RETURN_FROM_SUB_LIST,
+};
+
 static bool
-cl_alloc_bo(struct v3dv_cl *cl, uint32_t space, bool use_branch)
+cl_alloc_bo(struct v3dv_cl *cl, uint32_t space, enum
+            v3dv_cl_chain_type chain_type)
 {
+   /* The last bytes of a CLE buffer are unusable because of readahead
+    * prefetch, so we need to take it into account when allocating a new BO
+    * for the CL. We also reserve space for the BRANCH/RETURN_FROM_SUB_LIST
+    * packet so we can always emit these last packets to the BO when
+    * needed. We will need to increase cl->size by the packet length before
+    * calling cl_submit to use this reserved space.
+    */
+   uint32_t unusable_space = 0;
+   uint32_t cle_readahead = V3DV_X(cl->job->device, CLE_READAHEAD);
+   uint32_t cle_buffer_min_size = V3DV_X(cl->job->device, CLE_BUFFER_MIN_SIZE);
+   switch (chain_type) {
+   case V3D_CL_BO_CHAIN_WITH_BRANCH:
+      unusable_space = cle_readahead + cl_packet_length(BRANCH);
+      break;
+   case V3D_CL_BO_CHAIN_WITH_RETURN_FROM_SUB_LIST:
+      unusable_space = cle_readahead + cl_packet_length(RETURN_FROM_SUB_LIST);
+      break;
+   case V3D_CL_BO_CHAIN_NONE:
+      break;
+   }
+
    /* If we are growing, double the BO allocation size to reduce the number
     * of allocations with large command buffers. This has a very significant
     * impact on the number of draw calls per second reported by vkoverhead.
     */
-   space = align(space, 4096);
+   space = align(space + unusable_space, cle_buffer_min_size);
    if (cl->bo)
       space = MAX2(cl->bo->size * 2, space);
 
@@ -83,9 +121,28 @@ cl_alloc_bo(struct v3dv_cl *cl, uint32_t space, bool use_branch)
    }
 
    /* Chain to the new BO from the old one if requested */
-   if (use_branch && cl->bo) {
-      cl_emit(cl, BRANCH, branch) {
-         branch.address = v3dv_cl_address(bo, 0);
+   if (cl->bo) {
+      switch (chain_type) {
+      case V3D_CL_BO_CHAIN_WITH_BRANCH:
+         cl->size += cl_packet_length(BRANCH);
+         assert(cl->size + cle_readahead <= cl->bo->size);
+         cl_emit(cl, BRANCH, branch) {
+            branch.address = v3dv_cl_address(bo, 0);
+         }
+         break;
+      case V3D_CL_BO_CHAIN_WITH_RETURN_FROM_SUB_LIST:
+         /* We do not want to emit branches from secondary command lists, instead,
+          * we will branch to them when we execute them in a primary using
+          * 'branch to sub list' commands, expecting each linked secondary to
+          * end with a 'return from sub list' command.
+          */
+         cl->size += cl_packet_length(RETURN_FROM_SUB_LIST);
+         assert(cl->size + cle_readahead <= cl->bo->size);
+         cl_emit(cl, RETURN_FROM_SUB_LIST, ret);
+         FALLTHROUGH;
+      case V3D_CL_BO_CHAIN_NONE:
+         v3dv_job_add_bo_unchecked(cl->job, bo);
+         break;
       }
    } else {
       v3dv_job_add_bo_unchecked(cl->job, bo);
@@ -93,7 +150,11 @@ cl_alloc_bo(struct v3dv_cl *cl, uint32_t space, bool use_branch)
 
    cl->bo = bo;
    cl->base = cl->bo->map;
-   cl->size = cl->bo->size;
+   /* Take only into account the usable size of the BO to guarantee that
+    * we never write in the last bytes of the CL buffer because of the
+    * readahead of the CLE
+    */
+   cl->size = cl->bo->size - unusable_space;
    cl->next = cl->base;
 
    return true;
@@ -109,37 +170,20 @@ v3dv_cl_ensure_space(struct v3dv_cl *cl, uint32_t space, uint32_t alignment)
       return offset;
    }
 
-   cl_alloc_bo(cl, space, false);
+   cl_alloc_bo(cl, space, V3D_CL_BO_CHAIN_NONE);
+
    return 0;
 }
 
 void
 v3dv_cl_ensure_space_with_branch(struct v3dv_cl *cl, uint32_t space)
 {
-   /* We do not want to emit branches from secondary command lists, instead,
-    * we will branch to them when we execute them in a primary using
-    * 'branch to sub list' commands, expecting each linked secondary to
-    * end with a 'return from sub list' command.
-    */
-   bool needs_return_from_sub_list = false;
-   if (cl->job->type == V3DV_JOB_TYPE_GPU_CL_SECONDARY && cl->size > 0)
-         needs_return_from_sub_list = true;
-
-   /*
-    * The CLE processor in the simulator tries to read V3D_CL_MAX_INSTR_SIZE
-    * bytes form the CL for each new instruction. If the last instruction in our
-    * CL is smaller than that, and there are not at least V3D_CL_MAX_INSTR_SIZE
-    * bytes until the end of the BO, it will read out of bounds and possibly
-    * cause a GMP violation interrupt to trigger. Ensure we always have at
-    * least that many bytes available to read with the last instruction.
-    */
-   space += V3D_CL_MAX_INSTR_SIZE;
-
    if (v3dv_cl_offset(cl) + space <= cl->size)
       return;
 
-   if (needs_return_from_sub_list)
-      cl_emit(cl, RETURN_FROM_SUB_LIST, ret);
+   enum v3dv_cl_chain_type  chain_type = V3D_CL_BO_CHAIN_WITH_BRANCH;
+   if (cl->job->type == V3DV_JOB_TYPE_GPU_CL_SECONDARY)
+      chain_type = V3D_CL_BO_CHAIN_WITH_RETURN_FROM_SUB_LIST;
 
-   cl_alloc_bo(cl, space, !needs_return_from_sub_list);
+   cl_alloc_bo(cl, space, chain_type);
 }
diff --git a/src/broadcom/vulkan/v3dv_cl.h b/src/broadcom/vulkan/v3dv_cl.h
index 7e17ac395c4..96721530f77 100644
--- a/src/broadcom/vulkan/v3dv_cl.h
+++ b/src/broadcom/vulkan/v3dv_cl.h
@@ -182,6 +182,7 @@ void v3dv_cl_ensure_space_with_branch(struct v3dv_cl *cl, uint32_t space);
                 cl_packet_pack(packet)(cl, (uint8_t *)cl_out, &name); \
                 cl_advance_and_end(cl, cl_packet_length(packet)); \
                 _loop_terminate = NULL;                          \
+                assert(v3dv_cl_offset(cl) <= (cl)->size);        \
         }))                                                      \
 
 #define cl_emit_with_prepacked(cl, packet, prepacked, name)      \
@@ -215,9 +216,10 @@ cl_pack_emit_reloc(struct v3dv_cl *cl, const struct v3dv_cl_reloc *reloc)
                 v3dv_job_add_bo(cl->job, reloc->bo);
 }
 
-#define cl_emit_prepacked_sized(cl, packet, size) do {                \
-        memcpy((cl)->next, packet, size);             \
-        cl_advance(&(cl)->next, size);                \
+#define cl_emit_prepacked_sized(cl, packet, psize) do {          \
+        memcpy((cl)->next, packet, psize);                       \
+        cl_advance(&(cl)->next, psize);                          \
+        assert(v3dv_cl_offset(cl) <= (cl)->size);                \
 } while (0)
 
 #define cl_emit_prepacked(cl, packet) \
diff --git a/src/broadcom/vulkan/v3dv_cmd_buffer.c b/src/broadcom/vulkan/v3dv_cmd_buffer.c
index da4518de100..4b317ff21e7 100644
--- a/src/broadcom/vulkan/v3dv_cmd_buffer.c
+++ b/src/broadcom/vulkan/v3dv_cmd_buffer.c
@@ -1377,7 +1377,7 @@ cmd_buffer_emit_subpass_clears(struct v3dv_cmd_buffer *cmd_buffer)
     */
    if (cmd_buffer->state.tile_aligned_render_area &&
        !subpass->do_depth_clear_with_draw &&
-       !subpass->do_depth_clear_with_draw) {
+       !subpass->do_stencil_clear_with_draw) {
       return;
    }
 
@@ -4327,7 +4327,7 @@ cmd_buffer_create_csd_job(struct v3dv_cmd_buffer *cmd_buffer,
    if (cs_variant->prog_data.cs->shared_size > 0) {
       job->csd.shared_memory =
          v3dv_bo_alloc(cmd_buffer->device,
-                       cs_variant->prog_data.cs->shared_size * wgs_per_sg,
+                       cs_variant->prog_data.cs->shared_size * num_wgs,
                        "shared_vars", true);
       if (!job->csd.shared_memory) {
          v3dv_flag_oom(cmd_buffer, NULL);
diff --git a/src/broadcom/vulkan/v3dvx_cmd_buffer.c b/src/broadcom/vulkan/v3dvx_cmd_buffer.c
index 011f5c8e101..65b18ae639c 100644
--- a/src/broadcom/vulkan/v3dvx_cmd_buffer.c
+++ b/src/broadcom/vulkan/v3dvx_cmd_buffer.c
@@ -2596,11 +2596,12 @@ v3dX(cmd_buffer_emit_index_buffer)(struct v3dv_cmd_buffer *cmd_buffer)
          &job->bcl, cl_packet_length(INDEX_BUFFER_SETUP));
       v3dv_return_if_oom(cmd_buffer, NULL);
 
-      const uint32_t offset = cmd_buffer->state.index_buffer.offset;
+      const uint32_t offset = ibuffer->mem_offset +
+                              cmd_buffer->state.index_buffer.offset;
+      assert(ibuffer->mem->bo->size >= offset);
       cl_emit(&job->bcl, INDEX_BUFFER_SETUP, ib) {
-         ib.address = v3dv_cl_address(ibuffer->mem->bo,
-                                      ibuffer->mem_offset + offset);
-         ib.size = ibuffer->mem->bo->size;
+         ib.address = v3dv_cl_address(ibuffer->mem->bo, offset);
+         ib.size = ibuffer->mem->bo->size - offset;
       }
    }
 
diff --git a/src/compiler/glsl/ast_to_hir.cpp b/src/compiler/glsl/ast_to_hir.cpp
index 8b5cdd5cb35..3fe5195dcf4 100644
--- a/src/compiler/glsl/ast_to_hir.cpp
+++ b/src/compiler/glsl/ast_to_hir.cpp
@@ -6058,7 +6058,7 @@ ast_parameter_declarator::hir(exec_list *instructions,
     */
    if ((var->data.mode == ir_var_function_inout || var->data.mode == ir_var_function_out)
        && glsl_type_is_array(type)
-       && !state->check_version(120, 100, &loc,
+       && !state->check_version(state->allow_glsl_120_subset_in_110 ? 110 : 120, 100, &loc,
                                 "arrays cannot be out or inout parameters")) {
       type = &glsl_type_builtin_error;
    }
diff --git a/src/compiler/glsl/gl_nir_link_varyings.c b/src/compiler/glsl/gl_nir_link_varyings.c
index ffcccfe9fc1..6ca198da3f2 100644
--- a/src/compiler/glsl/gl_nir_link_varyings.c
+++ b/src/compiler/glsl/gl_nir_link_varyings.c
@@ -50,6 +50,7 @@
 /* Temporary storage for the set of attributes that need locations assigned. */
 struct temp_attr {
    unsigned slots;
+   unsigned original_idx;
    nir_variable *var;
 };
 
@@ -61,7 +62,10 @@ compare_attr(const void *a, const void *b)
    const struct temp_attr *const r = (const struct temp_attr *) b;
 
    /* Reversed because we want a descending order sort below. */
-   return r->slots - l->slots;
+   if (r->slots != l->slots)
+      return r->slots - l->slots;
+
+   return l->original_idx - r->original_idx;
 }
 
 /**
@@ -1238,6 +1242,7 @@ assign_attribute_or_color_locations(void *mem_ctx,
       }
       to_assign[num_attr].slots = slots;
       to_assign[num_attr].var = var;
+      to_assign[num_attr].original_idx = num_attr;
       num_attr++;
    }
 
@@ -2315,9 +2320,17 @@ static int
 varying_matches_xfb_comparator(const void *x_generic, const void *y_generic)
 {
    const struct match *x = (const struct match *) x_generic;
-
-   if (x->producer_var != NULL && x->producer_var->data.is_xfb_only)
-      return varying_matches_match_comparator(x_generic, y_generic);
+   const struct match *y = (const struct match *) y_generic;
+   /* if both varying are used by transform feedback, sort them */
+   if (x->producer_var != NULL && x->producer_var->data.is_xfb_only) {
+      if (y->producer_var != NULL && y->producer_var->data.is_xfb_only)
+         return 0;
+      /* if x is varying and y is not, put y first */
+      return +1;
+   } else if (y->producer_var != NULL && y->producer_var->data.is_xfb_only) {
+      /* if y is varying and x is not, leave x first */
+      return -1;
+   }
 
    /* FIXME: When the comparator returns 0 it means the elements being
     * compared are equivalent. However the qsort documentation says:
@@ -2340,8 +2353,11 @@ static int
 varying_matches_not_xfb_comparator(const void *x_generic, const void *y_generic)
 {
    const struct match *x = (const struct match *) x_generic;
+   const struct match *y = (const struct match *) y_generic;
 
-   if (x->producer_var != NULL && !x->producer_var->data.is_xfb)
+   if ( (x->producer_var != NULL && !x->producer_var->data.is_xfb)
+        && (y->producer_var != NULL && !y->producer_var->data.is_xfb) )
+      /* if both are non-xfb, then sort them */
       return varying_matches_match_comparator(x_generic, y_generic);
 
    /* FIXME: When the comparator returns 0 it means the elements being
diff --git a/src/compiler/glsl/gl_nir_linker.c b/src/compiler/glsl/gl_nir_linker.c
index 0c1344b683d..f40127f3621 100644
--- a/src/compiler/glsl/gl_nir_linker.c
+++ b/src/compiler/glsl/gl_nir_linker.c
@@ -921,6 +921,8 @@ gl_nir_add_point_size(nir_shader *nir)
       nir_deref_instr *deref = nir_build_deref_var(&b, psiz);
       nir_store_deref(&b, deref, nir_imm_float(&b, 1.0), BITFIELD_BIT(0));
    }
+
+   nir->info.outputs_written |= VARYING_BIT_PSIZ;
 }
 
 static void
@@ -1162,6 +1164,8 @@ gl_nir_link_spirv(const struct gl_constants *consts,
    if (!prelink_lowering(consts, exts, prog, linked_shader, num_shaders))
       return false;
 
+   gl_nir_link_assign_xfb_resources(consts, prog);
+
    /* Linking the stages in the opposite order (from fragment to vertex)
     * ensures that inter-shader outputs written to in an earlier stage
     * are eliminated if they are (transitively) not used in a later
@@ -1191,7 +1195,6 @@ gl_nir_link_spirv(const struct gl_constants *consts,
       return false;
 
    gl_nir_link_assign_atomic_counter_resources(consts, prog);
-   gl_nir_link_assign_xfb_resources(consts, prog);
 
    return true;
 }
diff --git a/src/compiler/glsl_types.c b/src/compiler/glsl_types.c
index 71edd9e98b5..88eeeae5c41 100644
--- a/src/compiler/glsl_types.c
+++ b/src/compiler/glsl_types.c
@@ -3145,6 +3145,11 @@ encode_type_to_blob(struct blob *blob, const glsl_type *type)
       encode_type_to_blob(blob, type->fields.array);
       return;
    case GLSL_TYPE_COOPERATIVE_MATRIX:
+      /* The first 5 bits of encoded/decoded are used to identify the
+       * actual type, but cmat_desc already is 32-bit without that tag, so
+       * encode just the cmat base type first, then the actual cmat desc.
+       */
+      blob_write_uint32(blob, encoded.u32);
       encoded.cmat_desc = type->cmat_desc;
       blob_write_uint32(blob, encoded.u32);
       return;
@@ -3255,6 +3260,7 @@ decode_type_from_blob(struct blob_reader *blob)
                              explicit_stride);
    }
    case GLSL_TYPE_COOPERATIVE_MATRIX: {
+      encoded.u32 = blob_read_uint32(blob);
       return glsl_cmat_type(&encoded.cmat_desc);
    }
    case GLSL_TYPE_STRUCT:
diff --git a/src/compiler/glsl_types.h b/src/compiler/glsl_types.h
index 15740f6087d..8af11e71745 100644
--- a/src/compiler/glsl_types.h
+++ b/src/compiler/glsl_types.h
@@ -924,12 +924,12 @@ static inline const glsl_type *glsl_float_type(void) { return &glsl_type_builtin
 static inline const glsl_type *glsl_float16_t_type(void) { return &glsl_type_builtin_float16_t; }
 static inline const glsl_type *glsl_double_type(void) { return &glsl_type_builtin_double; }
 static inline const glsl_type *glsl_vec2_type(void) { return &glsl_type_builtin_vec2; }
-static inline const glsl_type *glsl_dvec2_type(void) { return &glsl_type_builtin_ivec2; }
+static inline const glsl_type *glsl_dvec2_type(void) { return &glsl_type_builtin_dvec2; }
 static inline const glsl_type *glsl_uvec2_type(void) { return &glsl_type_builtin_uvec2; }
 static inline const glsl_type *glsl_ivec2_type(void) { return &glsl_type_builtin_ivec2; }
 static inline const glsl_type *glsl_bvec2_type(void) { return &glsl_type_builtin_bvec2; }
 static inline const glsl_type *glsl_vec4_type(void) { return &glsl_type_builtin_vec4; }
-static inline const glsl_type *glsl_dvec4_type(void) { return &glsl_type_builtin_ivec4; }
+static inline const glsl_type *glsl_dvec4_type(void) { return &glsl_type_builtin_dvec4; }
 static inline const glsl_type *glsl_uvec4_type(void) { return &glsl_type_builtin_uvec4; }
 static inline const glsl_type *glsl_ivec4_type(void) { return &glsl_type_builtin_ivec4; }
 static inline const glsl_type *glsl_bvec4_type(void) { return &glsl_type_builtin_bvec4; }
diff --git a/src/compiler/meson.build b/src/compiler/meson.build
index 1dae56d1b2b..8d73544c6d8 100644
--- a/src/compiler/meson.build
+++ b/src/compiler/meson.build
@@ -79,7 +79,7 @@ subdir('nir')
 
 subdir('spirv')
 
-if with_opencl_spirv
+if with_clc
   subdir('clc')
 endif
 if with_gallium
diff --git a/src/compiler/nir/nir.c b/src/compiler/nir/nir.c
index 70d5a8a2279..00981d4a1c4 100644
--- a/src/compiler/nir/nir.c
+++ b/src/compiler/nir/nir.c
@@ -3133,6 +3133,8 @@ nir_tex_instr_has_implicit_derivative(const nir_tex_instr *instr)
    case nir_texop_txb:
    case nir_texop_lod:
       return true;
+   case nir_texop_tg4:
+      return instr->is_gather_implicit_lod;
    default:
       return false;
    }
diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index 58b602e8a8b..62286b56b03 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -4185,7 +4185,7 @@ static inline nir_function *
 nir_shader_get_function_for_name(const nir_shader *shader, const char *name)
 {
    nir_foreach_function(func, shader) {
-      if (strcmp(func->name, name) == 0)
+      if (func->name && strcmp(func->name, name) == 0)
          return func;
    }
 
diff --git a/src/compiler/nir/nir_builder.c b/src/compiler/nir/nir_builder.c
index 8a93394730f..3e2a64ec976 100644
--- a/src/compiler/nir/nir_builder.c
+++ b/src/compiler/nir/nir_builder.c
@@ -379,6 +379,22 @@ nir_builder_instr_insert(nir_builder *build, nir_instr *instr)
    build->cursor = nir_after_instr(instr);
 }
 
+void
+nir_builder_instr_insert_at_top(nir_builder *build, nir_instr *instr)
+{
+   nir_cursor top = nir_before_impl(build->impl);
+   const bool at_top = build->cursor.block != NULL &&
+                       nir_cursors_equal(build->cursor, top);
+
+   nir_instr_insert(top, instr);
+
+   if (build->update_divergence)
+      nir_update_instr_divergence(build->shader, instr);
+
+   if (at_top)
+      build->cursor = nir_after_instr(instr);
+}
+
 void
 nir_builder_cf_insert(nir_builder *build, nir_cf_node *cf)
 {
diff --git a/src/compiler/nir/nir_builder.h b/src/compiler/nir/nir_builder.h
index 9e88b78b9fb..030ce0b7759 100644
--- a/src/compiler/nir/nir_builder.h
+++ b/src/compiler/nir/nir_builder.h
@@ -182,6 +182,7 @@ nir_shader_intrinsics_pass(nir_shader *shader,
 }
 
 void nir_builder_instr_insert(nir_builder *build, nir_instr *instr);
+void nir_builder_instr_insert_at_top(nir_builder *build, nir_instr *instr);
 
 static inline nir_instr *
 nir_builder_last_instr(nir_builder *build)
@@ -250,9 +251,7 @@ nir_undef(nir_builder *build, unsigned num_components, unsigned bit_size)
    if (!undef)
       return NULL;
 
-   nir_instr_insert(nir_before_impl(build->impl), &undef->instr);
-   if (build->update_divergence)
-      nir_update_instr_divergence(build->shader, &undef->instr);
+   nir_builder_instr_insert_at_top(build, &undef->instr);
 
    return &undef->def;
 }
@@ -1832,7 +1831,7 @@ nir_decl_reg(nir_builder *b, unsigned num_components, unsigned bit_size,
    nir_intrinsic_set_divergent(decl, true);
    nir_def_init(&decl->instr, &decl->def, 1, 32);
 
-   nir_instr_insert(nir_before_impl(b->impl), &decl->instr);
+   nir_builder_instr_insert_at_top(b, &decl->instr);
 
    return &decl->def;
 }
diff --git a/src/compiler/nir/nir_divergence_analysis.c b/src/compiler/nir/nir_divergence_analysis.c
index 4cb456bc747..216c092d281 100644
--- a/src/compiler/nir/nir_divergence_analysis.c
+++ b/src/compiler/nir/nir_divergence_analysis.c
@@ -143,6 +143,7 @@ visit_intrinsic(nir_shader *shader, nir_intrinsic_instr *instr)
    case nir_intrinsic_load_xfb_address:
    case nir_intrinsic_load_num_vertices:
    case nir_intrinsic_load_fb_layers_v3d:
+   case nir_intrinsic_load_fep_w_v3d:
    case nir_intrinsic_load_tcs_num_patches_amd:
    case nir_intrinsic_load_ring_tess_factors_amd:
    case nir_intrinsic_load_ring_tess_offchip_amd:
@@ -189,7 +190,6 @@ visit_intrinsic(nir_shader *shader, nir_intrinsic_instr *instr)
    case nir_intrinsic_load_resume_shader_address_amd:
    case nir_intrinsic_load_global_const_block_intel:
    case nir_intrinsic_load_reloc_const_intel:
-   case nir_intrinsic_load_global_block_intel:
    case nir_intrinsic_load_btd_global_arg_addr_intel:
    case nir_intrinsic_load_btd_local_arg_addr_intel:
    case nir_intrinsic_load_mesh_inline_data_intel:
@@ -216,9 +216,17 @@ visit_intrinsic(nir_shader *shader, nir_intrinsic_instr *instr)
    case nir_intrinsic_load_rasterization_primitive_amd:
    case nir_intrinsic_load_global_constant_uniform_block_intel:
    case nir_intrinsic_cmat_length:
+   case nir_intrinsic_load_printf_buffer_address:
       is_divergent = false;
       break;
 
+   /* This is divergent because it specifically loads sequential values into
+    * successive SIMD lanes.
+    */
+   case nir_intrinsic_load_global_block_intel:
+      is_divergent = true;
+      break;
+
    case nir_intrinsic_decl_reg:
       is_divergent = nir_intrinsic_divergent(instr);
       break;
diff --git a/src/compiler/nir/nir_gather_types.c b/src/compiler/nir/nir_gather_types.c
index 1516640a9a7..69acc23b91a 100644
--- a/src/compiler/nir/nir_gather_types.c
+++ b/src/compiler/nir/nir_gather_types.c
@@ -103,7 +103,7 @@ nir_gather_types(nir_function_impl *impl,
    do {
       progress = false;
 
-      nir_foreach_block(block, impl) {
+      nir_foreach_block_unstructured(block, impl) {
          nir_foreach_instr(instr, block) {
             switch (instr->type) {
             case nir_instr_type_alu: {
diff --git a/src/compiler/nir/nir_intrinsics.py b/src/compiler/nir/nir_intrinsics.py
index 60a7fa3732b..afa47d3dcac 100644
--- a/src/compiler/nir/nir_intrinsics.py
+++ b/src/compiler/nir/nir_intrinsics.py
@@ -1714,6 +1714,9 @@ store("tlb_sample_color_v3d", [1], [BASE, COMPONENT, SRC_TYPE], [])
 # the target framebuffer
 intrinsic("load_fb_layers_v3d", dest_comp=1, flags=[CAN_ELIMINATE, CAN_REORDER])
 
+# V3D-specific intrinsic to load W coordinate from the fragment shader payload
+intrinsic("load_fep_w_v3d", dest_comp=1, flags=[CAN_ELIMINATE, CAN_REORDER])
+
 # Load a bindless sampler handle mapping a binding table sampler.
 intrinsic("load_sampler_handle_agx", [1], 1, [],
           flags=[CAN_ELIMINATE, CAN_REORDER],
@@ -2022,11 +2025,15 @@ system_value("leaf_procedural_intel", 1, bit_sizes=[1])
 system_value("btd_shader_type_intel", 1)
 system_value("ray_query_global_intel", 1, bit_sizes=[64])
 
-# Source 0: A matrix (type specified by SRC_TYPE)
-# Source 1: B matrix (type specified by SRC_TYPE)
-# Source 2: Accumulator matrix (type specified by DEST_TYPE)
+# Source 0: Accumulator matrix (type specified by DEST_TYPE)
+# Source 1: A matrix (type specified by SRC_TYPE)
+# Source 2: B matrix (type specified by SRC_TYPE)
 #
 # The matrix parameters are the slices owned by the invocation.
+#
+# The accumulator is source 0 because that is the source the intrinsic
+# infrastructure in NIR uses to determine the number of components in the
+# result.
 intrinsic("dpas_intel", dest_comp=0, src_comp=[0, 0, 0],
           indices=[DEST_TYPE, SRC_TYPE, SATURATE, CMAT_SIGNED_MASK, SYSTOLIC_DEPTH, REPEAT_COUNT],
           flags=[CAN_ELIMINATE])
diff --git a/src/compiler/nir/nir_linking_helpers.c b/src/compiler/nir/nir_linking_helpers.c
index f3be6b1c6f6..f3c85791d54 100644
--- a/src/compiler/nir/nir_linking_helpers.c
+++ b/src/compiler/nir/nir_linking_helpers.c
@@ -120,11 +120,6 @@ tcs_add_output_reads(nir_shader *shader, uint64_t *read, uint64_t *patches_read)
  * progress = nir_remove_unused_io_vars(producer, nir_var_shader_out,
  *                                      read, patches_read) ||
  *                                      progress;
- *
- * The "used" should be an array of 4 uint64_ts (probably of VARYING_BIT_*)
- * representing each .location_frac used.  Note that for vector variables,
- * only the first channel (.location_frac) is examined for deciding if the
- * variable is used!
  */
 bool
 nir_remove_unused_io_vars(nir_shader *shader,
@@ -153,7 +148,9 @@ nir_remove_unused_io_vars(nir_shader *shader,
       if (var->data.explicit_xfb_buffer)
          continue;
 
-      uint64_t other_stage = used[var->data.location_frac];
+      uint64_t other_stage = 0;
+      for (unsigned i = 0; i < get_num_components(var); i++)
+         other_stage |= used[var->data.location_frac + i];
 
       if (!(other_stage & get_variable_io_mask(var, shader->info.stage))) {
          /* This one is invalid, make it a global variable instead */
@@ -1495,7 +1492,7 @@ nir_assign_io_var_locations(nir_shader *shader, nir_variable_mode mode,
                             unsigned *size, gl_shader_stage stage)
 {
    unsigned location = 0;
-   unsigned assigned_locations[VARYING_SLOT_TESS_MAX];
+   unsigned assigned_locations[VARYING_SLOT_TESS_MAX][2];
    uint64_t processed_locs[2] = { 0 };
 
    struct exec_list io_vars;
@@ -1587,7 +1584,7 @@ nir_assign_io_var_locations(nir_shader *shader, nir_variable_mode mode,
       if (processed) {
          /* TODO handle overlapping per-view variables */
          assert(!var->data.per_view);
-         unsigned driver_location = assigned_locations[var->data.location];
+         unsigned driver_location = assigned_locations[var->data.location][var->data.index];
          var->data.driver_location = driver_location;
 
          /* An array may be packed such that is crosses multiple other arrays
@@ -1608,7 +1605,7 @@ nir_assign_io_var_locations(nir_shader *shader, nir_variable_mode mode,
             unsigned num_unallocated_slots = last_slot_location - location;
             unsigned first_unallocated_slot = var_size - num_unallocated_slots;
             for (unsigned i = first_unallocated_slot; i < var_size; i++) {
-               assigned_locations[var->data.location + i] = location;
+               assigned_locations[var->data.location + i][var->data.index] = location;
                location++;
             }
          }
@@ -1616,7 +1613,7 @@ nir_assign_io_var_locations(nir_shader *shader, nir_variable_mode mode,
       }
 
       for (unsigned i = 0; i < var_size; i++) {
-         assigned_locations[var->data.location + i] = location + i;
+         assigned_locations[var->data.location + i][var->data.index] = location + i;
       }
 
       var->data.driver_location = location;
diff --git a/src/compiler/nir/nir_lower_blend.c b/src/compiler/nir/nir_lower_blend.c
index c35d33817a7..b38f3c330f5 100644
--- a/src/compiler/nir/nir_lower_blend.c
+++ b/src/compiler/nir/nir_lower_blend.c
@@ -319,9 +319,12 @@ nir_blend_logicop(
    if (util_format_is_float(format) || util_format_is_srgb(format))
       return src;
 
+   nir_alu_type type =
+      util_format_is_pure_integer(format) ? nir_type_uint : nir_type_float;
+
    if (bit_size != 32) {
-      src = nir_f2f32(b, src);
-      dst = nir_f2f32(b, dst);
+      src = nir_convert_to_bit_size(b, src, type, 32);
+      dst = nir_convert_to_bit_size(b, dst, type, 32);
    }
 
    assert(src->num_components <= 4);
@@ -358,8 +361,8 @@ nir_blend_logicop(
       assert(util_format_is_pure_integer(format));
    }
 
-   if (bit_size == 16)
-      out = nir_f2f16(b, out);
+   if (bit_size != 32)
+      out = nir_convert_to_bit_size(b, out, type, bit_size);
 
    return out;
 }
diff --git a/src/compiler/nir/nir_lower_cl_images.c b/src/compiler/nir/nir_lower_cl_images.c
index c172128b650..13063593aad 100644
--- a/src/compiler/nir/nir_lower_cl_images.c
+++ b/src/compiler/nir/nir_lower_cl_images.c
@@ -161,6 +161,7 @@ nir_lower_cl_images(nir_shader *shader, bool lower_image_derefs, bool lower_samp
          assert(var->data.location > last_loc);
          last_loc = var->data.location;
          var->data.driver_location = num_samplers++;
+         var->data.binding = var->data.driver_location;
       } else {
          /* CL shouldn't have any sampled images */
          assert(!glsl_type_is_sampler(var->type));
diff --git a/src/compiler/nir/nir_lower_clamp_color_outputs.c b/src/compiler/nir/nir_lower_clamp_color_outputs.c
index c13e90705d7..37e40f89a28 100644
--- a/src/compiler/nir/nir_lower_clamp_color_outputs.c
+++ b/src/compiler/nir/nir_lower_clamp_color_outputs.c
@@ -25,13 +25,13 @@
 #include "nir_builder.h"
 
 static bool
-is_color_output(nir_shader *shader, nir_variable *out)
+is_color_output(nir_shader *shader, int location)
 {
    switch (shader->info.stage) {
    case MESA_SHADER_VERTEX:
    case MESA_SHADER_GEOMETRY:
    case MESA_SHADER_TESS_EVAL:
-      switch (out->data.location) {
+      switch (location) {
       case VARYING_SLOT_COL0:
       case VARYING_SLOT_COL1:
       case VARYING_SLOT_BFC0:
@@ -42,8 +42,8 @@ is_color_output(nir_shader *shader, nir_variable *out)
       }
       break;
    case MESA_SHADER_FRAGMENT:
-      return (out->data.location == FRAG_RESULT_COLOR ||
-              out->data.location >= FRAG_RESULT_DATA0);
+      return (location == FRAG_RESULT_COLOR ||
+              location >= FRAG_RESULT_DATA0);
    default:
       return false;
    }
@@ -54,30 +54,23 @@ lower_intrinsic(nir_builder *b, nir_intrinsic_instr *intr, nir_shader *shader)
 {
    nir_variable *out = NULL;
    nir_def *s;
+   int loc = -1;
 
    switch (intr->intrinsic) {
    case nir_intrinsic_store_deref:
       out = nir_intrinsic_get_var(intr, 0);
+      if (out->data.mode != nir_var_shader_out)
+         return false;
+      loc = out->data.location;
       break;
    case nir_intrinsic_store_output:
-      /* already had i/o lowered.. lookup the matching output var: */
-      nir_foreach_shader_out_variable(var, shader) {
-         int drvloc = var->data.driver_location;
-         if (nir_intrinsic_base(intr) == drvloc) {
-            out = var;
-            break;
-         }
-      }
-      assume(out);
+      loc = nir_intrinsic_io_semantics(intr).location;
       break;
    default:
       return false;
    }
 
-   if (out->data.mode != nir_var_shader_out)
-      return false;
-
-   if (is_color_output(shader, out)) {
+   if (is_color_output(shader, loc)) {
       b->cursor = nir_before_instr(&intr->instr);
       int src = intr->intrinsic == nir_intrinsic_store_deref ? 1 : 0;
       s = intr->src[src].ssa;
diff --git a/src/compiler/nir/nir_lower_tex_shadow.c b/src/compiler/nir/nir_lower_tex_shadow.c
index 882a5cc2003..ec077a60d97 100644
--- a/src/compiler/nir/nir_lower_tex_shadow.c
+++ b/src/compiler/nir/nir_lower_tex_shadow.c
@@ -115,7 +115,7 @@ nir_lower_tex_shadow_impl(nir_builder *b, nir_instr *instr, void *options)
    nir_def *one = nir_imm_float(b, 1.0);
    nir_def *zero = nir_imm_float(b, 0.0);
 
-   nir_def *lookup[6] = { result, NULL, NULL, NULL, zero, one };
+   nir_def *lookup[6] = { result, zero, zero, one, zero, one };
    nir_def *r[4] = { result, result, result, result };
 
    if (sampler_binding < state->n_states) {
diff --git a/src/compiler/nir/nir_lower_texcoord_replace.c b/src/compiler/nir/nir_lower_texcoord_replace.c
index 17991f3df41..6695ccf8022 100644
--- a/src/compiler/nir/nir_lower_texcoord_replace.c
+++ b/src/compiler/nir/nir_lower_texcoord_replace.c
@@ -107,6 +107,7 @@ nir_lower_texcoord_replace_impl(nir_function_impl *impl,
          unsigned base = var->data.location - VARYING_SLOT_TEX0;
 
          b.cursor = nir_after_instr(instr);
+         uint32_t component_mask = BITFIELD_MASK(glsl_get_vector_elements(var->type)) << var->data.location_frac;
          nir_deref_instr *deref = nir_src_as_deref(intrin->src[0]);
          nir_def *index = get_io_index(&b, deref);
          nir_def *mask =
@@ -114,7 +115,7 @@ nir_lower_texcoord_replace_impl(nir_function_impl *impl,
                      nir_iadd_imm(&b, index, base));
 
          nir_def *cond = nir_test_mask(&b, mask, coord_replace);
-         nir_def *result = nir_bcsel(&b, cond, new_coord,
+         nir_def *result = nir_bcsel(&b, cond, nir_channels(&b, new_coord, component_mask),
                                      &intrin->def);
 
          nir_def_rewrite_uses_after(&intrin->def,
diff --git a/src/compiler/nir/nir_opt_preamble.c b/src/compiler/nir/nir_opt_preamble.c
index 29c0af232b2..3561839eb81 100644
--- a/src/compiler/nir/nir_opt_preamble.c
+++ b/src/compiler/nir/nir_opt_preamble.c
@@ -173,6 +173,7 @@ can_move_intrinsic(nir_intrinsic_instr *instr, opt_preamble_ctx *ctx)
    case nir_intrinsic_load_line_width:
    case nir_intrinsic_load_aa_line_width:
    case nir_intrinsic_load_fb_layers_v3d:
+   case nir_intrinsic_load_fep_w_v3d:
    case nir_intrinsic_load_tcs_num_patches_amd:
    case nir_intrinsic_load_sample_positions_pan:
    case nir_intrinsic_load_pipeline_stat_query_enabled_amd:
diff --git a/src/compiler/nir/nir_serialize.c b/src/compiler/nir/nir_serialize.c
index 64965b847ec..0930f0e7aeb 100644
--- a/src/compiler/nir/nir_serialize.c
+++ b/src/compiler/nir/nir_serialize.c
@@ -202,8 +202,6 @@ read_constant(read_ctx *ctx, nir_variable *nvar)
 
 enum var_data_encoding {
    var_encode_full,
-   var_encode_shader_temp,
-   var_encode_function_temp,
    var_encode_location_diff,
 };
 
@@ -264,30 +262,23 @@ write_variable(write_ctx *ctx, const nir_variable *var)
        data.mode != nir_var_shader_out)
       data.location = 0;
 
-   /* Temporary variables don't serialize var->data. */
-   if (data.mode == nir_var_shader_temp)
-      flags.u.data_encoding = var_encode_shader_temp;
-   else if (data.mode == nir_var_function_temp)
-      flags.u.data_encoding = var_encode_function_temp;
-   else {
-      struct nir_variable_data tmp = data;
+   struct nir_variable_data tmp = data;
 
-      tmp.location = ctx->last_var_data.location;
-      tmp.location_frac = ctx->last_var_data.location_frac;
-      tmp.driver_location = ctx->last_var_data.driver_location;
+   tmp.location = ctx->last_var_data.location;
+   tmp.location_frac = ctx->last_var_data.location_frac;
+   tmp.driver_location = ctx->last_var_data.driver_location;
 
-      /* See if we can encode only the difference in locations from the last
-       * variable.
-       */
-      if (memcmp(&ctx->last_var_data, &tmp, sizeof(tmp)) == 0 &&
-          abs((int)data.location -
-              (int)ctx->last_var_data.location) < (1 << 12) &&
-          abs((int)data.driver_location -
-              (int)ctx->last_var_data.driver_location) < (1 << 15))
-         flags.u.data_encoding = var_encode_location_diff;
-      else
-         flags.u.data_encoding = var_encode_full;
-   }
+   /* See if we can encode only the difference in locations from the last
+    * variable.
+    */
+   if (memcmp(&ctx->last_var_data, &tmp, sizeof(tmp)) == 0 &&
+       abs((int)data.location -
+           (int)ctx->last_var_data.location) < (1 << 12) &&
+       abs((int)data.driver_location -
+           (int)ctx->last_var_data.driver_location) < (1 << 15))
+      flags.u.data_encoding = var_encode_location_diff;
+   else
+      flags.u.data_encoding = var_encode_full;
 
    flags.u.ray_query = var->data.ray_query;
 
@@ -306,27 +297,24 @@ write_variable(write_ctx *ctx, const nir_variable *var)
    if (flags.u.has_name)
       blob_write_string(ctx->blob, var->name);
 
-   if (flags.u.data_encoding == var_encode_full ||
-       flags.u.data_encoding == var_encode_location_diff) {
-      if (flags.u.data_encoding == var_encode_full) {
-         blob_write_bytes(ctx->blob, &data, sizeof(data));
-      } else {
-         /* Serialize only the difference in locations from the last variable.
-          */
-         union packed_var_data_diff diff;
-
-         diff.u.location = data.location - ctx->last_var_data.location;
-         diff.u.location_frac = data.location_frac -
-                                ctx->last_var_data.location_frac;
-         diff.u.driver_location = data.driver_location -
-                                  ctx->last_var_data.driver_location;
+   if (flags.u.data_encoding == var_encode_full) {
+      blob_write_bytes(ctx->blob, &data, sizeof(data));
+   } else {
+      /* Serialize only the difference in locations from the last variable.
+       */
+      union packed_var_data_diff diff;
 
-         blob_write_uint32(ctx->blob, diff.u32);
-      }
+      diff.u.location = data.location - ctx->last_var_data.location;
+      diff.u.location_frac = data.location_frac -
+                             ctx->last_var_data.location_frac;
+      diff.u.driver_location = data.driver_location -
+                               ctx->last_var_data.driver_location;
 
-      ctx->last_var_data = data;
+      blob_write_uint32(ctx->blob, diff.u32);
    }
 
+   ctx->last_var_data = data;
+
    for (unsigned i = 0; i < var->num_state_slots; i++) {
       blob_write_bytes(ctx->blob, &var->state_slots[i],
                        sizeof(var->state_slots[i]));
@@ -374,11 +362,7 @@ read_variable(read_ctx *ctx)
       var->name = NULL;
    }
 
-   if (flags.u.data_encoding == var_encode_shader_temp)
-      var->data.mode = nir_var_shader_temp;
-   else if (flags.u.data_encoding == var_encode_function_temp)
-      var->data.mode = nir_var_function_temp;
-   else if (flags.u.data_encoding == var_encode_full) {
+   if (flags.u.data_encoding == var_encode_full) {
       blob_copy_bytes(ctx->blob, (uint8_t *)&var->data, sizeof(var->data));
       ctx->last_var_data = var->data;
    } else { /* var_encode_location_diff */
diff --git a/src/compiler/spirv/spirv_to_nir.c b/src/compiler/spirv/spirv_to_nir.c
index ed2a003d722..f57c9ba42a2 100644
--- a/src/compiler/spirv/spirv_to_nir.c
+++ b/src/compiler/spirv/spirv_to_nir.c
@@ -4383,6 +4383,7 @@ vtn_handle_composite(struct vtn_builder *b, SpvOp opcode,
       break;
    }
    case SpvOpCopyObject:
+   case SpvOpExpectKHR:
       vtn_copy_value(b, w[3], w[2]);
       return;
 
@@ -6458,18 +6459,18 @@ vtn_handle_body_instruction(struct vtn_builder *b, SpvOp opcode,
       vtn_handle_integer_dot(b, opcode, w, count);
       break;
 
+   case SpvOpBitcast:
+      vtn_handle_bitcast(b, w, count);
+      break;
+
    /* TODO: One day, we should probably do something with this information
     * For now, though, it's safe to implement them as no-ops.
     * Needed for Rusticl sycl support.
     */
    case SpvOpAssumeTrueKHR:
-   case SpvOpExpectKHR:
-      break;
-
-   case SpvOpBitcast:
-      vtn_handle_bitcast(b, w, count);
       break;
 
+   case SpvOpExpectKHR:
    case SpvOpVectorExtractDynamic:
    case SpvOpVectorInsertDynamic:
    case SpvOpVectorShuffle:
diff --git a/src/compiler/spirv/vtn_opencl.c b/src/compiler/spirv/vtn_opencl.c
index 59ec75da885..5785057da1d 100644
--- a/src/compiler/spirv/vtn_opencl.c
+++ b/src/compiler/spirv/vtn_opencl.c
@@ -743,8 +743,15 @@ vtn_add_printf_string(struct vtn_builder *b, uint32_t id, u_printf_info *info)
 {
    nir_deref_instr *deref = vtn_nir_deref(b, id);
 
-   while (deref && deref->deref_type != nir_deref_type_var)
-      deref = nir_deref_instr_parent(deref);
+   while (deref->deref_type != nir_deref_type_var) {
+      nir_scalar parent = nir_scalar_resolved(deref->parent.ssa, 0);
+      if (parent.def->parent_instr->type != nir_instr_type_deref) {
+         deref = NULL;
+         break;
+      }
+      vtn_assert(parent.comp == 0);
+      deref = nir_instr_as_deref(parent.def->parent_instr);
+   }
 
    vtn_fail_if(deref == NULL || !nir_deref_mode_is(deref, nir_var_mem_constant),
                "Printf string argument must be a pointer to a constant variable");
diff --git a/src/compiler/spirv/vtn_variables.c b/src/compiler/spirv/vtn_variables.c
index ce4b19134ab..49c07b950c9 100644
--- a/src/compiler/spirv/vtn_variables.c
+++ b/src/compiler/spirv/vtn_variables.c
@@ -2024,7 +2024,9 @@ adjust_patch_locations(struct vtn_builder *b, struct vtn_variable *var)
 
    for (uint16_t i = 0; i < num_data; i++) {
       vtn_assert(data[i].location < VARYING_SLOT_PATCH0);
-      if (data[i].patch && data[i].location >= VARYING_SLOT_VAR0)
+      if (data[i].patch &&
+          (data[i].mode == nir_var_shader_in || data[i].mode == nir_var_shader_out) &&
+          data[i].location >= VARYING_SLOT_VAR0)
          data[i].location += VARYING_SLOT_PATCH0 - VARYING_SLOT_VAR0;
    }
 }
diff --git a/src/drm-shim/drm_shim.c b/src/drm-shim/drm_shim.c
index f7481aa293b..3597fec2e97 100644
--- a/src/drm-shim/drm_shim.c
+++ b/src/drm-shim/drm_shim.c
@@ -30,6 +30,7 @@
 
 /* Prevent glibc from defining open64 when we want to alias it. */
 #undef _FILE_OFFSET_BITS
+#undef _TIME_BITS
 #define _LARGEFILE64_SOURCE
 
 #include <stdbool.h>
diff --git a/src/egl/drivers/dri2/egl_dri2.c b/src/egl/drivers/dri2/egl_dri2.c
index f98bc0308f5..cd23a796a67 100644
--- a/src/egl/drivers/dri2/egl_dri2.c
+++ b/src/egl/drivers/dri2/egl_dri2.c
@@ -1239,7 +1239,7 @@ dri2_setup_extensions(_EGLDisplay *disp)
 
    extensions = dri2_dpy->core->getExtensions(dri2_dpy->dri_screen_render_gpu);
 
-   if (dri2_dpy->image_driver || dri2_dpy->dri2 || disp->Options.Zink) {
+   if (dri2_dpy->image_driver || dri2_dpy->dri2) {
       if (!loader_bind_extensions(dri2_dpy, dri2_core_extensions,
                                   ARRAY_SIZE(dri2_core_extensions), extensions))
          return EGL_FALSE;
@@ -1259,16 +1259,15 @@ dri2_setup_extensions(_EGLDisplay *disp)
        (dri2_dpy->present_major_version == 1 &&
         dri2_dpy->present_minor_version >= 2)) &&
       (dri2_dpy->image && dri2_dpy->image->base.version >= 15);
-#endif
    if (disp->Options.Zink && !disp->Options.ForceSoftware &&
-#ifdef HAVE_DRI3_MODIFIERS
        dri2_dpy->dri3_major_version != -1 &&
        !dri2_dpy->multibuffers_available &&
-#endif
-       (disp->Platform == EGL_PLATFORM_X11_KHR ||
-        disp->Platform == EGL_PLATFORM_XCB_EXT) &&
+       /* this is enum _egl_platform_type */
+       (disp->Platform == _EGL_PLATFORM_X11 ||
+        disp->Platform == _EGL_PLATFORM_XCB) &&
        !debug_get_bool_option("LIBGL_KOPPER_DRI2", false))
       return EGL_FALSE;
+#endif
 
    loader_bind_extensions(dri2_dpy, optional_core_extensions,
                           ARRAY_SIZE(optional_core_extensions), extensions);
diff --git a/src/egl/drivers/dri2/platform_android.c b/src/egl/drivers/dri2/platform_android.c
index 76fbef104de..8e2b705ea9b 100644
--- a/src/egl/drivers/dri2/platform_android.c
+++ b/src/egl/drivers/dri2/platform_android.c
@@ -977,11 +977,14 @@ droid_load_driver(_EGLDisplay *disp, bool swrast)
 {
    struct dri2_egl_display *dri2_dpy = dri2_egl_display(disp);
 
-   dri2_dpy->driver_name = loader_get_driver_for_fd(dri2_dpy->fd_render_gpu);
+   if (disp->Options.Zink)
+      dri2_dpy->driver_name = strdup("zink");
+   else
+      dri2_dpy->driver_name = loader_get_driver_for_fd(dri2_dpy->fd_render_gpu);
    if (dri2_dpy->driver_name == NULL)
       return false;
 
-   if (swrast) {
+   if (swrast && !disp->Options.Zink) {
       /* Use kms swrast only with vgem / virtio_gpu.
        * virtio-gpu fallbacks to software rendering when 3D features
        * are unavailable since 6c5ab.
diff --git a/src/egl/drivers/dri2/platform_wayland.c b/src/egl/drivers/dri2/platform_wayland.c
index bd432a8195f..1f718ef8a74 100644
--- a/src/egl/drivers/dri2/platform_wayland.c
+++ b/src/egl/drivers/dri2/platform_wayland.c
@@ -73,6 +73,7 @@ static const struct dri2_wl_visual {
     */
    int alt_dri_image_format;
    int bpp;
+   int opaque_wl_drm_format;
    int rgba_shifts[4];
    unsigned int rgba_sizes[4];
 } dri2_wl_visuals[] = {
@@ -83,6 +84,7 @@ static const struct dri2_wl_visual {
       __DRI_IMAGE_FORMAT_ABGR16161616F,
       0,
       64,
+      WL_DRM_FORMAT_XBGR16F,
       {0, 16, 32, 48},
       {16, 16, 16, 16},
    },
@@ -93,6 +95,7 @@ static const struct dri2_wl_visual {
       __DRI_IMAGE_FORMAT_XBGR16161616F,
       0,
       64,
+      WL_DRM_FORMAT_XBGR16F,
       {0, 16, 32, -1},
       {16, 16, 16, 0},
    },
@@ -103,6 +106,7 @@ static const struct dri2_wl_visual {
       __DRI_IMAGE_FORMAT_XRGB2101010,
       __DRI_IMAGE_FORMAT_XBGR2101010,
       32,
+      WL_DRM_FORMAT_XRGB2101010,
       {20, 10, 0, -1},
       {10, 10, 10, 0},
    },
@@ -113,6 +117,7 @@ static const struct dri2_wl_visual {
       __DRI_IMAGE_FORMAT_ARGB2101010,
       __DRI_IMAGE_FORMAT_ABGR2101010,
       32,
+      WL_DRM_FORMAT_XRGB2101010,
       {20, 10, 0, 30},
       {10, 10, 10, 2},
    },
@@ -123,6 +128,7 @@ static const struct dri2_wl_visual {
       __DRI_IMAGE_FORMAT_XBGR2101010,
       __DRI_IMAGE_FORMAT_XRGB2101010,
       32,
+      WL_DRM_FORMAT_XBGR2101010,
       {0, 10, 20, -1},
       {10, 10, 10, 0},
    },
@@ -133,6 +139,7 @@ static const struct dri2_wl_visual {
       __DRI_IMAGE_FORMAT_ABGR2101010,
       __DRI_IMAGE_FORMAT_ARGB2101010,
       32,
+      WL_DRM_FORMAT_XBGR2101010,
       {0, 10, 20, 30},
       {10, 10, 10, 2},
    },
@@ -143,6 +150,7 @@ static const struct dri2_wl_visual {
       __DRI_IMAGE_FORMAT_XRGB8888,
       __DRI_IMAGE_FORMAT_NONE,
       32,
+      WL_DRM_FORMAT_XRGB8888,
       {16, 8, 0, -1},
       {8, 8, 8, 0},
    },
@@ -153,6 +161,7 @@ static const struct dri2_wl_visual {
       __DRI_IMAGE_FORMAT_ARGB8888,
       __DRI_IMAGE_FORMAT_NONE,
       32,
+      WL_DRM_FORMAT_XRGB8888,
       {16, 8, 0, 24},
       {8, 8, 8, 8},
    },
@@ -163,6 +172,7 @@ static const struct dri2_wl_visual {
       __DRI_IMAGE_FORMAT_ABGR8888,
       __DRI_IMAGE_FORMAT_NONE,
       32,
+      WL_DRM_FORMAT_XBGR8888,
       {0, 8, 16, 24},
       {8, 8, 8, 8},
    },
@@ -173,6 +183,7 @@ static const struct dri2_wl_visual {
       __DRI_IMAGE_FORMAT_XBGR8888,
       __DRI_IMAGE_FORMAT_NONE,
       32,
+      WL_DRM_FORMAT_XBGR8888,
       {0, 8, 16, -1},
       {8, 8, 8, 0},
    },
@@ -183,6 +194,7 @@ static const struct dri2_wl_visual {
       __DRI_IMAGE_FORMAT_RGB565,
       __DRI_IMAGE_FORMAT_NONE,
       16,
+      WL_DRM_FORMAT_RGB565,
       {11, 5, 0, -1},
       {5, 6, 5, 0},
    },
@@ -193,6 +205,7 @@ static const struct dri2_wl_visual {
       __DRI_IMAGE_FORMAT_ARGB1555,
       __DRI_IMAGE_FORMAT_ABGR1555,
       16,
+      WL_DRM_FORMAT_XRGB1555,
       {10, 5, 0, 15},
       {5, 5, 5, 1},
    },
@@ -203,6 +216,7 @@ static const struct dri2_wl_visual {
       __DRI_IMAGE_FORMAT_XRGB1555,
       __DRI_IMAGE_FORMAT_XBGR1555,
       16,
+      WL_DRM_FORMAT_XRGB1555,
       {10, 5, 0, -1},
       {5, 5, 5, 0},
    },
@@ -213,6 +227,7 @@ static const struct dri2_wl_visual {
       __DRI_IMAGE_FORMAT_ARGB4444,
       __DRI_IMAGE_FORMAT_XBGR4444,
       16,
+      WL_DRM_FORMAT_XRGB4444,
       {8, 4, 0, 12},
       {4, 4, 4, 4},
    },
@@ -223,6 +238,7 @@ static const struct dri2_wl_visual {
       __DRI_IMAGE_FORMAT_XRGB4444,
       __DRI_IMAGE_FORMAT_XBGR4444,
       16,
+      WL_DRM_FORMAT_XRGB4444,
       {8, 4, 0, -1},
       {4, 4, 4, 0},
    },
@@ -240,7 +256,7 @@ static const struct dri2_wl_visual {
 
 static int
 dri2_wl_visual_idx_from_config(struct dri2_egl_display *dri2_dpy,
-                               const __DRIconfig *config, bool force_opaque)
+                               const __DRIconfig *config)
 {
    int shifts[4];
    unsigned int sizes[4];
@@ -250,16 +266,13 @@ dri2_wl_visual_idx_from_config(struct dri2_egl_display *dri2_dpy,
    for (unsigned int i = 0; i < ARRAY_SIZE(dri2_wl_visuals); i++) {
       const struct dri2_wl_visual *wl_visual = &dri2_wl_visuals[i];
 
-      int cmp_rgb_shifts =
-         memcmp(shifts, wl_visual->rgba_shifts, 3 * sizeof(shifts[0]));
-      int cmp_rgb_sizes =
-         memcmp(sizes, wl_visual->rgba_sizes, 3 * sizeof(sizes[0]));
+      int cmp_rgba_shifts =
+         memcmp(shifts, wl_visual->rgba_shifts, 4 * sizeof(shifts[0]));
+      int cmp_rgba_sizes =
+         memcmp(sizes, wl_visual->rgba_sizes, 4 * sizeof(sizes[0]));
 
-      if (cmp_rgb_shifts == 0 && cmp_rgb_sizes == 0 &&
-          wl_visual->rgba_shifts[3] == (force_opaque ? -1 : shifts[3]) &&
-          wl_visual->rgba_sizes[3] == (force_opaque ? 0 : sizes[3])) {
+      if (cmp_rgba_shifts == 0 && cmp_rgba_sizes == 0)
          return i;
-      }
    }
 
    return -1;
@@ -312,7 +325,7 @@ dri2_wl_is_format_supported(void *user_data, uint32_t format)
 
    for (int i = 0; dri2_dpy->driver_configs[i]; i++)
       if (j == dri2_wl_visual_idx_from_config(
-                  dri2_dpy, dri2_dpy->driver_configs[i], false))
+                  dri2_dpy, dri2_dpy->driver_configs[i]))
          return true;
 
    return false;
@@ -724,43 +737,10 @@ dri2_wl_create_window_surface(_EGLDisplay *disp, _EGLConfig *conf,
    dri2_surf->base.Width = window->width;
    dri2_surf->base.Height = window->height;
 
-#ifndef NDEBUG
-   /* Enforce that every visual has an opaque variant (requirement to support
-    * EGL_EXT_present_opaque)
-    */
-   for (unsigned int i = 0; i < ARRAY_SIZE(dri2_wl_visuals); i++) {
-      const struct dri2_wl_visual *transparent_visual = &dri2_wl_visuals[i];
-      if (transparent_visual->rgba_sizes[3] == 0) {
-         continue;
-      }
-
-      bool found_opaque_equivalent = false;
-      for (unsigned int j = 0; j < ARRAY_SIZE(dri2_wl_visuals); j++) {
-         const struct dri2_wl_visual *opaque_visual = &dri2_wl_visuals[j];
-         if (opaque_visual->rgba_sizes[3] != 0) {
-            continue;
-         }
-
-         int cmp_rgb_shifts =
-            memcmp(transparent_visual->rgba_shifts, opaque_visual->rgba_shifts,
-                   3 * sizeof(opaque_visual->rgba_shifts[0]));
-         int cmp_rgb_sizes =
-            memcmp(transparent_visual->rgba_sizes, opaque_visual->rgba_sizes,
-                   3 * sizeof(opaque_visual->rgba_sizes[0]));
-
-         if (cmp_rgb_shifts == 0 && cmp_rgb_sizes == 0) {
-            found_opaque_equivalent = true;
-            break;
-         }
-      }
-
-      assert(found_opaque_equivalent);
-   }
-#endif
-
-   visual_idx = dri2_wl_visual_idx_from_config(dri2_dpy, config,
-                                               dri2_surf->base.PresentOpaque);
+   visual_idx = dri2_wl_visual_idx_from_config(dri2_dpy, config);
    assert(visual_idx != -1);
+   assert(dri2_wl_visuals[visual_idx].dri_image_format !=
+          __DRI_IMAGE_FORMAT_NONE);
 
    if (dri2_dpy->wl_dmabuf || dri2_dpy->wl_drm) {
       dri2_surf->format = dri2_wl_visuals[visual_idx].wl_drm_format;
@@ -1724,10 +1704,13 @@ create_wl_buffer(struct dri2_egl_display *dri2_dpy,
          close(fd);
       }
 
+      if (dri2_surf && dri2_surf->base.PresentOpaque)
+         fourcc = dri2_wl_visuals[visual_idx].opaque_wl_drm_format;
+
       ret = zwp_linux_buffer_params_v1_create_immed(params, width, height,
                                                     fourcc, 0);
       zwp_linux_buffer_params_v1_destroy(params);
-   } else {
+   } else if (dri2_dpy->wl_drm) {
       struct wl_drm *wl_drm =
          dri2_surf ? dri2_surf->wl_drm_wrapper : dri2_dpy->wl_drm;
       int fd = -1, stride;
@@ -1870,6 +1853,12 @@ dri2_wl_swap_buffers_with_damage(_EGLDisplay *disp, _EGLSurface *draw,
          dri2_surf->current->dri_image, 0, 0, dri2_surf->base.Width,
          dri2_surf->base.Height, 0, 0, dri2_surf->base.Width,
          dri2_surf->base.Height, __BLIT_FLAG_FLUSH);
+
+      if (dri2_dpy->flush) {
+         __DRIdrawable *dri_drawable = dri2_dpy->vtbl->get_dri_drawable(draw);
+
+         dri2_dpy->flush->flush(dri_drawable);
+      }
    }
 
    wl_surface_commit(dri2_surf->wl_surface_wrapper);
@@ -2320,7 +2309,7 @@ dri2_wl_add_configs_for_visuals(_EGLDisplay *disp, bool allow_preserve)
 
          /* No match for config. Try if we can blitImage convert to a visual */
          c = dri2_wl_visual_idx_from_config(dri2_dpy,
-                                            dri2_dpy->driver_configs[i], false);
+                                            dri2_dpy->driver_configs[i]);
 
          if (c == -1)
             continue;
diff --git a/src/egl/drivers/dri2/platform_x11.c b/src/egl/drivers/dri2/platform_x11.c
index 10a78a403d9..cc8bc19b2cd 100644
--- a/src/egl/drivers/dri2/platform_x11.c
+++ b/src/egl/drivers/dri2/platform_x11.c
@@ -1523,7 +1523,8 @@ dri2_initialize_x11_swrast(_EGLDisplay *disp)
     */
    dri2_dpy->driver_name = strdup(disp->Options.Zink ? "zink" : "swrast");
    if (disp->Options.Zink &&
-       !debug_get_bool_option("LIBGL_DRI3_DISABLE", false))
+       !debug_get_bool_option("LIBGL_DRI3_DISABLE", false) &&
+       !debug_get_bool_option("LIBGL_KOPPER_DRI2", false))
       dri3_x11_connect(dri2_dpy);
    if (!dri2_load_driver_swrast(disp))
       goto cleanup;
diff --git a/src/freedreno/.gitlab-ci/reference/afuc_test.asm b/src/freedreno/.gitlab-ci/reference/afuc_test.asm
index 0bbdf37a4ba..b97dbaf2af7 100644
--- a/src/freedreno/.gitlab-ci/reference/afuc_test.asm
+++ b/src/freedreno/.gitlab-ci/reference/afuc_test.asm
@@ -162,7 +162,6 @@ nop
 CP_BLIT:
 CP_BOOTSTRAP_UCODE:
 CP_COND_EXEC:
-CP_COND_INDIRECT_BUFFER_PFE:
 CP_COND_REG_EXEC:
 CP_COND_WRITE5:
 CP_CONTEXT_REG_BUNCH:
@@ -268,6 +267,7 @@ UNKN31:
 UNKN32:
 UNKN48:
 UNKN5:
+UNKN58:
 UNKN6:
 UNKN7:
 UNKN73:
diff --git a/src/freedreno/.gitlab-ci/traces/afuc_test.asm b/src/freedreno/.gitlab-ci/traces/afuc_test.asm
index e8d6f8b2b42..f60ee28667d 100644
--- a/src/freedreno/.gitlab-ci/traces/afuc_test.asm
+++ b/src/freedreno/.gitlab-ci/traces/afuc_test.asm
@@ -269,7 +269,7 @@ CP_LOAD_STATE6:
 CP_INDIRECT_BUFFER_PFD:
 CP_DRAW_INDX_OFFSET:
 CP_REG_TEST:
-CP_COND_INDIRECT_BUFFER_PFE:
+UNKN58:
 CP_INVALIDATE_STATE:
 CP_WAIT_REG_MEM:
 CP_REG_TO_MEM:
diff --git a/src/freedreno/ci/gitlab-ci-inc.yml b/src/freedreno/ci/gitlab-ci-inc.yml
index 5367b48c8cf..f71dcb1c067 100644
--- a/src/freedreno/ci/gitlab-ci-inc.yml
+++ b/src/freedreno/ci/gitlab-ci-inc.yml
@@ -292,25 +292,6 @@
   tags:
     - google-freedreno-db410c
 
-# New jobs. Leave it as manual for now.
-.a306_piglit:
-  extends:
-    - .piglit-test
-    - .a306-test
-    - .google-freedreno-manual-rules
-  variables:
-    HWCI_START_XORG: 1
-
-# Something happened and now this hangchecks and doesn't recover.  Unkown when
-# it started.
-.a306_piglit_gl:
-  extends:
-    - .a306_piglit
-  variables:
-    PIGLIT_PROFILES: quick_gl
-    BM_KERNEL_EXTRA_ARGS: "msm.num_hw_submissions=1"
-    FDO_CI_CONCURRENT: 3
-
 # 8 devices (2023-04-15)
 .a530-test:
   extends:
diff --git a/src/freedreno/ci/gitlab-ci.yml b/src/freedreno/ci/gitlab-ci.yml
index f3c0b1738e2..9d19c21502a 100644
--- a/src/freedreno/ci/gitlab-ci.yml
+++ b/src/freedreno/ci/gitlab-ci.yml
@@ -10,6 +10,25 @@ a306_gl:
     FDO_CI_CONCURRENT: 6
   parallel: 5
 
+# New jobs. Leave it as manual for now.
+.a306_piglit:
+  extends:
+    - .piglit-test
+    - .a306-test
+    - .google-freedreno-manual-rules
+  variables:
+    HWCI_START_XORG: 1
+
+# Something happened and now this hangchecks and doesn't recover.  Unkown when
+# it started.
+.a306_piglit_gl:
+  extends:
+    - .a306_piglit
+  variables:
+    PIGLIT_PROFILES: quick_gl
+    BM_KERNEL_EXTRA_ARGS: "msm.num_hw_submissions=1"
+    FDO_CI_CONCURRENT: 3
+
 a306_piglit_shader:
   extends:
     - .a306_piglit
diff --git a/src/freedreno/ir3/ir3_spill.c b/src/freedreno/ir3/ir3_spill.c
index 475c132f6fa..9fba8d3ce80 100644
--- a/src/freedreno/ir3/ir3_spill.c
+++ b/src/freedreno/ir3/ir3_spill.c
@@ -673,13 +673,13 @@ get_spill_slot(struct ra_spill_ctx *ctx, struct ir3_register *reg)
    if (reg->merge_set) {
       if (reg->merge_set->spill_slot == ~0) {
          reg->merge_set->spill_slot = ALIGN_POT(ctx->spill_slot,
-                                                reg->merge_set->alignment);
+                                                reg->merge_set->alignment * 2);
          ctx->spill_slot = reg->merge_set->spill_slot + reg->merge_set->size * 2;
       }
       return reg->merge_set->spill_slot + reg->merge_set_offset * 2;
    } else {
       if (reg->spill_slot == ~0) {
-         reg->spill_slot = ALIGN_POT(ctx->spill_slot, reg_elem_size(reg));
+         reg->spill_slot = ALIGN_POT(ctx->spill_slot, reg_elem_size(reg) * 2);
          ctx->spill_slot = reg->spill_slot + reg_size(reg) * 2;
       }
       return reg->spill_slot;
diff --git a/src/freedreno/registers/adreno/adreno_pm4.xml b/src/freedreno/registers/adreno/adreno_pm4.xml
index 1b687eed5a7..d775f3afef8 100644
--- a/src/freedreno/registers/adreno/adreno_pm4.xml
+++ b/src/freedreno/registers/adreno/adreno_pm4.xml
@@ -371,7 +371,7 @@ xsi:schemaLocation="http://nouveau.freedesktop.org/ rules-ng.xsd">
 	<value name="CP_LOAD_STATE" value="0x30" variants="A3XX"/>
 	<value name="CP_LOAD_STATE4" value="0x30" variants="A4XX-A5XX"/>
 	<doc>Conditionally load a IB based on a flag, prefetch enabled</doc>
-	<value name="CP_COND_INDIRECT_BUFFER_PFE" value="0x3a"/>
+	<value name="CP_COND_INDIRECT_BUFFER_PFE" value="0x3a" variants="A3XX-A5XX"/>
 	<doc>Conditionally load a IB based on a flag, prefetch disabled</doc>
 	<value name="CP_COND_INDIRECT_BUFFER_PFD" value="0x32" variants="A3XX"/>
 	<doc>Load a buffer with pre-fetch enabled</doc>
@@ -648,6 +648,9 @@ xsi:schemaLocation="http://nouveau.freedesktop.org/ rules-ng.xsd">
 
 	<doc>Reset various on-chip state used for synchronization</doc>
 	<value name="CP_RESET_CONTEXT_STATE" value="0x1f" variants="A7XX-"/>
+
+	<doc>Invalidates the "CCHE" introduced on a740</doc>
+	<value name="CP_CCHE_INVALIDATE" value="0x3a" variants="A7XX-"/>
 </enum>
 
 
diff --git a/src/freedreno/vulkan/.clang-format b/src/freedreno/vulkan/.clang-format
index f7f9e5755db..8a1ae374067 100644
--- a/src/freedreno/vulkan/.clang-format
+++ b/src/freedreno/vulkan/.clang-format
@@ -20,5 +20,8 @@ IncludeCategories:
   - Regex:           '.*'
     Priority:        1
 
+ForEachMacros:
+  - u_vector_foreach
+
 SpaceAfterCStyleCast: true
 SpaceBeforeCpp11BracedList: true
diff --git a/src/freedreno/vulkan/tu_cmd_buffer.cc b/src/freedreno/vulkan/tu_cmd_buffer.cc
index 091c8cd29d7..206ea4efe78 100644
--- a/src/freedreno/vulkan/tu_cmd_buffer.cc
+++ b/src/freedreno/vulkan/tu_cmd_buffer.cc
@@ -187,6 +187,10 @@ tu6_emit_flushes(struct tu_cmd_buffer *cmd_buffer,
             .gfx_bindless = CHIP == A6XX ? 0x1f : 0xff,
       ));
    }
+   if (CHIP >= A7XX && (flushes & TU_CMD_FLAG_CCHE_INVALIDATE) &&
+       /* Invalidating UCHE seems to also invalidate CCHE */
+       !(flushes & TU_CMD_FLAG_CACHE_INVALIDATE))
+      tu_cs_emit_pkt7(cs, CP_CCHE_INVALIDATE, 0);
    if (flushes & TU_CMD_FLAG_WAIT_MEM_WRITES)
       tu_cs_emit_pkt7(cs, CP_WAIT_MEM_WRITES, 0);
    if (flushes & TU_CMD_FLAG_WAIT_FOR_IDLE)
@@ -2093,7 +2097,7 @@ tu_reset_cmd_buffer(struct vk_command_buffer *vk_cmd_buffer,
       memset(&cmd_buffer->descriptors[i].push_set, 0, sizeof(cmd_buffer->descriptors[i].push_set));
       cmd_buffer->descriptors[i].push_set.base.type = VK_OBJECT_TYPE_DESCRIPTOR_SET;
       cmd_buffer->descriptors[i].max_sets_bound = 0;
-      cmd_buffer->descriptors[i].dynamic_bound = 0;
+      cmd_buffer->descriptors[i].max_dynamic_offset_size = 0;
    }
 
    u_trace_fini(&cmd_buffer->trace);
@@ -2385,12 +2389,12 @@ tu6_emit_descriptor_sets(struct tu_cmd_buffer *cmd,
          cmd->state.desc_sets =
             tu_cs_draw_state(&cmd->sub_cs, &state_cs,
                              4 + 4 * descriptors_state->max_sets_bound +
-                             (descriptors_state->dynamic_bound ? 6 : 0));
+                             (descriptors_state->max_dynamic_offset_size ? 6 : 0));
       } else {
          cmd->state.desc_sets =
             tu_cs_draw_state(&cmd->sub_cs, &state_cs,
                              3 + 2 * descriptors_state->max_sets_bound +
-                             (descriptors_state->dynamic_bound ? 3 : 0));
+                             (descriptors_state->max_dynamic_offset_size ? 3 : 0));
       }
       cs = &state_cs;
    } else {
@@ -2410,7 +2414,7 @@ tu6_emit_descriptor_sets(struct tu_cmd_buffer *cmd,
    }
 
    /* Dynamic descriptors get the reserved descriptor set. */
-   if (descriptors_state->dynamic_bound) {
+   if (descriptors_state->max_dynamic_offset_size) {
       int reserved_set_idx = cmd->device->physical_device->reserved_set_idx;
       assert(reserved_set_idx >= 0); /* reserved set must be bound */
 
@@ -2561,22 +2565,26 @@ tu_CmdBindDescriptorSets(VkCommandBuffer commandBuffer,
    assert(dyn_idx == dynamicOffsetCount);
 
    if (dynamic_offset_offset) {
+      descriptors_state->max_dynamic_offset_size =
+         MAX2(descriptors_state->max_dynamic_offset_size, dynamic_offset_offset);
+
       /* allocate and fill out dynamic descriptor set */
       struct tu_cs_memory dynamic_desc_set;
       int reserved_set_idx = cmd->device->physical_device->reserved_set_idx;
-      VkResult result = tu_cs_alloc(&cmd->sub_cs,
-                                    dynamic_offset_offset / (4 * A6XX_TEX_CONST_DWORDS),
-                                    A6XX_TEX_CONST_DWORDS, &dynamic_desc_set);
+      VkResult result =
+         tu_cs_alloc(&cmd->sub_cs,
+                     descriptors_state->max_dynamic_offset_size /
+                     (4 * A6XX_TEX_CONST_DWORDS),
+                     A6XX_TEX_CONST_DWORDS, &dynamic_desc_set);
       if (result != VK_SUCCESS) {
          vk_command_buffer_set_error(&cmd->vk, result);
          return;
       }
 
       memcpy(dynamic_desc_set.map, descriptors_state->dynamic_descriptors,
-             dynamic_offset_offset);
+             descriptors_state->max_dynamic_offset_size);
       assert(reserved_set_idx >= 0); /* reserved set must be bound */
       descriptors_state->set_iova[reserved_set_idx] = dynamic_desc_set.iova | BINDLESS_DESCRIPTOR_64B;
-      descriptors_state->dynamic_bound = true;
    }
 
    tu_dirty_desc_sets(cmd, pipelineBindPoint);
@@ -3054,6 +3062,17 @@ tu_CmdBindPipeline(VkCommandBuffer commandBuffer,
    tu_bind_gs(cmd, pipeline->shaders[MESA_SHADER_GEOMETRY]);
    tu_bind_fs(cmd, pipeline->shaders[MESA_SHADER_FRAGMENT]);
 
+   /* We precompile static state and count it as dynamic, so we have to
+    * manually clear bitset that tells which dynamic state is set, in order to
+    * make sure that future dynamic state will be emitted. The issue is that
+    * framework remembers only a past REAL dynamic state and compares a new
+    * dynamic state against it, and not against our static state masquaraded
+    * as dynamic.
+    */
+   BITSET_ANDNOT(cmd->vk.dynamic_graphics_state.set,
+                 cmd->vk.dynamic_graphics_state.set,
+                 pipeline->static_state_mask);
+
    vk_cmd_set_dynamic_graphics_state(&cmd->vk,
                                      &gfx_pipeline->dynamic_state);
    cmd->state.program = pipeline->program;
@@ -3231,6 +3250,13 @@ tu_flush_for_access(struct tu_cache_state *cache,
       flush_bits |= TU_CMD_FLAG_BINDLESS_DESCRIPTOR_INVALIDATE;
    }
 
+   /* There are multiple incoherent copies of CCHE, so any read through it may
+    * require invalidating it and we cannot optimize away invalidates.
+    */
+   if (dst_mask & TU_ACCESS_CCHE_READ) {
+      flush_bits |= TU_CMD_FLAG_CCHE_INVALIDATE;
+   }
+
 #undef DST_INCOHERENT_FLUSH
 
    cache->flush_bits |= flush_bits;
@@ -3332,12 +3358,13 @@ vk2tu_access(VkAccessFlags2 flags, VkPipelineStageFlags2 stages, bool image_only
                        VK_PIPELINE_STAGE_2_VERTEX_INPUT_BIT |
                        VK_PIPELINE_STAGE_2_VERTEX_ATTRIBUTE_INPUT_BIT |
                        SHADER_STAGES))
-       mask |= TU_ACCESS_UCHE_READ;
+       mask |= TU_ACCESS_UCHE_READ | TU_ACCESS_CCHE_READ;
 
    if (gfx_read_access(flags, stages,
                        VK_ACCESS_2_DESCRIPTOR_BUFFER_READ_BIT_EXT,
                        SHADER_STAGES)) {
-      mask |= TU_ACCESS_UCHE_READ | TU_ACCESS_BINDLESS_DESCRIPTOR_READ;
+      mask |= TU_ACCESS_UCHE_READ | TU_ACCESS_BINDLESS_DESCRIPTOR_READ |
+              TU_ACCESS_CCHE_READ;
    }
 
    if (gfx_write_access(flags, stages,
diff --git a/src/freedreno/vulkan/tu_cmd_buffer.h b/src/freedreno/vulkan/tu_cmd_buffer.h
index 7538ad3a71c..07325b66fa0 100644
--- a/src/freedreno/vulkan/tu_cmd_buffer.h
+++ b/src/freedreno/vulkan/tu_cmd_buffer.h
@@ -54,7 +54,7 @@ struct tu_descriptor_state
    uint32_t dynamic_descriptors[MAX_DYNAMIC_BUFFERS_SIZE];
    uint64_t set_iova[MAX_SETS];
    uint32_t max_sets_bound;
-   bool dynamic_bound;
+   uint32_t max_dynamic_offset_size;
 };
 
 enum tu_cmd_dirty_bits
@@ -132,6 +132,13 @@ enum tu_cmd_access_mask {
     */
    TU_ACCESS_BINDLESS_DESCRIPTOR_READ = 1 << 13,
 
+   /* The CCHE is a write-through cache which sits behind UCHE, with multiple
+    * incoherent copies. Because it's write-through we only have to worry
+    * about invalidating it for reads. It's invalidated by "ccinv" in the
+    * shader and CP_CCHE_INVALIDATE in the command stream.
+    */
+   TU_ACCESS_CCHE_READ = 1 << 16,
+
    TU_ACCESS_READ =
       TU_ACCESS_UCHE_READ |
       TU_ACCESS_CCU_COLOR_READ |
@@ -139,7 +146,8 @@ enum tu_cmd_access_mask {
       TU_ACCESS_CCU_COLOR_INCOHERENT_READ |
       TU_ACCESS_CCU_DEPTH_INCOHERENT_READ |
       TU_ACCESS_SYSMEM_READ |
-      TU_ACCESS_BINDLESS_DESCRIPTOR_READ,
+      TU_ACCESS_BINDLESS_DESCRIPTOR_READ |
+      TU_ACCESS_CCHE_READ,
 
    TU_ACCESS_WRITE =
       TU_ACCESS_UCHE_WRITE |
@@ -186,10 +194,11 @@ enum tu_cmd_flush_bits {
    TU_CMD_FLAG_CCU_INVALIDATE_COLOR = 1 << 3,
    TU_CMD_FLAG_CACHE_FLUSH = 1 << 4,
    TU_CMD_FLAG_CACHE_INVALIDATE = 1 << 5,
-   TU_CMD_FLAG_WAIT_MEM_WRITES = 1 << 6,
-   TU_CMD_FLAG_WAIT_FOR_IDLE = 1 << 7,
-   TU_CMD_FLAG_WAIT_FOR_ME = 1 << 8,
-   TU_CMD_FLAG_BINDLESS_DESCRIPTOR_INVALIDATE = 1 << 9,
+   TU_CMD_FLAG_CCHE_INVALIDATE = 1 << 6,
+   TU_CMD_FLAG_WAIT_MEM_WRITES = 1 << 7,
+   TU_CMD_FLAG_WAIT_FOR_IDLE = 1 << 8,
+   TU_CMD_FLAG_WAIT_FOR_ME = 1 << 9,
+   TU_CMD_FLAG_BINDLESS_DESCRIPTOR_INVALIDATE = 1 << 10,
 
    TU_CMD_FLAG_ALL_FLUSH =
       TU_CMD_FLAG_CCU_FLUSH_DEPTH |
@@ -205,6 +214,7 @@ enum tu_cmd_flush_bits {
       TU_CMD_FLAG_CCU_INVALIDATE_COLOR |
       TU_CMD_FLAG_CACHE_INVALIDATE |
       TU_CMD_FLAG_BINDLESS_DESCRIPTOR_INVALIDATE |
+      TU_CMD_FLAG_CCHE_INVALIDATE |
       /* Treat CP_WAIT_FOR_ME as a "cache" that needs to be invalidated when a
        * a command that needs CP_WAIT_FOR_ME is executed. This means we may
        * insert an extra WAIT_FOR_ME before an indirect command requiring it
diff --git a/src/freedreno/vulkan/tu_knl.h b/src/freedreno/vulkan/tu_knl.h
index e9293e3d08b..f10ba6fdd09 100644
--- a/src/freedreno/vulkan/tu_knl.h
+++ b/src/freedreno/vulkan/tu_knl.h
@@ -15,12 +15,12 @@
 struct tu_u_trace_syncobj;
 struct vdrm_bo;
 
-enum tu_bo_alloc_flags
-{
+enum tu_bo_alloc_flags {
    TU_BO_ALLOC_NO_FLAGS = 0,
    TU_BO_ALLOC_ALLOW_DUMP = 1 << 0,
    TU_BO_ALLOC_GPU_READ_ONLY = 1 << 1,
    TU_BO_ALLOC_REPLAYABLE = 1 << 2,
+   TU_BO_ALLOC_DMABUF = 1 << 4,
 };
 
 /* Define tu_timeline_sync type based on drm syncobj for a point type
diff --git a/src/freedreno/vulkan/tu_knl_drm_msm.cc b/src/freedreno/vulkan/tu_knl_drm_msm.cc
index 581a65e0e5b..7f48b3cb48e 100644
--- a/src/freedreno/vulkan/tu_knl_drm_msm.cc
+++ b/src/freedreno/vulkan/tu_knl_drm_msm.cc
@@ -321,44 +321,68 @@ tu_free_zombie_vma_locked(struct tu_device *dev, bool wait)
          last_signaled_fence = vma->fence;
       }
 
-      /* Ensure that internal kernel's vma is freed. */
-      struct drm_msm_gem_info req = {
-         .handle = vma->gem_handle,
-         .info = MSM_INFO_SET_IOVA,
-         .value = 0,
-      };
+      if (vma->gem_handle) {
+         /* Ensure that internal kernel's vma is freed. */
+         struct drm_msm_gem_info req = {
+            .handle = vma->gem_handle,
+            .info = MSM_INFO_SET_IOVA,
+            .value = 0,
+         };
+
+         int ret =
+            drmCommandWriteRead(dev->fd, DRM_MSM_GEM_INFO, &req, sizeof(req));
+         if (ret < 0) {
+            mesa_loge("MSM_INFO_SET_IOVA(0) failed! %d (%s)", ret,
+                      strerror(errno));
+            return VK_ERROR_UNKNOWN;
+         }
 
-      int ret =
-         drmCommandWriteRead(dev->fd, DRM_MSM_GEM_INFO, &req, sizeof(req));
-      if (ret < 0) {
-         mesa_loge("MSM_INFO_SET_IOVA(0) failed! %d (%s)", ret,
-                   strerror(errno));
-         return VK_ERROR_UNKNOWN;
-      }
+         tu_gem_close(dev, vma->gem_handle);
 
-      tu_gem_close(dev, vma->gem_handle);
+         util_vma_heap_free(&dev->vma, vma->iova, vma->size);
+      }
 
-      util_vma_heap_free(&dev->vma, vma->iova, vma->size);
       u_vector_remove(&dev->zombie_vmas);
    }
 
    return VK_SUCCESS;
 }
 
+static bool
+tu_restore_from_zombie_vma_locked(struct tu_device *dev,
+                                  uint32_t gem_handle,
+                                  uint64_t *iova)
+{
+   struct tu_zombie_vma *vma;
+   u_vector_foreach (vma, &dev->zombie_vmas) {
+      if (vma->gem_handle == gem_handle) {
+         *iova = vma->iova;
+
+         /* mark to skip later gem and iova cleanup */
+         vma->gem_handle = 0;
+         return true;
+      }
+   }
+
+   return false;
+}
+
 static VkResult
-msm_allocate_userspace_iova(struct tu_device *dev,
-                            uint32_t gem_handle,
-                            uint64_t size,
-                            uint64_t client_iova,
-                            enum tu_bo_alloc_flags flags,
-                            uint64_t *iova)
+msm_allocate_userspace_iova_locked(struct tu_device *dev,
+                                   uint32_t gem_handle,
+                                   uint64_t size,
+                                   uint64_t client_iova,
+                                   enum tu_bo_alloc_flags flags,
+                                   uint64_t *iova)
 {
    VkResult result;
 
-   mtx_lock(&dev->vma_mutex);
-
    *iova = 0;
 
+   if ((flags & TU_BO_ALLOC_DMABUF) &&
+       tu_restore_from_zombie_vma_locked(dev, gem_handle, iova))
+      return VK_SUCCESS;
+
    tu_free_zombie_vma_locked(dev, false);
 
    result = tu_allocate_userspace_iova(dev, size, client_iova, flags, iova);
@@ -372,8 +396,6 @@ msm_allocate_userspace_iova(struct tu_device *dev,
       result = tu_allocate_userspace_iova(dev, size, client_iova, flags, iova);
    }
 
-   mtx_unlock(&dev->vma_mutex);
-
    if (result != VK_SUCCESS)
       return result;
 
@@ -386,6 +408,7 @@ msm_allocate_userspace_iova(struct tu_device *dev,
    int ret =
       drmCommandWriteRead(dev->fd, DRM_MSM_GEM_INFO, &req, sizeof(req));
    if (ret < 0) {
+      util_vma_heap_free(&dev->vma, *iova, size);
       mesa_loge("MSM_INFO_SET_IOVA failed! %d (%s)", ret, strerror(errno));
       return VK_ERROR_OUT_OF_HOST_MEMORY;
    }
@@ -420,8 +443,8 @@ tu_bo_init(struct tu_device *dev,
    assert(!client_iova || dev->physical_device->has_set_iova);
 
    if (dev->physical_device->has_set_iova) {
-      result = msm_allocate_userspace_iova(dev, gem_handle, size, client_iova,
-                                           flags, &iova);
+      result = msm_allocate_userspace_iova_locked(dev, gem_handle, size,
+                                                  client_iova, flags, &iova);
    } else {
       result = tu_allocate_kernel_iova(dev, gem_handle, &iova);
    }
@@ -445,6 +468,8 @@ tu_bo_init(struct tu_device *dev,
       if (!new_ptr) {
          dev->bo_count--;
          mtx_unlock(&dev->bo_mutex);
+         if (dev->physical_device->has_set_iova)
+            util_vma_heap_free(&dev->vma, iova, size);
          tu_gem_close(dev, gem_handle);
          return VK_ERROR_OUT_OF_HOST_MEMORY;
       }
@@ -506,6 +531,20 @@ tu_bo_set_kernel_name(struct tu_device *dev, struct tu_bo *bo, const char *name)
    }
 }
 
+static inline void
+msm_vma_lock(struct tu_device *dev)
+{
+   if (dev->physical_device->has_set_iova)
+      mtx_lock(&dev->vma_mutex);
+}
+
+static inline void
+msm_vma_unlock(struct tu_device *dev)
+{
+   if (dev->physical_device->has_set_iova)
+      mtx_unlock(&dev->vma_mutex);
+}
+
 static VkResult
 msm_bo_init(struct tu_device *dev,
             struct tu_bo **out_bo,
@@ -541,9 +580,15 @@ msm_bo_init(struct tu_device *dev,
    struct tu_bo* bo = tu_device_lookup_bo(dev, req.handle);
    assert(bo && bo->gem_handle == 0);
 
+   assert(!(flags & TU_BO_ALLOC_DMABUF));
+
+   msm_vma_lock(dev);
+
    VkResult result =
       tu_bo_init(dev, bo, req.handle, size, client_iova, flags, name);
 
+   msm_vma_unlock(dev);
+
    if (result != VK_SUCCESS)
       memset(bo, 0, sizeof(*bo));
    else
@@ -591,11 +636,13 @@ msm_bo_init_dmabuf(struct tu_device *dev,
     * to happen in parallel.
     */
    u_rwlock_wrlock(&dev->dma_bo_lock);
+   msm_vma_lock(dev);
 
    uint32_t gem_handle;
    int ret = drmPrimeFDToHandle(dev->fd, prime_fd,
                                 &gem_handle);
    if (ret) {
+      msm_vma_unlock(dev);
       u_rwlock_wrunlock(&dev->dma_bo_lock);
       return vk_error(dev, VK_ERROR_INVALID_EXTERNAL_HANDLE);
    }
@@ -604,6 +651,7 @@ msm_bo_init_dmabuf(struct tu_device *dev,
 
    if (bo->refcnt != 0) {
       p_atomic_inc(&bo->refcnt);
+      msm_vma_unlock(dev);
       u_rwlock_wrunlock(&dev->dma_bo_lock);
 
       *out_bo = bo;
@@ -611,13 +659,14 @@ msm_bo_init_dmabuf(struct tu_device *dev,
    }
 
    VkResult result =
-      tu_bo_init(dev, bo, gem_handle, size, 0, TU_BO_ALLOC_NO_FLAGS, "dmabuf");
+      tu_bo_init(dev, bo, gem_handle, size, 0, TU_BO_ALLOC_DMABUF, "dmabuf");
 
    if (result != VK_SUCCESS)
       memset(bo, 0, sizeof(*bo));
    else
       *out_bo = bo;
 
+   msm_vma_unlock(dev);
    u_rwlock_wrunlock(&dev->dma_bo_lock);
 
    return result;
diff --git a/src/freedreno/vulkan/tu_knl_drm_virtio.cc b/src/freedreno/vulkan/tu_knl_drm_virtio.cc
index 999a4af2fd4..e69c370dbab 100644
--- a/src/freedreno/vulkan/tu_knl_drm_virtio.cc
+++ b/src/freedreno/vulkan/tu_knl_drm_virtio.cc
@@ -412,14 +412,16 @@ tu_free_zombie_vma_locked(struct tu_device *dev, bool wait)
          last_signaled_fence = vma->fence;
       }
 
-      set_iova(dev, vma->res_id, 0);
-
       u_vector_remove(&dev->zombie_vmas);
 
-      struct tu_zombie_vma *vma2 = (struct tu_zombie_vma *)
-            u_vector_add(&vdev->zombie_vmas_stage_2);
+      if (vma->gem_handle) {
+         set_iova(dev, vma->res_id, 0);
+
+         struct tu_zombie_vma *vma2 =
+            (struct tu_zombie_vma *) u_vector_add(&vdev->zombie_vmas_stage_2);
 
-      *vma2 = *vma;
+         *vma2 = *vma;
+      }
    }
 
    /* And _then_ close the GEM handles: */
@@ -434,19 +436,44 @@ tu_free_zombie_vma_locked(struct tu_device *dev, bool wait)
    return VK_SUCCESS;
 }
 
+static bool
+tu_restore_from_zombie_vma_locked(struct tu_device *dev,
+                                  uint32_t gem_handle,
+                                  uint64_t *iova)
+{
+   struct tu_zombie_vma *vma;
+   u_vector_foreach (vma, &dev->zombie_vmas) {
+      if (vma->gem_handle == gem_handle) {
+         *iova = vma->iova;
+
+         /* mark to skip later vdrm bo and iova cleanup */
+         vma->gem_handle = 0;
+         return true;
+      }
+   }
+
+   return false;
+}
+
 static VkResult
-virtio_allocate_userspace_iova(struct tu_device *dev,
-                               uint64_t size,
-                               uint64_t client_iova,
-                               enum tu_bo_alloc_flags flags,
-                               uint64_t *iova)
+virtio_allocate_userspace_iova_locked(struct tu_device *dev,
+                                      uint32_t gem_handle,
+                                      uint64_t size,
+                                      uint64_t client_iova,
+                                      enum tu_bo_alloc_flags flags,
+                                      uint64_t *iova)
 {
    VkResult result;
 
-   mtx_lock(&dev->vma_mutex);
-
    *iova = 0;
 
+   if (flags & TU_BO_ALLOC_DMABUF) {
+      assert(gem_handle);
+
+      if (tu_restore_from_zombie_vma_locked(dev, gem_handle, iova))
+         return VK_SUCCESS;
+   }
+
    tu_free_zombie_vma_locked(dev, false);
 
    result = tu_allocate_userspace_iova(dev, size, client_iova, flags, iova);
@@ -460,8 +487,6 @@ virtio_allocate_userspace_iova(struct tu_device *dev,
       result = tu_allocate_userspace_iova(dev, size, client_iova, flags, iova);
    }
 
-   mtx_unlock(&dev->vma_mutex);
-
    return result;
 }
 
@@ -571,12 +596,8 @@ virtio_bo_init(struct tu_device *dev,
          .size = size,
    };
    VkResult result;
-
-   result = virtio_allocate_userspace_iova(dev, size, client_iova,
-                                           flags, &req.iova);
-   if (result != VK_SUCCESS) {
-      return result;
-   }
+   uint32_t res_id;
+   struct tu_bo *bo;
 
    if (mem_property & VK_MEMORY_PROPERTY_HOST_CACHED_BIT) {
       if (mem_property & VK_MEMORY_PROPERTY_HOST_COHERENT_BIT) {
@@ -601,6 +622,16 @@ virtio_bo_init(struct tu_device *dev,
    if (flags & TU_BO_ALLOC_GPU_READ_ONLY)
       req.flags |= MSM_BO_GPU_READONLY;
 
+   assert(!(flags & TU_BO_ALLOC_DMABUF));
+
+   mtx_lock(&dev->vma_mutex);
+   result = virtio_allocate_userspace_iova_locked(dev, 0, size, client_iova,
+                                                  flags, &req.iova);
+   mtx_unlock(&dev->vma_mutex);
+
+   if (result != VK_SUCCESS)
+      return result;
+
    /* tunneled cmds are processed separately on host side,
     * before the renderer->get_blob() callback.. the blob_id
     * is used to link the created bo to the get_blob() call
@@ -611,27 +642,28 @@ virtio_bo_init(struct tu_device *dev,
       vdrm_bo_create(vdev->vdrm, size, blob_flags, req.blob_id, &req.hdr);
 
    if (!handle) {
-      util_vma_heap_free(&dev->vma, req.iova, size);
-      return vk_error(dev, VK_ERROR_OUT_OF_DEVICE_MEMORY);
+      result = VK_ERROR_OUT_OF_DEVICE_MEMORY;
+      goto fail;
    }
 
-   uint32_t res_id = vdrm_handle_to_res_id(vdev->vdrm, handle);
-   struct tu_bo* bo = tu_device_lookup_bo(dev, res_id);
+   res_id = vdrm_handle_to_res_id(vdev->vdrm, handle);
+   bo = tu_device_lookup_bo(dev, res_id);
    assert(bo && bo->gem_handle == 0);
 
    bo->res_id = res_id;
 
    result = tu_bo_init(dev, bo, handle, size, req.iova, flags, name);
-   if (result != VK_SUCCESS)
+   if (result != VK_SUCCESS) {
       memset(bo, 0, sizeof(*bo));
-   else
-      *out_bo = bo;
+      goto fail;
+   }
+
+   *out_bo = bo;
 
    /* We don't use bo->name here because for the !TU_DEBUG=bo case bo->name is NULL. */
    tu_bo_set_kernel_name(dev, bo, name);
 
-   if (result == VK_SUCCESS &&
-       (mem_property & VK_MEMORY_PROPERTY_HOST_CACHED_BIT) &&
+   if ((mem_property & VK_MEMORY_PROPERTY_HOST_CACHED_BIT) &&
        !(mem_property & VK_MEMORY_PROPERTY_HOST_COHERENT_BIT)) {
       tu_bo_map(dev, bo);
 
@@ -644,6 +676,12 @@ virtio_bo_init(struct tu_device *dev,
       tu_sync_cache_bo(dev, bo, 0, VK_WHOLE_SIZE, TU_MEM_SYNC_CACHE_TO_GPU);
    }
 
+   return VK_SUCCESS;
+
+fail:
+   mtx_lock(&dev->vma_mutex);
+   util_vma_heap_free(&dev->vma, req.iova, size);
+   mtx_unlock(&dev->vma_mutex);
    return result;
 }
 
@@ -666,11 +704,6 @@ virtio_bo_init_dmabuf(struct tu_device *dev,
    /* iova allocation needs to consider the object's *real* size: */
    size = real_size;
 
-   uint64_t iova;
-   result = virtio_allocate_userspace_iova(dev, size, 0, TU_BO_ALLOC_NO_FLAGS, &iova);
-   if (result != VK_SUCCESS)
-      return result;
-
    /* Importing the same dmabuf several times would yield the same
     * gem_handle. Thus there could be a race when destroying
     * BO and importing the same dmabuf from different threads.
@@ -678,8 +711,10 @@ virtio_bo_init_dmabuf(struct tu_device *dev,
     * to happen in parallel.
     */
    u_rwlock_wrlock(&dev->dma_bo_lock);
+   mtx_lock(&dev->vma_mutex);
 
    uint32_t handle, res_id;
+   uint64_t iova;
 
    handle = vdrm_dmabuf_to_handle(vdrm, prime_fd);
    if (!handle) {
@@ -689,6 +724,7 @@ virtio_bo_init_dmabuf(struct tu_device *dev,
 
    res_id = vdrm_handle_to_res_id(vdrm, handle);
    if (!res_id) {
+      /* XXX gem_handle potentially leaked here since no refcnt */
       result = vk_error(dev, VK_ERROR_INVALID_EXTERNAL_HANDLE);
       goto out_unlock;
    }
@@ -702,21 +738,25 @@ virtio_bo_init_dmabuf(struct tu_device *dev,
       goto out_unlock;
    }
 
-   result = tu_bo_init(dev, bo, handle, size, iova,
-                       TU_BO_ALLOC_NO_FLAGS, "dmabuf");
-   if (result != VK_SUCCESS)
-      memset(bo, 0, sizeof(*bo));
-   else
-      *out_bo = bo;
+   result = virtio_allocate_userspace_iova_locked(dev, handle, size, 0,
+                                                  TU_BO_ALLOC_DMABUF, &iova);
+   if (result != VK_SUCCESS) {
+      vdrm_bo_close(dev->vdev->vdrm, handle);
+      goto out_unlock;
+   }
 
-out_unlock:
-   u_rwlock_wrunlock(&dev->dma_bo_lock);
+   result =
+      tu_bo_init(dev, bo, handle, size, iova, TU_BO_ALLOC_NO_FLAGS, "dmabuf");
    if (result != VK_SUCCESS) {
-      mtx_lock(&dev->vma_mutex);
       util_vma_heap_free(&dev->vma, iova, size);
-      mtx_unlock(&dev->vma_mutex);
+      memset(bo, 0, sizeof(*bo));
+   } else {
+      *out_bo = bo;
    }
 
+out_unlock:
+   mtx_unlock(&dev->vma_mutex);
+   u_rwlock_wrunlock(&dev->dma_bo_lock);
    return result;
 }
 
diff --git a/src/freedreno/vulkan/tu_pipeline.cc b/src/freedreno/vulkan/tu_pipeline.cc
index e0a119af4ac..a775f83ca03 100644
--- a/src/freedreno/vulkan/tu_pipeline.cc
+++ b/src/freedreno/vulkan/tu_pipeline.cc
@@ -1963,6 +1963,9 @@ tu_pipeline_builder_parse_libraries(struct tu_pipeline_builder *builder,
          }
       }
 
+      BITSET_OR(pipeline->static_state_mask, pipeline->static_state_mask,
+                library->base.static_state_mask);
+
       vk_graphics_pipeline_state_merge(&builder->graphics_state,
                                        &library->graphics_state);
    }
@@ -3276,6 +3279,9 @@ tu_pipeline_builder_emit_state(struct tu_pipeline_builder *builder,
     * binding the pipeline by making it "dynamic".
     */
    BITSET_ANDNOT(remove, remove, keep);
+
+   BITSET_OR(pipeline->static_state_mask, pipeline->static_state_mask, remove);
+
    BITSET_OR(builder->graphics_state.dynamic, builder->graphics_state.dynamic,
              remove);
 }
diff --git a/src/freedreno/vulkan/tu_pipeline.h b/src/freedreno/vulkan/tu_pipeline.h
index a99675ccd4c..2c7b7f5b887 100644
--- a/src/freedreno/vulkan/tu_pipeline.h
+++ b/src/freedreno/vulkan/tu_pipeline.h
@@ -138,6 +138,8 @@ struct tu_pipeline
    uint32_t set_state_mask;
    struct tu_draw_state dynamic_state[TU_DYNAMIC_STATE_COUNT];
 
+   BITSET_DECLARE(static_state_mask, MESA_VK_DYNAMIC_GRAPHICS_STATE_ENUM_MAX);
+
    struct {
       bool raster_order_attachment_access;
    } ds;
diff --git a/src/freedreno/vulkan/tu_shader.cc b/src/freedreno/vulkan/tu_shader.cc
index 2740e7aa9ea..1c2d1451810 100644
--- a/src/freedreno/vulkan/tu_shader.cc
+++ b/src/freedreno/vulkan/tu_shader.cc
@@ -2104,20 +2104,21 @@ tu_shader_deserialize(struct vk_pipeline_cache *cache,
                       struct blob_reader *blob);
 
 static void
-tu_shader_destroy(struct vk_device *device,
-                  struct vk_pipeline_cache_object *object)
+tu_shader_pipeline_cache_object_destroy(struct vk_device *vk_device,
+                                        struct vk_pipeline_cache_object *object)
 {
+   struct tu_device *device = container_of(vk_device, struct tu_device, vk);
    struct tu_shader *shader =
       container_of(object, struct tu_shader, base);
 
    vk_pipeline_cache_object_finish(&shader->base);
-   vk_free(&device->alloc, shader);
+   tu_shader_destroy(device, shader);
 }
 
 const struct vk_pipeline_cache_object_ops tu_shader_ops = {
    .serialize = tu_shader_serialize,
    .deserialize = tu_shader_deserialize,
-   .destroy = tu_shader_destroy,
+   .destroy = tu_shader_pipeline_cache_object_destroy,
 };
 
 static struct tu_shader *
@@ -2376,6 +2377,8 @@ tu_shader_create(struct tu_device *dev,
                                    executable_info);
    }
 
+   ir3_shader_destroy(ir3_shader);
+
    shader->view_mask = key->multiview_mask;
 
    switch (shader->variant->type) {
@@ -2778,6 +2781,7 @@ tu_empty_fs_create(struct tu_device *dev, struct tu_shader **shader,
    struct ir3_shader *ir3_shader =
       ir3_shader_from_nir(dev->compiler, fs_b.shader, &options, &so_info);
    (*shader)->variant = ir3_shader_create_variant(ir3_shader, &key, false);
+   ir3_shader_destroy(ir3_shader);
 
    return tu_upload_shader(dev, *shader);
 }
@@ -2846,5 +2850,10 @@ tu_shader_destroy(struct tu_device *dev,
    if (shader->pvtmem_bo)
       tu_bo_finish(dev, shader->pvtmem_bo);
 
+   if (shader->variant)
+      ralloc_free((void *)shader->variant);
+   if (shader->safe_const_variant)
+      ralloc_free((void *)shader->safe_const_variant);
+
    vk_free(&dev->vk.alloc, shader);
 }
diff --git a/src/gallium/auxiliary/gallivm/lp_bld_init.c b/src/gallium/auxiliary/gallivm/lp_bld_init.c
index cd2108f3a08..1345d85b224 100644
--- a/src/gallium/auxiliary/gallivm/lp_bld_init.c
+++ b/src/gallium/auxiliary/gallivm/lp_bld_init.c
@@ -609,7 +609,11 @@ gallivm_compile_module(struct gallivm_state *gallivm)
    LLVMRunPasses(gallivm->module, passes, LLVMGetExecutionEngineTargetMachine(gallivm->engine), opts);
 
    if (!(gallivm_perf & GALLIVM_PERF_NO_OPT))
+#if LLVM_VERSION_MAJOR >= 18
+      strcpy(passes, "sroa,early-cse,simplifycfg,reassociate,mem2reg,instsimplify,instcombine<no-verify-fixpoint>");
+#else
       strcpy(passes, "sroa,early-cse,simplifycfg,reassociate,mem2reg,instsimplify,instcombine");
+#endif
    else
       strcpy(passes, "mem2reg");
 
diff --git a/src/gallium/auxiliary/gallivm/lp_bld_ir_common.c b/src/gallium/auxiliary/gallivm/lp_bld_ir_common.c
index 17212874152..f2d6eadeef0 100644
--- a/src/gallium/auxiliary/gallivm/lp_bld_ir_common.c
+++ b/src/gallium/auxiliary/gallivm/lp_bld_ir_common.c
@@ -27,6 +27,7 @@
  **************************************************************************/
 
 #include "util/u_memory.h"
+#include "lp_bld_const.h"
 #include "lp_bld_type.h"
 #include "lp_bld_init.h"
 #include "lp_bld_flow.h"
@@ -271,18 +272,17 @@ void lp_exec_bgnloop(struct lp_exec_mask *mask, bool load)
 }
 
 void lp_exec_endloop(struct gallivm_state *gallivm,
-                     struct lp_exec_mask *mask)
+                     struct lp_exec_mask *exec_mask,
+                     struct lp_build_mask_context *mask)
 {
-   LLVMBuilderRef builder = mask->bld->gallivm->builder;
-   struct function_ctx *ctx = func_ctx(mask);
+   LLVMBuilderRef builder = exec_mask->bld->gallivm->builder;
+   struct function_ctx *ctx = func_ctx(exec_mask);
    LLVMBasicBlockRef endloop;
-   LLVMTypeRef int_type = LLVMInt32TypeInContext(mask->bld->gallivm->context);
-   LLVMTypeRef reg_type = LLVMIntTypeInContext(gallivm->context,
-                                               mask->bld->type.width *
-                                               mask->bld->type.length);
+   LLVMTypeRef int_type = LLVMInt32TypeInContext(exec_mask->bld->gallivm->context);
+   LLVMTypeRef mask_type = LLVMIntTypeInContext(exec_mask->bld->gallivm->context, exec_mask->bld->type.length);
    LLVMValueRef i1cond, i2cond, icond, limiter;
 
-   assert(mask->break_mask);
+   assert(exec_mask->break_mask);
 
    assert(ctx->loop_stack_size);
    if (ctx->loop_stack_size > LP_MAX_TGSI_NESTING) {
@@ -294,14 +294,14 @@ void lp_exec_endloop(struct gallivm_state *gallivm,
    /*
     * Restore the cont_mask, but don't pop
     */
-   mask->cont_mask = ctx->loop_stack[ctx->loop_stack_size - 1].cont_mask;
-   lp_exec_mask_update(mask);
+   exec_mask->cont_mask = ctx->loop_stack[ctx->loop_stack_size - 1].cont_mask;
+   lp_exec_mask_update(exec_mask);
 
    /*
     * Unlike the continue mask, the break_mask must be preserved across loop
     * iterations
     */
-   LLVMBuildStore(builder, mask->break_mask, ctx->break_var);
+   LLVMBuildStore(builder, exec_mask->break_mask, ctx->break_var);
 
    /* Decrement the loop limiter */
    limiter = LLVMBuildLoad2(builder, int_type, ctx->loop_limiter, "");
@@ -314,12 +314,18 @@ void lp_exec_endloop(struct gallivm_state *gallivm,
 
    LLVMBuildStore(builder, limiter, ctx->loop_limiter);
 
-   /* i1cond = (mask != 0) */
+   LLVMValueRef end_mask = exec_mask->exec_mask;
+   if (mask)
+      end_mask = LLVMBuildAnd(builder, exec_mask->exec_mask, lp_build_mask_value(mask), "");
+   end_mask = LLVMBuildICmp(builder, LLVMIntNE, end_mask, lp_build_zero(gallivm, exec_mask->bld->type), "");
+   end_mask = LLVMBuildBitCast(builder, end_mask, mask_type, "");
+
+   /* i1cond = (end_mask != 0) */
    i1cond = LLVMBuildICmp(
       builder,
       LLVMIntNE,
-      LLVMBuildBitCast(builder, mask->exec_mask, reg_type, ""),
-      LLVMConstNull(reg_type), "i1cond");
+      end_mask,
+      LLVMConstNull(mask_type), "i1cond");
 
    /* i2cond = (looplimiter > 0) */
    i2cond = LLVMBuildICmp(
@@ -331,7 +337,7 @@ void lp_exec_endloop(struct gallivm_state *gallivm,
    /* if( i1cond && i2cond ) */
    icond = LLVMBuildAnd(builder, i1cond, i2cond, "");
 
-   endloop = lp_build_insert_new_block(mask->bld->gallivm, "endloop");
+   endloop = lp_build_insert_new_block(exec_mask->bld->gallivm, "endloop");
 
    LLVMBuildCondBr(builder,
                    icond, ctx->loop_block, endloop);
@@ -341,14 +347,14 @@ void lp_exec_endloop(struct gallivm_state *gallivm,
    assert(ctx->loop_stack_size);
    --ctx->loop_stack_size;
    --ctx->bgnloop_stack_size;
-   mask->cont_mask = ctx->loop_stack[ctx->loop_stack_size].cont_mask;
-   mask->break_mask = ctx->loop_stack[ctx->loop_stack_size].break_mask;
+   exec_mask->cont_mask = ctx->loop_stack[ctx->loop_stack_size].cont_mask;
+   exec_mask->break_mask = ctx->loop_stack[ctx->loop_stack_size].break_mask;
    ctx->loop_block = ctx->loop_stack[ctx->loop_stack_size].loop_block;
    ctx->break_var = ctx->loop_stack[ctx->loop_stack_size].break_var;
    ctx->break_type = ctx->break_type_stack[ctx->loop_stack_size +
          ctx->switch_stack_size];
 
-   lp_exec_mask_update(mask);
+   lp_exec_mask_update(exec_mask);
 }
 
 void lp_exec_mask_cond_push(struct lp_exec_mask *mask,
diff --git a/src/gallium/auxiliary/gallivm/lp_bld_ir_common.h b/src/gallium/auxiliary/gallivm/lp_bld_ir_common.h
index 0e0a7f74f1b..0def76ebd92 100644
--- a/src/gallium/auxiliary/gallivm/lp_bld_ir_common.h
+++ b/src/gallium/auxiliary/gallivm/lp_bld_ir_common.h
@@ -101,6 +101,8 @@ struct lp_exec_mask {
    int function_stack_size;
 };
 
+struct lp_build_mask_context;
+
 void lp_exec_mask_function_init(struct lp_exec_mask *mask, int function_idx);
 void lp_exec_mask_init(struct lp_exec_mask *mask, struct lp_build_context *bld);
 void lp_exec_mask_fini(struct lp_exec_mask *mask);
@@ -112,7 +114,8 @@ void lp_exec_mask_update(struct lp_exec_mask *mask);
 void lp_exec_bgnloop_post_phi(struct lp_exec_mask *mask);
 void lp_exec_bgnloop(struct lp_exec_mask *mask, bool load_mask);
 void lp_exec_endloop(struct gallivm_state *gallivm,
-                     struct lp_exec_mask *mask);
+                     struct lp_exec_mask *exec_mask,
+                     struct lp_build_mask_context *mask);
 void lp_exec_mask_cond_push(struct lp_exec_mask *mask,
                             LLVMValueRef val);
 void lp_exec_mask_cond_invert(struct lp_exec_mask *mask);
diff --git a/src/gallium/auxiliary/gallivm/lp_bld_nir_soa.c b/src/gallium/auxiliary/gallivm/lp_bld_nir_soa.c
index 9a730ad6c66..fc440e9c71e 100644
--- a/src/gallium/auxiliary/gallivm/lp_bld_nir_soa.c
+++ b/src/gallium/auxiliary/gallivm/lp_bld_nir_soa.c
@@ -2024,7 +2024,7 @@ static void bgnloop(struct lp_build_nir_context *bld_base)
 static void endloop(struct lp_build_nir_context *bld_base)
 {
    struct lp_build_nir_soa_context *bld = (struct lp_build_nir_soa_context *)bld_base;
-   lp_exec_endloop(bld_base->base.gallivm, &bld->exec_mask);
+   lp_exec_endloop(bld_base->base.gallivm, &bld->exec_mask, bld->mask);
 }
 
 static void if_cond(struct lp_build_nir_context *bld_base, LLVMValueRef cond)
diff --git a/src/gallium/auxiliary/gallivm/lp_bld_tgsi_soa.c b/src/gallium/auxiliary/gallivm/lp_bld_tgsi_soa.c
index d4b0f8846c2..6b2a10e1b8a 100644
--- a/src/gallium/auxiliary/gallivm/lp_bld_tgsi_soa.c
+++ b/src/gallium/auxiliary/gallivm/lp_bld_tgsi_soa.c
@@ -4268,7 +4268,7 @@ endloop_emit(
 {
    struct lp_build_tgsi_soa_context * bld = lp_soa_context(bld_base);
 
-   lp_exec_endloop(bld_base->base.gallivm, &bld->exec_mask);
+   lp_exec_endloop(bld_base->base.gallivm, &bld->exec_mask, bld->mask);
 }
 
 static void
diff --git a/src/gallium/auxiliary/meson.build b/src/gallium/auxiliary/meson.build
index 96b0272c69b..17522637bcc 100644
--- a/src/gallium/auxiliary/meson.build
+++ b/src/gallium/auxiliary/meson.build
@@ -549,7 +549,7 @@ if with_tests
   )
 endif
 
-libgalliumvl_stub = static_library(
+_libgalliumvl_stub = static_library(
   'galliumvl_stub',
   'vl/vl_stubs.c',
   c_args : [c_msvc_compat_args],
@@ -571,6 +571,15 @@ libgalliumvl = static_library(
   build_by_default : false,
 )
 
+# some drivers export their screen creation function globally, so all frontends have to contain the
+# full libgalliumvl. So we'll handle this here globally for everybody.
+if (with_gallium_va or with_gallium_vdpau or with_gallium_omx != 'disabled' or
+   with_dri or with_gallium_radeonsi)
+  libgalliumvl_stub = libgalliumvl
+else
+  libgalliumvl_stub = _libgalliumvl_stub
+endif
+
 # XXX: The dependencies here may be off...
 libgalliumvlwinsys = static_library(
   'galliumvlwinsys',
diff --git a/src/gallium/auxiliary/nir/nir_draw_helpers.c b/src/gallium/auxiliary/nir/nir_draw_helpers.c
index 630f37b97e7..e10687cf8ef 100644
--- a/src/gallium/auxiliary/nir/nir_draw_helpers.c
+++ b/src/gallium/auxiliary/nir/nir_draw_helpers.c
@@ -177,6 +177,9 @@ lower_aaline_instr(nir_builder *b, nir_instr *instr, void *data)
       return false;
    if (var->data.location < FRAG_RESULT_DATA0 && var->data.location != FRAG_RESULT_COLOR)
       return false;
+   uint32_t mask = nir_intrinsic_write_mask(intrin) << var->data.location_frac;
+   if (!(mask & BITFIELD_BIT(3)))
+      return false;
 
    nir_def *out_input = intrin->src[1].ssa;
    b->cursor = nir_before_instr(instr);
@@ -223,12 +226,10 @@ lower_aaline_instr(nir_builder *b, nir_instr *instr, void *data)
 
    tmp = nir_fmul(b, nir_channel(b, tmp, 0),
                   nir_fmin(b, nir_channel(b, tmp, 1), max));
-   tmp = nir_fmul(b, nir_channel(b, out_input, 3), tmp);
+   tmp = nir_fmul(b, nir_channel(b, out_input, out_input->num_components - 1), tmp);
 
-   nir_def *out = nir_vec4(b, nir_channel(b, out_input, 0),
-                                 nir_channel(b, out_input, 1),
-                                 nir_channel(b, out_input, 2),
-                                 tmp);
+   nir_def *out = nir_vector_insert_imm(b, out_input, tmp,
+                                        out_input->num_components - 1);
    nir_src_rewrite(&intrin->src[1], out);
    return true;
 }
diff --git a/src/gallium/auxiliary/util/u_blitter.c b/src/gallium/auxiliary/util/u_blitter.c
index f4a0a2f898b..3300a0f776c 100644
--- a/src/gallium/auxiliary/util/u_blitter.c
+++ b/src/gallium/auxiliary/util/u_blitter.c
@@ -2014,6 +2014,7 @@ void util_blitter_blit_generic(struct blitter_context *blitter,
                                unsigned dst_sample)
 {
    struct blitter_context_priv *ctx = (struct blitter_context_priv*)blitter;
+   unsigned count = 0;
    struct pipe_context *pipe = ctx->base.pipe;
    enum pipe_texture_target src_target = src->target;
    unsigned src_samples = src->texture->nr_samples;
@@ -2038,7 +2039,7 @@ void util_blitter_blit_generic(struct blitter_context *blitter,
 
    /* Return if there is nothing to do. */
    if (!dst_has_color && !dst_has_depth && !dst_has_stencil) {
-      return;
+      goto out;
    }
 
    bool is_scaled = dstbox->width != abs(srcbox->width) ||
@@ -2170,7 +2171,6 @@ void util_blitter_blit_generic(struct blitter_context *blitter,
    }
 
    /* Set samplers. */
-   unsigned count = 0;
    if (src_has_depth && src_has_stencil &&
        (dst_has_color || (dst_has_depth && dst_has_stencil))) {
       /* Setup two samplers, one for depth and the other one for stencil. */
@@ -2223,7 +2223,8 @@ void util_blitter_blit_generic(struct blitter_context *blitter,
    do_blits(ctx, dst, dstbox, src, src_width0, src_height0,
             srcbox, dst_has_depth || dst_has_stencil, use_txf, sample0_only,
             dst_sample);
-
+   util_blitter_unset_running_flag(blitter);
+out:
    util_blitter_restore_vertex_states(blitter);
    util_blitter_restore_fragment_states(blitter);
    util_blitter_restore_textures_internal(blitter, count);
@@ -2232,7 +2233,6 @@ void util_blitter_blit_generic(struct blitter_context *blitter,
       pipe->set_scissor_states(pipe, 0, 1, &ctx->base.saved_scissor);
    }
    util_blitter_restore_render_cond(blitter);
-   util_blitter_unset_running_flag(blitter);
 }
 
 void
@@ -2952,33 +2952,36 @@ util_blitter_stencil_fallback(struct blitter_context *blitter,
    struct pipe_stencil_ref sr = { { (1u << stencil_bits) - 1 } };
    pipe->set_stencil_ref(pipe, sr);
 
-   union blitter_attrib coord;
-   get_texcoords(src_view, src->width0, src->height0,
-                 srcbox->x, srcbox->y,
-                 srcbox->x + srcbox->width, srcbox->y + srcbox->height,
-                 srcbox->z, 0, true,
-                 &coord);
-
-   for (int i = 0; i < stencil_bits; ++i) {
-      uint32_t mask = 1 << i;
-      struct pipe_constant_buffer cb = {
-         .user_buffer = &mask,
-         .buffer_size = sizeof(mask),
-      };
-      pipe->set_constant_buffer(pipe, PIPE_SHADER_FRAGMENT, blitter->cb_slot,
-                                false, &cb);
-
-      pipe->bind_depth_stencil_alpha_state(pipe,
-         get_stencil_blit_fallback_dsa(ctx, i));
-
-      blitter->draw_rectangle(blitter, ctx->velem_state,
-                              get_vs_passthrough_pos_generic,
-                              dstbox->x, dstbox->y,
-                              dstbox->x + dstbox->width,
-                              dstbox->y + dstbox->height,
-                              0, 1,
-                              UTIL_BLITTER_ATTRIB_TEXCOORD_XYZW,
-                              &coord);
+   for (unsigned i = 0; i <= util_res_sample_count(dst) - 1; i++) {
+      pipe->set_sample_mask(pipe, 1 << i);
+      union blitter_attrib coord;
+      get_texcoords(src_view, src->width0, src->height0,
+                  srcbox->x, srcbox->y,
+                  srcbox->x + srcbox->width, srcbox->y + srcbox->height,
+                  srcbox->z, i, true,
+                  &coord);
+
+      for (int i = 0; i < stencil_bits; ++i) {
+         uint32_t mask = 1 << i;
+         struct pipe_constant_buffer cb = {
+            .user_buffer = &mask,
+            .buffer_size = sizeof(mask),
+         };
+         pipe->set_constant_buffer(pipe, PIPE_SHADER_FRAGMENT, blitter->cb_slot,
+                                 false, &cb);
+
+         pipe->bind_depth_stencil_alpha_state(pipe,
+            get_stencil_blit_fallback_dsa(ctx, i));
+
+         blitter->draw_rectangle(blitter, ctx->velem_state,
+                                 get_vs_passthrough_pos_generic,
+                                 dstbox->x, dstbox->y,
+                                 dstbox->x + dstbox->width,
+                                 dstbox->y + dstbox->height,
+                                 0, 1,
+                                 UTIL_BLITTER_ATTRIB_TEXCOORD_XYZW,
+                                 &coord);
+      }
    }
 
    if (scissor)
diff --git a/src/gallium/auxiliary/vl/vl_compositor.c b/src/gallium/auxiliary/vl/vl_compositor.c
index 658a87adb14..c618c6dea68 100644
--- a/src/gallium/auxiliary/vl/vl_compositor.c
+++ b/src/gallium/auxiliary/vl/vl_compositor.c
@@ -259,7 +259,7 @@ init_buffers(struct vl_compositor *c)
            vertex_elems[1].vertex_buffer_index = 0;
            vertex_elems[1].src_format = PIPE_FORMAT_R32G32B32A32_FLOAT;
            vertex_elems[2].src_offset = sizeof(struct vertex2f) + sizeof(struct vertex4f);
-           vertex_elems[1].src_stride = VL_COMPOSITOR_VB_STRIDE;
+           vertex_elems[2].src_stride = VL_COMPOSITOR_VB_STRIDE;
            vertex_elems[2].instance_divisor = 0;
            vertex_elems[2].vertex_buffer_index = 0;
            vertex_elems[2].src_format = PIPE_FORMAT_R32G32B32A32_FLOAT;
diff --git a/src/gallium/auxiliary/vl/vl_stubs.c b/src/gallium/auxiliary/vl/vl_stubs.c
index 194e7a8700b..cd6b73b850b 100644
--- a/src/gallium/auxiliary/vl/vl_stubs.c
+++ b/src/gallium/auxiliary/vl/vl_stubs.c
@@ -108,6 +108,16 @@ vl_video_buffer_create_ex2(struct pipe_context *pipe,
    return NULL;
 }
 
+struct pipe_video_buffer *
+vl_video_buffer_create_as_resource(struct pipe_context *pipe,
+                                   const struct pipe_video_buffer *tmpl,
+                                   const uint64_t *modifiers,
+                                   int modifiers_count)
+{
+   assert(0);
+   return NULL;
+}
+
 void
 vl_video_buffer_destroy(struct pipe_video_buffer *buffer)
 {
@@ -145,9 +155,3 @@ vl_create_mpeg12_decoder(struct pipe_context *pipe,
    assert(0);
    return NULL;
 }
-
-/*
- * vl_zscan
- */
-const int vl_zscan_normal[] = {0};
-const int vl_zscan_alternate[] = {0};
diff --git a/src/gallium/auxiliary/vl/vl_video_buffer.c b/src/gallium/auxiliary/vl/vl_video_buffer.c
index 4d95f762510..69cc4563df7 100644
--- a/src/gallium/auxiliary/vl/vl_video_buffer.c
+++ b/src/gallium/auxiliary/vl/vl_video_buffer.c
@@ -118,21 +118,21 @@ vl_video_buffer_is_format_supported(struct pipe_screen *screen,
    vl_get_video_buffer_formats(screen, format, resource_formats);
 
    for (i = 0; i < VL_NUM_COMPONENTS; ++i) {
-      enum pipe_format format = resource_formats[i];
+      enum pipe_format fmt = resource_formats[i];
 
-      if (format == PIPE_FORMAT_NONE)
+      if (fmt == PIPE_FORMAT_NONE)
          continue;
 
       /* we at least need to sample from it */
-      if (!screen->is_format_supported(screen, format, PIPE_TEXTURE_2D, 0, 0, PIPE_BIND_SAMPLER_VIEW))
-         return false;
+      if (!screen->is_format_supported(screen, fmt, PIPE_TEXTURE_2D, 0, 0, PIPE_BIND_SAMPLER_VIEW))
+         continue;
 
-      format = vl_video_buffer_surface_format(format);
-      if (!screen->is_format_supported(screen, format, PIPE_TEXTURE_2D, 0, 0, PIPE_BIND_RENDER_TARGET))
-         return false;
+      fmt = vl_video_buffer_surface_format(fmt);
+      if (screen->is_format_supported(screen, fmt, PIPE_TEXTURE_2D, 0, 0, PIPE_BIND_RENDER_TARGET))
+         return true;
    }
 
-   return true;
+   return false;
 }
 
 unsigned
diff --git a/src/gallium/drivers/crocus/crocus_resource.c b/src/gallium/drivers/crocus/crocus_resource.c
index eed025a8003..7afb8bfe0cb 100644
--- a/src/gallium/drivers/crocus/crocus_resource.c
+++ b/src/gallium/drivers/crocus/crocus_resource.c
@@ -216,6 +216,10 @@ crocus_resource_configure_main(const struct crocus_screen *screen,
          tiling_flags = ISL_TILING_W_BIT;
    }
 
+   /* Disable aux for external memory objects. */
+   if (!res->mod_info && res->external_format != PIPE_FORMAT_NONE)
+      usage |= ISL_SURF_USAGE_DISABLE_AUX_BIT;
+
    const enum isl_format format =
       crocus_format_for_usage(&screen->devinfo, templ->format, usage).fmt;
 
diff --git a/src/gallium/drivers/etnaviv/etnaviv_rs.c b/src/gallium/drivers/etnaviv/etnaviv_rs.c
index 0d07b845172..5e11bd95ee0 100644
--- a/src/gallium/drivers/etnaviv/etnaviv_rs.c
+++ b/src/gallium/drivers/etnaviv/etnaviv_rs.c
@@ -711,12 +711,17 @@ etna_try_rs_blit(struct pipe_context *pctx,
       width = align(width, w_align);
 
    if (height & (h_align - 1) && height >= src_lev->height * src_yscale && height >= dst_lev->height) {
-      if (!ctx->screen->specs.single_buffer &&
-          align(height, h_align * ctx->screen->specs.pixel_pipes) <=
-          dst_lev->padded_height * src_yscale)
-         height = align(height, h_align * ctx->screen->specs.pixel_pipes);
-      else
-         height = align(height, h_align);
+      height = align(height, h_align);
+
+      /* Try to increase alignment to multi-pipe requirements to unlock
+       * multi-pipe resolve for increased performance. */
+      if (!ctx->screen->specs.single_buffer) {
+          unsigned int pipe_align = align(height, h_align * ctx->screen->specs.pixel_pipes);
+
+          if (pipe_align <= src_lev->padded_height &&
+              pipe_align <= dst_lev->padded_height * src_yscale)
+             height = pipe_align;
+      }
    }
 
    /* The padded dimensions are in samples */
diff --git a/src/gallium/drivers/etnaviv/etnaviv_screen.c b/src/gallium/drivers/etnaviv/etnaviv_screen.c
index 801562602fe..f210a98d0ce 100644
--- a/src/gallium/drivers/etnaviv/etnaviv_screen.c
+++ b/src/gallium/drivers/etnaviv/etnaviv_screen.c
@@ -896,7 +896,7 @@ etna_get_specs(struct etna_screen *screen)
       DBG("could not get ETNA_GPU_NUM_VARYINGS");
       goto fail;
    }
-   screen->specs.max_varyings = MAX2(val, ETNA_NUM_VARYINGS);
+   screen->specs.max_varyings = MIN2(val, ETNA_NUM_VARYINGS);
 
    /* Figure out gross GPU architecture. See rnndb/common.xml for a specific
     * description of the differences. */
diff --git a/src/gallium/drivers/etnaviv/etnaviv_util.h b/src/gallium/drivers/etnaviv/etnaviv_util.h
index 4f3061f2d2e..53f1f4ccf27 100644
--- a/src/gallium/drivers/etnaviv/etnaviv_util.h
+++ b/src/gallium/drivers/etnaviv/etnaviv_util.h
@@ -35,14 +35,14 @@
 static inline uint32_t
 etna_float_to_fixp55(float f)
 {
-   return U_FIXED(f, 5);
+   return S_FIXED(f, 5);
 }
 
 /* float to fixp 8.8 */
 static inline uint32_t
 etna_float_to_fixp88(float f)
 {
-   return U_FIXED(f, 8);
+   return S_FIXED(f, 8);
 }
 
 /* texture size to log2 in fixp 5.5 format */
diff --git a/src/gallium/drivers/etnaviv/etnaviv_zsa.c b/src/gallium/drivers/etnaviv/etnaviv_zsa.c
index 38971e72916..7bdbf53f694 100644
--- a/src/gallium/drivers/etnaviv/etnaviv_zsa.c
+++ b/src/gallium/drivers/etnaviv/etnaviv_zsa.c
@@ -48,7 +48,7 @@ etna_zsa_state_create(struct pipe_context *pctx,
    cs->base = *so;
 
    cs->z_test_enabled = so->depth_enabled && so->depth_func != PIPE_FUNC_ALWAYS;
-   cs->z_write_enabled = so->depth_enabled && so->depth_writemask;
+   cs->z_write_enabled = so->depth_writemask;
 
    /* XXX does stencil[0] / stencil[1] order depend on rs->front_ccw? */
 
diff --git a/src/gallium/drivers/freedreno/a6xx/fd6_program.cc b/src/gallium/drivers/freedreno/a6xx/fd6_program.cc
index 4ee1852b757..183bba9b1af 100644
--- a/src/gallium/drivers/freedreno/a6xx/fd6_program.cc
+++ b/src/gallium/drivers/freedreno/a6xx/fd6_program.cc
@@ -119,7 +119,7 @@ fd6_emit_shader(struct fd_context *ctx, struct fd_ringbuffer *ring,
 #endif
 
    gl_shader_stage type = so->type;
-   if (type == MESA_SHADER_COMPUTE)
+   if (type == MESA_SHADER_KERNEL)
       type = MESA_SHADER_COMPUTE;
 
    enum a6xx_threadsize thrsz =
diff --git a/src/gallium/drivers/i915/i915_screen.c b/src/gallium/drivers/i915/i915_screen.c
index a6d6d67a39c..bfa7ea04b9f 100644
--- a/src/gallium/drivers/i915/i915_screen.c
+++ b/src/gallium/drivers/i915/i915_screen.c
@@ -456,7 +456,7 @@ i915_get_param(struct pipe_screen *screen, enum pipe_cap cap)
    case PIPE_CAP_MAX_TEXTURE_3D_LEVELS:
       return I915_MAX_TEXTURE_3D_LEVELS;
    case PIPE_CAP_MAX_TEXTURE_CUBE_LEVELS:
-      return 1 << (I915_MAX_TEXTURE_2D_LEVELS - 1);
+      return I915_MAX_TEXTURE_2D_LEVELS;
 
    /* Render targets. */
    case PIPE_CAP_MAX_RENDER_TARGETS:
diff --git a/src/gallium/drivers/iris/iris_batch.c b/src/gallium/drivers/iris/iris_batch.c
index 1505fe37f79..2d61048282c 100644
--- a/src/gallium/drivers/iris/iris_batch.c
+++ b/src/gallium/drivers/iris/iris_batch.c
@@ -862,8 +862,8 @@ iris_batch_name_to_string(enum iris_batch_name name)
    return names[name];
 }
 
-static inline bool
-context_or_exec_queue_was_banned(struct iris_bufmgr *bufmgr, int ret)
+bool
+iris_batch_is_banned(struct iris_bufmgr *bufmgr, int ret)
 {
    enum intel_kmd_type kmd_type = iris_bufmgr_get_device_info(bufmgr)->kmd_type;
 
@@ -960,7 +960,7 @@ _iris_batch_flush(struct iris_batch *batch, const char *file, int line)
     * has been lost and needs to be re-initialized.  If this succeeds,
     * dubiously claim success...
     */
-   if (ret && context_or_exec_queue_was_banned(bufmgr, ret)) {
+   if (ret && iris_batch_is_banned(bufmgr, ret)) {
       enum pipe_reset_status status = iris_batch_check_for_reset(batch);
 
       if (status != PIPE_NO_RESET || ice->context_reset_signaled)
diff --git a/src/gallium/drivers/iris/iris_batch.h b/src/gallium/drivers/iris/iris_batch.h
index 341a3c9fe5e..f0cfe4fb031 100644
--- a/src/gallium/drivers/iris/iris_batch.h
+++ b/src/gallium/drivers/iris/iris_batch.h
@@ -446,6 +446,9 @@ iris_batch_mark_reset_sync(struct iris_batch *batch)
 const char *
 iris_batch_name_to_string(enum iris_batch_name name);
 
+bool
+iris_batch_is_banned(struct iris_bufmgr *bufmgr, int ret);
+
 #define iris_foreach_batch(ice, batch)                \
    for (struct iris_batch *batch = &ice->batches[0];  \
         batch <= &ice->batches[((struct iris_screen *)ice->ctx.screen)->devinfo->ver >= 12 ? IRIS_BATCH_BLITTER : IRIS_BATCH_COMPUTE]; \
diff --git a/src/gallium/drivers/iris/iris_blit.c b/src/gallium/drivers/iris/iris_blit.c
index 4630e65ce32..921e9ef3f43 100644
--- a/src/gallium/drivers/iris/iris_blit.c
+++ b/src/gallium/drivers/iris/iris_blit.c
@@ -230,13 +230,14 @@ apply_blit_scissor(const struct pipe_scissor_state *scissor,
 }
 
 void
-iris_blorp_surf_for_resource(struct isl_device *isl_dev,
+iris_blorp_surf_for_resource(struct iris_batch *batch,
                              struct blorp_surf *surf,
                              struct pipe_resource *p_res,
                              enum isl_aux_usage aux_usage,
                              unsigned level,
                              bool is_dest)
 {
+   const struct isl_device *isl_dev = &batch->screen->isl_dev;
    struct iris_resource *res = (void *) p_res;
    const struct intel_device_info *devinfo = isl_dev->info;
 
@@ -247,8 +248,7 @@ iris_blorp_surf_for_resource(struct isl_device *isl_dev,
          .offset = res->offset,
          .reloc_flags = is_dest ? IRIS_BLORP_RELOC_FLAGS_EXEC_OBJECT_WRITE : 0,
          .mocs = iris_mocs(res->bo, isl_dev,
-                           is_dest ? ISL_SURF_USAGE_RENDER_TARGET_BIT
-                                   : ISL_SURF_USAGE_TEXTURE_BIT),
+                           iris_blorp_batch_usage(batch, is_dest)),
          .local_hint = iris_bo_likely_local(res->bo),
       },
       .aux_usage = aux_usage,
@@ -515,10 +515,10 @@ iris_blit(struct pipe_context *ctx, const struct pipe_blit_info *info)
                                    IRIS_DOMAIN_RENDER_WRITE);
 
       struct blorp_surf src_surf, dst_surf;
-      iris_blorp_surf_for_resource(&screen->isl_dev,  &src_surf,
+      iris_blorp_surf_for_resource(batch,  &src_surf,
                                    &src_res->base.b, src_aux_usage,
                                    info->src.level, false);
-      iris_blorp_surf_for_resource(&screen->isl_dev, &dst_surf,
+      iris_blorp_surf_for_resource(batch, &dst_surf,
                                    &dst_res->base.b, dst_aux_usage,
                                    info->dst.level, true);
 
@@ -683,14 +683,14 @@ iris_copy_region(struct blorp_context *blorp,
       struct blorp_address src_addr = {
          .buffer = src_res->bo, .offset = src_res->offset + src_box->x,
          .mocs = iris_mocs(src_res->bo, &screen->isl_dev,
-                           ISL_SURF_USAGE_TEXTURE_BIT),
+                           iris_blorp_batch_usage(batch, false /* is_dest */)),
          .local_hint = iris_bo_likely_local(src_res->bo),
       };
       struct blorp_address dst_addr = {
          .buffer = dst_res->bo, .offset = dst_res->offset + dstx,
          .reloc_flags = IRIS_BLORP_RELOC_FLAGS_EXEC_OBJECT_WRITE,
          .mocs = iris_mocs(dst_res->bo, &screen->isl_dev,
-                           ISL_SURF_USAGE_RENDER_TARGET_BIT),
+                           iris_blorp_batch_usage(batch, true /* is_dest */)),
          .local_hint = iris_bo_likely_local(dst_res->bo),
       };
 
@@ -716,10 +716,10 @@ iris_copy_region(struct blorp_context *blorp,
       iris_emit_buffer_barrier_for(batch, dst_res->bo, write_domain);
 
       struct blorp_surf src_surf, dst_surf;
-      iris_blorp_surf_for_resource(&screen->isl_dev, &src_surf,
-                                   src, src_aux_usage, src_level, false);
-      iris_blorp_surf_for_resource(&screen->isl_dev, &dst_surf,
-                                   dst, dst_aux_usage, dst_level, true);
+      iris_blorp_surf_for_resource(batch, &src_surf, src,
+                                   src_aux_usage, src_level, false);
+      iris_blorp_surf_for_resource(batch, &dst_surf, dst,
+                                   dst_aux_usage, dst_level, true);
 
       for (int slice = 0; slice < src_box->depth; slice++) {
          iris_batch_maybe_flush(batch, 1500);
diff --git a/src/gallium/drivers/iris/iris_clear.c b/src/gallium/drivers/iris/iris_clear.c
index ecb52fc9d08..1ceeabaa7c1 100644
--- a/src/gallium/drivers/iris/iris_clear.c
+++ b/src/gallium/drivers/iris/iris_clear.c
@@ -326,7 +326,7 @@ fast_clear_color(struct iris_context *ice,
     */
    if (devinfo->ver >= 11) {
       iris_emit_pipe_control_flush(batch, "fast clear: pre-flush",
-         PIPE_CONTROL_STATE_CACHE_INVALIDATE | 
+         PIPE_CONTROL_STATE_CACHE_INVALIDATE |
          PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE);
    }
 
@@ -342,8 +342,8 @@ fast_clear_color(struct iris_context *ice,
    blorp_batch_init(&ice->blorp, &blorp_batch, batch, blorp_flags);
 
    struct blorp_surf surf;
-   iris_blorp_surf_for_resource(&batch->screen->isl_dev, &surf,
-                                p_res, res->aux.usage, level, true);
+   iris_blorp_surf_for_resource(batch, &surf, p_res, res->aux.usage,
+                                level, true);
 
    blorp_fast_clear(&blorp_batch, &surf, res->surf.format,
                     ISL_SWIZZLE_IDENTITY,
@@ -412,8 +412,7 @@ clear_color(struct iris_context *ice,
    iris_emit_buffer_barrier_for(batch, res->bo, IRIS_DOMAIN_RENDER_WRITE);
 
    struct blorp_surf surf;
-   iris_blorp_surf_for_resource(&batch->screen->isl_dev, &surf,
-                                p_res, aux_usage, level, true);
+   iris_blorp_surf_for_resource(batch, &surf, p_res, aux_usage, level, true);
 
    iris_batch_sync_region_start(batch);
 
@@ -631,8 +630,8 @@ clear_depth_stencil(struct iris_context *ice,
       iris_resource_prepare_render(ice, z_res, z_res->surf.format, level,
                                    box->z, box->depth, aux_usage);
       iris_emit_buffer_barrier_for(batch, z_res->bo, IRIS_DOMAIN_DEPTH_WRITE);
-      iris_blorp_surf_for_resource(&batch->screen->isl_dev, &z_surf,
-                                   &z_res->base.b, aux_usage, level, true);
+      iris_blorp_surf_for_resource(batch, &z_surf, &z_res->base.b,
+                                   aux_usage, level, true);
    }
 
    uint8_t stencil_mask = clear_stencil && stencil_res ? 0xff : 0;
@@ -641,8 +640,7 @@ clear_depth_stencil(struct iris_context *ice,
                                    box->depth, stencil_res->aux.usage, false);
       iris_emit_buffer_barrier_for(batch, stencil_res->bo,
                                    IRIS_DOMAIN_DEPTH_WRITE);
-      iris_blorp_surf_for_resource(&batch->screen->isl_dev,
-                                   &stencil_surf, &stencil_res->base.b,
+      iris_blorp_surf_for_resource(batch, &stencil_surf, &stencil_res->base.b,
                                    stencil_res->aux.usage, level, true);
    }
 
diff --git a/src/gallium/drivers/iris/iris_context.c b/src/gallium/drivers/iris/iris_context.c
index 82cdd54f365..016bc849ee3 100644
--- a/src/gallium/drivers/iris/iris_context.c
+++ b/src/gallium/drivers/iris/iris_context.c
@@ -298,7 +298,11 @@ iris_create_context(struct pipe_screen *pscreen, void *priv, unsigned flags)
    ctx->screen = pscreen;
    ctx->priv = priv;
 
-   ctx->stream_uploader = u_upload_create_default(ctx);
+   ctx->stream_uploader = u_upload_create(ctx, 1024 * 1024 * 2,
+                                          PIPE_BIND_VERTEX_BUFFER |
+                                          PIPE_BIND_INDEX_BUFFER |
+                                          PIPE_BIND_CONSTANT_BUFFER,
+                                          PIPE_USAGE_STREAM, 0);
    if (!ctx->stream_uploader) {
       ralloc_free(ice);
       return NULL;
diff --git a/src/gallium/drivers/iris/iris_context.h b/src/gallium/drivers/iris/iris_context.h
index 74461c18eac..7d154ddfb1c 100644
--- a/src/gallium/drivers/iris/iris_context.h
+++ b/src/gallium/drivers/iris/iris_context.h
@@ -955,7 +955,7 @@ void iris_fill_cs_push_const_buffer(struct brw_cs_prog_data *cs_prog_data,
 /* iris_blit.c */
 #define IRIS_BLORP_RELOC_FLAGS_EXEC_OBJECT_WRITE      (1 << 2)
 
-void iris_blorp_surf_for_resource(struct isl_device *isl_dev,
+void iris_blorp_surf_for_resource(struct iris_batch *batch,
                                   struct blorp_surf *surf,
                                   struct pipe_resource *p_res,
                                   enum isl_aux_usage aux_usage,
@@ -982,6 +982,21 @@ iris_blorp_flags_for_batch(struct iris_batch *batch)
    return 0;
 }
 
+static inline isl_surf_usage_flags_t
+iris_blorp_batch_usage(struct iris_batch *batch, bool is_dest)
+{
+   switch (batch->name) {
+   case IRIS_BATCH_RENDER:
+      return is_dest ? ISL_SURF_USAGE_RENDER_TARGET_BIT : ISL_SURF_USAGE_TEXTURE_BIT;
+   case IRIS_BATCH_COMPUTE:
+      return is_dest ? ISL_SURF_USAGE_STORAGE_BIT : ISL_SURF_USAGE_TEXTURE_BIT;
+   case IRIS_BATCH_BLITTER:
+      return is_dest ? ISL_SURF_USAGE_BLITTER_DST_BIT : ISL_SURF_USAGE_BLITTER_SRC_BIT;
+   default:
+      unreachable("Unhandled batch type");
+   }
+}
+
 /* iris_draw.c */
 
 void iris_draw_vbo(struct pipe_context *ctx, const struct pipe_draw_info *info,
diff --git a/src/gallium/drivers/iris/iris_resolve.c b/src/gallium/drivers/iris/iris_resolve.c
index a6c2c62d785..c4da1761b7a 100644
--- a/src/gallium/drivers/iris/iris_resolve.c
+++ b/src/gallium/drivers/iris/iris_resolve.c
@@ -508,8 +508,8 @@ iris_resolve_color(struct iris_context *ice,
    //DBG("%s to mt %p level %u layer %u\n", __func__, mt, level, layer);
 
    struct blorp_surf surf;
-   iris_blorp_surf_for_resource(&batch->screen->isl_dev, &surf,
-                                &res->base.b, res->aux.usage, level, true);
+   iris_blorp_surf_for_resource(batch, &surf, &res->base.b,
+                                res->aux.usage, level, true);
 
    iris_batch_maybe_flush(batch, 1500);
 
@@ -576,8 +576,8 @@ iris_mcs_exec(struct iris_context *ice,
    iris_batch_maybe_flush(batch, 1500);
 
    struct blorp_surf surf;
-   iris_blorp_surf_for_resource(&batch->screen->isl_dev, &surf,
-                                &res->base.b, res->aux.usage, 0, true);
+   iris_blorp_surf_for_resource(batch, &surf, &res->base.b,
+                                res->aux.usage, 0, true);
 
    /* MCS partial resolve will read from the MCS surface. */
    assert(res->aux.bo == res->bo);
@@ -596,10 +596,10 @@ iris_mcs_exec(struct iris_context *ice,
        * the full resolve.
        */
       struct blorp_surf src_surf, dst_surf;
-      iris_blorp_surf_for_resource(&batch->screen->isl_dev, &src_surf,
-                                   &res->base.b, res->aux.usage, 0, false);
-      iris_blorp_surf_for_resource(&batch->screen->isl_dev, &dst_surf,
-                                   &res->base.b, ISL_AUX_USAGE_NONE, 0, true);
+      iris_blorp_surf_for_resource(batch, &src_surf, &res->base.b,
+                                   res->aux.usage, 0, false);
+      iris_blorp_surf_for_resource(batch, &dst_surf, &res->base.b,
+                                   ISL_AUX_USAGE_NONE, 0, true);
 
       blorp_copy(&blorp_batch, &src_surf, 0, 0, &dst_surf, 0, 0,
                  0, 0, 0, 0, surf.surf->logical_level0_px.width,
@@ -730,8 +730,8 @@ iris_hiz_exec(struct iris_context *ice,
    iris_batch_sync_region_start(batch);
 
    struct blorp_surf surf;
-   iris_blorp_surf_for_resource(&batch->screen->isl_dev, &surf,
-                                &res->base.b, res->aux.usage, level, true);
+   iris_blorp_surf_for_resource(batch, &surf, &res->base.b,
+                                res->aux.usage, level, true);
 
    struct blorp_batch blorp_batch;
    enum blorp_batch_flags flags = 0;
diff --git a/src/gallium/drivers/iris/iris_resource.c b/src/gallium/drivers/iris/iris_resource.c
index 09dbcc0c5d4..9b9bfd267cd 100644
--- a/src/gallium/drivers/iris/iris_resource.c
+++ b/src/gallium/drivers/iris/iris_resource.c
@@ -745,6 +745,10 @@ iris_resource_configure_main(const struct iris_screen *screen,
 
    if (res->mod_info && !isl_drm_modifier_has_aux(modifier))
       usage |= ISL_SURF_USAGE_DISABLE_AUX_BIT;
+
+   else if (!res->mod_info && res->external_format != PIPE_FORMAT_NONE)
+      usage |= ISL_SURF_USAGE_DISABLE_AUX_BIT;
+
    else if (templ->bind & PIPE_BIND_CONST_BW)
       usage |= ISL_SURF_USAGE_DISABLE_AUX_BIT;
 
diff --git a/src/gallium/drivers/iris/iris_state.c b/src/gallium/drivers/iris/iris_state.c
index 51bf69debef..276949aaca9 100644
--- a/src/gallium/drivers/iris/iris_state.c
+++ b/src/gallium/drivers/iris/iris_state.c
@@ -6052,7 +6052,9 @@ batch_emit_fast_color_dummy_blit(struct iris_batch *batch)
 #if GFX_VERx10 >= 125
    iris_emit_cmd(batch, GENX(XY_FAST_COLOR_BLT), blt) {
       blt.DestinationBaseAddress = batch->screen->workaround_address;
-      blt.DestinationMOCS = batch->screen->isl_dev.mocs.blitter_dst;
+      blt.DestinationMOCS = iris_mocs(batch->screen->workaround_address.bo,
+                                      &batch->screen->isl_dev,
+                                      ISL_SURF_USAGE_BLITTER_DST_BIT);
       blt.DestinationPitch = 63;
       blt.DestinationX2 = 1;
       blt.DestinationY2 = 4;
@@ -7135,7 +7137,12 @@ iris_upload_dirty_render_state(struct iris_context *ice,
       }
    }
 
+#if GFX_VERx10 >= 125
+   /* This is only used on >= gfx125 for dynamic 3DSTATE_TE emission
+    * related workarounds.
+    */
    bool program_needs_wa_14015055625 = false;
+#endif
 
 #if INTEL_WA_14015055625_GFX_VER
    /* Check if FS stage will use primitive ID overrides for Wa_14015055625. */
@@ -7239,16 +7246,14 @@ iris_upload_dirty_render_state(struct iris_context *ice,
                             GENX(3DSTATE_PS_length));
             iris_emit_merge(batch, shader_psx, psx_state,
                             GENX(3DSTATE_PS_EXTRA_length));
-         } else if (stage == MESA_SHADER_TESS_EVAL &&
-                    intel_needs_workaround(batch->screen->devinfo, 14015055625) &&
-                    !program_needs_wa_14015055625) {
-            /* This program doesn't require Wa_14015055625, so we can enable
-             * a Tessellation Distribution Mode.
-             */
 #if GFX_VERx10 >= 125
+         } else if (stage == MESA_SHADER_TESS_EVAL) {
             uint32_t te_state[GENX(3DSTATE_TE_length)] = { 0 };
             iris_pack_command(GENX(3DSTATE_TE), te_state, te) {
-               if (intel_needs_workaround(batch->screen->devinfo, 22012699309))
+               if (intel_needs_workaround(screen->devinfo, 14015055625) &&
+                   program_needs_wa_14015055625)
+                  te.TessellationDistributionMode = TEDMODE_OFF;
+               else if (intel_needs_workaround(screen->devinfo, 22012699309))
                   te.TessellationDistributionMode = TEDMODE_RR_STRICT;
                else
                   te.TessellationDistributionMode = TEDMODE_RR_FREE;
@@ -7273,7 +7278,13 @@ iris_upload_dirty_render_state(struct iris_context *ice,
             switch (stage) {
             case MESA_SHADER_VERTEX:    MERGE_SCRATCH_ADDR(3DSTATE_VS); break;
             case MESA_SHADER_TESS_CTRL: MERGE_SCRATCH_ADDR(3DSTATE_HS); break;
-            case MESA_SHADER_TESS_EVAL: MERGE_SCRATCH_ADDR(3DSTATE_DS); break;
+            case MESA_SHADER_TESS_EVAL: {
+               uint32_t *shader_ds = (uint32_t *) shader->derived_data;
+               uint32_t *shader_te = shader_ds + GENX(3DSTATE_DS_length);
+               iris_batch_emit(batch, shader_te, 4 * GENX(3DSTATE_TE_length));
+               MERGE_SCRATCH_ADDR(3DSTATE_DS);
+               break;
+            }
             case MESA_SHADER_GEOMETRY:  MERGE_SCRATCH_ADDR(3DSTATE_GS); break;
             }
          } else {
@@ -9526,10 +9537,12 @@ iris_emit_raw_pipe_control(struct iris_batch *batch,
    /* "GPGPU specific workarounds" (both post-sync and flush) ------------ */
 
    if (IS_COMPUTE_PIPELINE(batch)) {
-      if ((GFX_VER == 9 || GFX_VER == 11) &&
-          (flags & PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE)) {
-         /* Project: SKL, ICL / Argument: Tex Invalidate
-          * "Requires stall bit ([20] of DW) set for all GPGPU Workloads."
+      if (GFX_VER >= 9 && (flags & PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE)) {
+         /* SKL PRMs, Volume 7: 3D-Media-GPGPU, Programming Restrictions for
+          * PIPE_CONTROL, Flush Types:
+          *   "Requires stall bit ([20] of DW) set for all GPGPU Workloads."
+          * For newer platforms this is documented in the PIPE_CONTROL
+          * instruction page.
           */
          flags |= PIPE_CONTROL_CS_STALL;
       }
diff --git a/src/gallium/drivers/iris/xe/iris_batch.c b/src/gallium/drivers/iris/xe/iris_batch.c
index 0c0fc208cb9..7e09d352773 100644
--- a/src/gallium/drivers/iris/xe/iris_batch.c
+++ b/src/gallium/drivers/iris/xe/iris_batch.c
@@ -151,7 +151,45 @@ void iris_xe_init_batches(struct iris_context *ice)
    free(engines_info);
 }
 
-void iris_xe_destroy_batch(struct iris_batch *batch)
+/*
+ * Wait for all previous DRM_IOCTL_XE_EXEC calls over the
+ * drm_xe_exec_queue in this iris_batch to complete.
+ **/
+static void
+iris_xe_wait_exec_queue_idle(struct iris_batch *batch)
+{
+   struct iris_bufmgr *bufmgr = batch->screen->bufmgr;
+   struct iris_syncobj *syncobj = iris_create_syncobj(bufmgr);
+   struct drm_xe_sync xe_sync = {
+      .type = DRM_XE_SYNC_TYPE_SYNCOBJ,
+      .flags = DRM_XE_SYNC_FLAG_SIGNAL,
+   };
+   struct drm_xe_exec exec = {
+      .exec_queue_id = batch->xe.exec_queue_id,
+      .num_syncs = 1,
+      .syncs = (uintptr_t)&xe_sync,
+   };
+   int ret;
+
+   if (!syncobj)
+      return;
+
+   xe_sync.handle = syncobj->handle;
+   /* Using the special exec.num_batch_buffer == 0 handling to get syncobj
+    * signaled when the last DRM_IOCTL_XE_EXEC is completed.
+    */
+   ret = intel_ioctl(iris_bufmgr_get_fd(bufmgr), DRM_IOCTL_XE_EXEC, &exec);
+   if (ret == 0) {
+      assert(iris_wait_syncobj(bufmgr, syncobj, INT64_MAX));
+   } else {
+      assert(iris_batch_is_banned(bufmgr, errno) == true);
+   }
+
+   iris_syncobj_destroy(bufmgr, syncobj);
+}
+
+static void
+iris_xe_destroy_exec_queue(struct iris_batch *batch)
 {
    struct iris_screen *screen = batch->screen;
    struct iris_bufmgr *bufmgr = screen->bufmgr;
@@ -165,6 +203,15 @@ void iris_xe_destroy_batch(struct iris_batch *batch)
    assert(ret == 0);
 }
 
+void iris_xe_destroy_batch(struct iris_batch *batch)
+{
+   /* Xe KMD don't refcount anything, so resources could be freed while they
+    * are still in use if we don't wait for exec_queue to be idle.
+    */
+   iris_xe_wait_exec_queue_idle(batch);
+   iris_xe_destroy_exec_queue(batch);
+}
+
 bool iris_xe_replace_batch(struct iris_batch *batch)
 {
    enum intel_engine_class engine_classes[IRIS_BATCH_COUNT];
@@ -184,7 +231,7 @@ bool iris_xe_replace_batch(struct iris_batch *batch)
    ret = iris_xe_init_batch(bufmgr, engines_info, engine_classes[batch->name],
                             ice->priority, &new_exec_queue_id);
    if (ret) {
-      iris_xe_destroy_batch(batch);
+      iris_xe_destroy_exec_queue(batch);
       batch->xe.exec_queue_id = new_exec_queue_id;
       iris_lost_context_state(batch);
    }
diff --git a/src/gallium/drivers/iris/xe/iris_kmd_backend.c b/src/gallium/drivers/iris/xe/iris_kmd_backend.c
index 2f49c4d8bea..ea49f6da8db 100644
--- a/src/gallium/drivers/iris/xe/iris_kmd_backend.c
+++ b/src/gallium/drivers/iris/xe/iris_kmd_backend.c
@@ -138,10 +138,6 @@ xe_gem_vm_bind_op(struct iris_bo *bo, uint32_t op)
          op = DRM_XE_VM_BIND_OP_MAP_USERPTR;
    }
 
-   uint16_t pat_index = 0;
-   if (op != DRM_XE_VM_BIND_OP_UNMAP)
-      pat_index = iris_heap_to_pat_entry(devinfo, bo->real.heap)->index;
-
    struct drm_xe_vm_bind args = {
       .vm_id = iris_bufmgr_get_global_vm_id(bo->bufmgr),
       .num_syncs = 1,
@@ -152,7 +148,7 @@ xe_gem_vm_bind_op(struct iris_bo *bo, uint32_t op)
       .bind.range = range,
       .bind.addr = intel_48b_address(bo->address),
       .bind.op = op,
-      .bind.pat_index = pat_index,
+      .bind.pat_index = iris_heap_to_pat_entry(devinfo, bo->real.heap)->index,
    };
    ret = intel_ioctl(fd, DRM_IOCTL_XE_VM_BIND, &args);
    if (ret == 0) {
diff --git a/src/gallium/drivers/lima/ci/lima-fails.txt b/src/gallium/drivers/lima/ci/lima-fails.txt
index 948ca471b95..4c4c4ccfa46 100644
--- a/src/gallium/drivers/lima/ci/lima-fails.txt
+++ b/src/gallium/drivers/lima/ci/lima-fails.txt
@@ -63,22 +63,12 @@ x11-dEQP-EGL.functional.wide_color.window_8888_colorspace_srgb,Fail
 
 shaders@glsl-arb-fragment-coord-conventions,Fail
 shaders@glsl-bug-110796,Fail
-shaders@glsl-bug-22603,Fail
 shaders@glsl-fs-flat-color,Fail
 shaders@glsl-predication-on-large-array,Fail
 shaders@glsl-routing,Fail
 spec@arb_arrays_of_arrays@execution@glsl-arrays-copy-size-mismatch,Fail
 spec@arb_color_buffer_float@gl_rgba8-render,Fail
 spec@arb_color_buffer_float@gl_rgba8-render-sanity,Fail
-spec@arb_depth_texture@fbo-depth-gl_depth_component16-blit,Fail
-spec@arb_depth_texture@fbo-depth-gl_depth_component16-copypixels,Fail
-spec@arb_depth_texture@fbo-depth-gl_depth_component16-readpixels,Fail
-spec@arb_depth_texture@fbo-depth-gl_depth_component24-blit,Fail
-spec@arb_depth_texture@fbo-depth-gl_depth_component24-copypixels,Fail
-spec@arb_depth_texture@fbo-depth-gl_depth_component24-readpixels,Fail
-spec@arb_depth_texture@fbo-depth-gl_depth_component32-blit,Fail
-spec@arb_depth_texture@fbo-depth-gl_depth_component32-copypixels,Fail
-spec@arb_depth_texture@fbo-depth-gl_depth_component32-readpixels,Fail
 spec@arb_depth_texture@fbo-generatemipmap-formats,Fail
 spec@arb_depth_texture@fbo-generatemipmap-formats@GL_DEPTH_COMPONENT24,Fail
 spec@arb_depth_texture@fbo-generatemipmap-formats@GL_DEPTH_COMPONENT24 NPOT,Fail
@@ -157,22 +147,22 @@ spec@arb_texture_rg@texwrap formats bordercolor@GL_RG8- border color only,Fail
 spec@arb_texture_rg@texwrap formats bordercolor-swizzled,Fail
 spec@arb_texture_rg@texwrap formats bordercolor-swizzled@GL_RG8- swizzled- border color only,Fail
 spec@arb_texture_storage@texture-storage@cube array texture,Fail
-spec@arb_vertex_program@arl,Crash
+spec@arb_vertex_program@arl,Fail
 spec@arb_vertex_program@big-param,Fail
 spec@arb_vertex_program@clip-plane-transformation arb,Fail
-spec@arb_vertex_program@instructions@arl,Crash
-spec@arb_vertex_program@vp-address-01,Crash
-spec@arb_vertex_program@vp-arl-constant-array,Crash
-spec@arb_vertex_program@vp-arl-constant-array-huge,Crash
-spec@arb_vertex_program@vp-arl-constant-array-huge-offset,Crash
-spec@arb_vertex_program@vp-arl-constant-array-huge-offset-neg,Crash
-spec@arb_vertex_program@vp-arl-constant-array-huge-relative-offset,Crash
-spec@arb_vertex_program@vp-arl-constant-array-huge-varying,Crash
-spec@arb_vertex_program@vp-arl-constant-array-varying,Crash
-spec@arb_vertex_program@vp-arl-env-array,Crash
-spec@arb_vertex_program@vp-arl-local-array,Crash
-spec@arb_vertex_program@vp-arl-neg-array-2,Crash
-spec@arb_vertex_program@vp-arl-neg-array,Crash
+spec@arb_vertex_program@instructions@arl,Fail
+spec@arb_vertex_program@vp-address-01,Fail
+spec@arb_vertex_program@vp-arl-constant-array,Fail
+spec@arb_vertex_program@vp-arl-constant-array-huge,Fail
+spec@arb_vertex_program@vp-arl-constant-array-huge-offset,Fail
+spec@arb_vertex_program@vp-arl-constant-array-huge-offset-neg,Fail
+spec@arb_vertex_program@vp-arl-constant-array-huge-relative-offset,Fail
+spec@arb_vertex_program@vp-arl-constant-array-huge-varying,Fail
+spec@arb_vertex_program@vp-arl-constant-array-varying,Fail
+spec@arb_vertex_program@vp-arl-env-array,Fail
+spec@arb_vertex_program@vp-arl-local-array,Fail
+spec@arb_vertex_program@vp-arl-neg-array-2,Fail
+spec@arb_vertex_program@vp-arl-neg-array,Fail
 spec@ati_fragment_shader@ati_fragment_shader-render-default,Fail
 spec@ati_fragment_shader@ati_fragment_shader-render-notexture,Fail
 spec@ati_fragment_shader@ati_fragment_shader-render-sources,Fail
@@ -197,26 +187,15 @@ spec@ext_framebuffer_object@fbo-blending-formats@GL_RGB,Fail
 spec@ext_framebuffer_object@fbo-cubemap,Fail
 spec@ext_framebuffer_object@fbo-depth-sample-compare,Fail
 spec@ext_framebuffer_object@fbo-maxsize,Fail
-spec@ext_framebuffer_object@fbo-readpixels-depth-formats,Fail
-spec@ext_framebuffer_object@fbo-readpixels-depth-formats@GL_DEPTH_COMPONENT/GL_FLOAT,Fail
-spec@ext_framebuffer_object@fbo-readpixels-depth-formats@GL_DEPTH_COMPONENT/GL_UNSIGNED_INT,Fail
 spec@ext_framebuffer_object@fbo-scissor-bitmap,Fail
 spec@ext_framebuffer_object@fbo-stencil-gl_stencil_index16-blit,Fail
 spec@ext_framebuffer_object@fbo-stencil-gl_stencil_index16-copypixels,Fail
-spec@ext_framebuffer_object@fbo-stencil-gl_stencil_index16-drawpixels,Fail
-spec@ext_framebuffer_object@fbo-stencil-gl_stencil_index16-readpixels,Fail
 spec@ext_framebuffer_object@fbo-stencil-gl_stencil_index1-blit,Fail
 spec@ext_framebuffer_object@fbo-stencil-gl_stencil_index1-copypixels,Fail
-spec@ext_framebuffer_object@fbo-stencil-gl_stencil_index1-drawpixels,Fail
-spec@ext_framebuffer_object@fbo-stencil-gl_stencil_index1-readpixels,Fail
 spec@ext_framebuffer_object@fbo-stencil-gl_stencil_index4-blit,Fail
 spec@ext_framebuffer_object@fbo-stencil-gl_stencil_index4-copypixels,Fail
-spec@ext_framebuffer_object@fbo-stencil-gl_stencil_index4-drawpixels,Fail
-spec@ext_framebuffer_object@fbo-stencil-gl_stencil_index4-readpixels,Fail
 spec@ext_framebuffer_object@fbo-stencil-gl_stencil_index8-blit,Fail
 spec@ext_framebuffer_object@fbo-stencil-gl_stencil_index8-copypixels,Fail
-spec@ext_framebuffer_object@fbo-stencil-gl_stencil_index8-drawpixels,Fail
-spec@ext_framebuffer_object@fbo-stencil-gl_stencil_index8-readpixels,Fail
 spec@ext_image_dma_buf_import@ext_image_dma_buf_import-export,Fail
 spec@ext_image_dma_buf_import@ext_image_dma_buf_import-sample_p010,Fail
 spec@ext_image_dma_buf_import@ext_image_dma_buf_import-sample_p012,Fail
@@ -229,19 +208,8 @@ spec@ext_image_dma_buf_import@ext_image_dma_buf_import-sample_y412,Fail
 spec@ext_image_dma_buf_import@ext_image_dma_buf_import-sample_y416,Fail
 spec@ext_packed_depth_stencil@depth_stencil texture,Fail
 spec@ext_packed_depth_stencil@fbo-blit-d24s8,Fail
-spec@ext_packed_depth_stencil@fbo-depth-gl_depth24_stencil8-blit,Fail
-spec@ext_packed_depth_stencil@fbo-depth-gl_depth24_stencil8-copypixels,Fail
-spec@ext_packed_depth_stencil@fbo-depth-gl_depth24_stencil8-readpixels,Fail
-spec@ext_packed_depth_stencil@fbo-depthstencil-gl_depth24_stencil8-blit,Fail
-spec@ext_packed_depth_stencil@fbo-depthstencil-gl_depth24_stencil8-copypixels,Fail
-spec@ext_packed_depth_stencil@fbo-depthstencil-gl_depth24_stencil8-drawpixels-24_8,Fail
-spec@ext_packed_depth_stencil@fbo-depthstencil-gl_depth24_stencil8-readpixels-24_8,Fail
-spec@ext_packed_depth_stencil@fbo-depthstencil-gl_depth24_stencil8-readpixels-float-and-ushort,Fail
 spec@ext_packed_depth_stencil@fbo-stencil-gl_depth24_stencil8-blit,Fail
 spec@ext_packed_depth_stencil@fbo-stencil-gl_depth24_stencil8-copypixels,Fail
-spec@ext_packed_depth_stencil@fbo-stencil-gl_depth24_stencil8-drawpixels,Fail
-spec@ext_packed_depth_stencil@fbo-stencil-gl_depth24_stencil8-readpixels,Fail
-spec@ext_packed_depth_stencil@readpixels-24_8,Fail
 spec@ext_provoking_vertex@provoking-vertex,Fail
 spec@ext_texture_format_bgra8888@api-errors,Fail
 spec@ext_texture_lod_bias@lodbias,Fail
diff --git a/src/gallium/drivers/lima/ir/gp/nir.c b/src/gallium/drivers/lima/ir/gp/nir.c
index 765da082ed0..4b02e60a8fc 100644
--- a/src/gallium/drivers/lima/ir/gp/nir.c
+++ b/src/gallium/drivers/lima/ir/gp/nir.c
@@ -253,6 +253,11 @@ static bool gpir_emit_intrinsic(gpir_block *block, nir_instr *ni)
    case nir_intrinsic_load_uniform:
    {
       int offset = nir_intrinsic_base(instr);
+
+      if (!nir_src_is_const(instr->src[0])) {
+         gpir_error("indirect indexing for uniforms is not implemented\n");
+         return false;
+      }
       offset += (int)nir_src_as_float(instr->src[0]);
 
       return gpir_create_load(block, &instr->def,
diff --git a/src/gallium/drivers/lima/ir/pp/nir.c b/src/gallium/drivers/lima/ir/pp/nir.c
index 3bc129beda9..517ec628b41 100644
--- a/src/gallium/drivers/lima/ir/pp/nir.c
+++ b/src/gallium/drivers/lima/ir/pp/nir.c
@@ -403,6 +403,8 @@ static bool ppir_emit_intrinsic(ppir_block *block, nir_instr *ni)
          default: {
             ppir_dest *dest = ppir_node_get_dest(node);
             dest->ssa.out_type = out_type;
+            dest->ssa.num_components = 4;
+            dest->write_mask = u_bit_consecutive(0, 4);
             node->is_out = 1;
             return true;
             }
@@ -415,9 +417,9 @@ static bool ppir_emit_intrinsic(ppir_block *block, nir_instr *ni)
 
       ppir_dest *dest = ppir_node_get_dest(&alu_node->node);
       dest->type = ppir_target_ssa;
-      dest->ssa.num_components = instr->num_components;
+      dest->ssa.num_components = 4;
       dest->ssa.index = 0;
-      dest->write_mask = u_bit_consecutive(0, instr->num_components);
+      dest->write_mask = u_bit_consecutive(0, 4);
       dest->ssa.out_type = out_type;
 
       alu_node->num_src = 1;
@@ -427,7 +429,7 @@ static bool ppir_emit_intrinsic(ppir_block *block, nir_instr *ni)
 
       nir_legacy_src legacy_src = nir_legacy_chase_src(instr->src);
       ppir_node_add_src(block->comp, &alu_node->node, alu_node->src, &legacy_src,
-                        u_bit_consecutive(0, instr->num_components));
+                        u_bit_consecutive(0, 4));
 
       alu_node->node.is_out = 1;
 
diff --git a/src/gallium/drivers/lima/lima_program.c b/src/gallium/drivers/lima/lima_program.c
index 2d6dd4c9944..0ef3ebbfc98 100644
--- a/src/gallium/drivers/lima/lima_program.c
+++ b/src/gallium/drivers/lima/lima_program.c
@@ -323,15 +323,25 @@ static bool
 lima_fs_upload_shader(struct lima_context *ctx,
                       struct lima_fs_compiled_shader *fs)
 {
+   static const uint32_t pp_clear_program[] = {
+      PP_CLEAR_PROGRAM
+   };
+   int shader_size = sizeof(pp_clear_program);
+   void *shader = (void *)pp_clear_program;
    struct lima_screen *screen = lima_screen(ctx->base.screen);
 
-   fs->bo = lima_bo_create(screen, fs->state.shader_size, 0);
+   if (fs->state.shader_size) {
+      shader_size = fs->state.shader_size;
+      shader = fs->shader;
+   }
+
+   fs->bo = lima_bo_create(screen, shader_size, 0);
    if (!fs->bo) {
       fprintf(stderr, "lima: create fs shader bo fail\n");
       return false;
    }
 
-   memcpy(lima_bo_map(fs->bo), fs->shader, fs->state.shader_size);
+   memcpy(lima_bo_map(fs->bo), shader, shader_size);
 
    return true;
 }
diff --git a/src/gallium/drivers/lima/lima_screen.c b/src/gallium/drivers/lima/lima_screen.c
index 8a58b5c6b2b..2e779693d0e 100644
--- a/src/gallium/drivers/lima/lima_screen.c
+++ b/src/gallium/drivers/lima/lima_screen.c
@@ -692,11 +692,9 @@ lima_screen_create(int fd, const struct pipe_screen_config *config,
    screen->pp_buffer->cacheable = false;
 
    /* fs program for clear buffer?
-    * const0 1 0 0 -1.67773, mov.v0 $0 ^const0.xxxx, stop
     */
    static const uint32_t pp_clear_program[] = {
-      0x00020425, 0x0000000c, 0x01e007cf, 0xb0000000,
-      0x000005f5, 0x00000000, 0x00000000, 0x00000000,
+      PP_CLEAR_PROGRAM
    };
    memcpy(lima_bo_map(screen->pp_buffer) + pp_clear_program_offset,
           pp_clear_program, sizeof(pp_clear_program));
diff --git a/src/gallium/drivers/lima/lima_screen.h b/src/gallium/drivers/lima/lima_screen.h
index 82800052b15..5f76edd551a 100644
--- a/src/gallium/drivers/lima/lima_screen.h
+++ b/src/gallium/drivers/lima/lima_screen.h
@@ -60,6 +60,11 @@ struct ra_regs;
 
 #define NR_BO_CACHE_BUCKETS (MAX_BO_CACHE_BUCKET - MIN_BO_CACHE_BUCKET + 1)
 
+/* const0 1 0 0 -1.67773, mov.v0 $0 ^const0.xxxx, stop */
+#define PP_CLEAR_PROGRAM \
+   0x00020425, 0x0000000c, 0x01e007cf, 0xb0000000, \
+   0x000005f5, 0x00000000, 0x00000000, 0x00000000, \
+
 struct lima_screen {
    struct pipe_screen base;
    struct renderonly *ro;
diff --git a/src/gallium/drivers/llvmpipe/ci/traces-llvmpipe.yml b/src/gallium/drivers/llvmpipe/ci/traces-llvmpipe.yml
index 57a0bfda02a..14edd04c9b0 100644
--- a/src/gallium/drivers/llvmpipe/ci/traces-llvmpipe.yml
+++ b/src/gallium/drivers/llvmpipe/ci/traces-llvmpipe.yml
@@ -111,7 +111,7 @@ traces:
       checksum: 58a6a276abc0e28fcb2a8acea3342712
   gputest/pixmark-piano-v2.trace:
     gl-vmware-llvmpipe:
-      checksum: edc09da55fea262e76686d99548f2cfd
+      checksum: b0077264046fe6dd2cdec059d9e53bf5
   gputest/triangle-v2.trace:
     gl-vmware-llvmpipe:
       checksum: 7812de00011a3a059892e36cea19c696
diff --git a/src/gallium/drivers/llvmpipe/lp_query.c b/src/gallium/drivers/llvmpipe/lp_query.c
index d63c6aae0a3..4de16e9b657 100644
--- a/src/gallium/drivers/llvmpipe/lp_query.c
+++ b/src/gallium/drivers/llvmpipe/lp_query.c
@@ -372,12 +372,12 @@ llvmpipe_get_query_result_resource(struct pipe_context *pipe,
       switch (result_type) {
       case PIPE_QUERY_TYPE_I32: {
          int32_t *iptr = (int32_t *)dst;
-         *iptr = (int32_t) MIN2(value, INT32_MAX);
+         *iptr = (int32_t) (value & INT32_MAX);
          break;
       }
       case PIPE_QUERY_TYPE_U32: {
          uint32_t *uptr = (uint32_t *)dst;
-         *uptr = (uint32_t) MIN2(value, UINT32_MAX);
+         *uptr = (uint32_t) (value & UINT32_MAX);
          break;
       }
       case PIPE_QUERY_TYPE_I64: {
diff --git a/src/gallium/drivers/llvmpipe/lp_screen.c b/src/gallium/drivers/llvmpipe/lp_screen.c
index ad3d66424e1..ae341bfda13 100644
--- a/src/gallium/drivers/llvmpipe/lp_screen.c
+++ b/src/gallium/drivers/llvmpipe/lp_screen.c
@@ -410,6 +410,11 @@ llvmpipe_get_shader_param(struct pipe_screen *screen,
             return PIPE_MAX_SHADER_SAMPLER_VIEWS;
          else
             return 0;
+      case PIPE_SHADER_CAP_MAX_CONST_BUFFERS:
+         if (debug_get_bool_option("DRAW_USE_LLVM", false))
+            return LP_MAX_TGSI_CONST_BUFFERS;
+         else
+            return draw_get_shader_param(shader, param);
       default:
          return draw_get_shader_param(shader, param);
       }
diff --git a/src/gallium/drivers/llvmpipe/lp_state_cs.c b/src/gallium/drivers/llvmpipe/lp_state_cs.c
index c4661ced025..46cc5ffb06f 100644
--- a/src/gallium/drivers/llvmpipe/lp_state_cs.c
+++ b/src/gallium/drivers/llvmpipe/lp_state_cs.c
@@ -95,7 +95,7 @@ enum {
    CS_ARG_VERTEX_DATA,
    CS_ARG_PER_THREAD_DATA,
    CS_ARG_OUTER_COUNT,
-   CS_ARG_CORO_X_LOOPS = CS_ARG_OUTER_COUNT,
+   CS_ARG_CORO_SUBGROUP_COUNT = CS_ARG_OUTER_COUNT,
    CS_ARG_CORO_PARTIALS,
    CS_ARG_CORO_BLOCK_X_SIZE,
    CS_ARG_CORO_BLOCK_Y_SIZE,
@@ -374,7 +374,7 @@ generate_compute(struct llvmpipe_context *lp,
    else
       arg_types[CS_ARG_VERTEX_DATA] = LLVMPointerType(LLVMInt8TypeInContext(gallivm->context), 0); /* mesh shaders only */
    arg_types[CS_ARG_PER_THREAD_DATA] = variant->jit_cs_thread_data_ptr_type;  /* per thread data */
-   arg_types[CS_ARG_CORO_X_LOOPS] = int32_type;                        /* coro only - num X loops */
+   arg_types[CS_ARG_CORO_SUBGROUP_COUNT] = int32_type;                 /* coro only - subgroup count */
    arg_types[CS_ARG_CORO_PARTIALS] = int32_type;                       /* coro only - partials */
    arg_types[CS_ARG_CORO_BLOCK_X_SIZE] = int32_type;                   /* coro block_x_size */
    arg_types[CS_ARG_CORO_BLOCK_Y_SIZE] = int32_type;                   /* coro block_y_size */
@@ -560,23 +560,24 @@ generate_compute(struct llvmpipe_context *lp,
       output_array = lp_build_array_alloca(gallivm, output_type, lp_build_const_int32(gallivm, align(MAX2(nir->info.mesh.max_primitives_out, nir->info.mesh.max_vertices_out), 8)), "outputs");
    }
 
-   struct lp_build_loop_state loop_state[4];
-   LLVMValueRef num_x_loop;
+   struct lp_build_loop_state loop_state[2];
+
    LLVMValueRef vec_length = lp_build_const_int32(gallivm, cs_type.length);
-   num_x_loop = LLVMBuildAdd(gallivm->builder, block_x_size_arg, vec_length, "");
-   num_x_loop = LLVMBuildSub(gallivm->builder, num_x_loop, lp_build_const_int32(gallivm, 1), "");
-   num_x_loop = LLVMBuildUDiv(gallivm->builder, num_x_loop, vec_length, "");
-   LLVMValueRef partials = LLVMBuildURem(gallivm->builder, block_x_size_arg, vec_length, "");
 
-   LLVMValueRef coro_num_hdls = LLVMBuildMul(gallivm->builder, num_x_loop, block_y_size_arg, "");
-   coro_num_hdls = LLVMBuildMul(gallivm->builder, coro_num_hdls, block_z_size_arg, "");
+   LLVMValueRef invocation_count = LLVMBuildMul(gallivm->builder, block_x_size_arg, block_y_size_arg, "");
+   invocation_count = LLVMBuildMul(gallivm->builder, invocation_count, block_z_size_arg, "");
+
+   LLVMValueRef partials = LLVMBuildURem(gallivm->builder, invocation_count, vec_length, "");
+
+   LLVMValueRef num_subgroup_loop = LLVMBuildAdd(gallivm->builder, invocation_count, lp_build_const_int32(gallivm, cs_type.length - 1), "");
+   num_subgroup_loop = LLVMBuildUDiv(gallivm->builder, num_subgroup_loop, vec_length, "");
 
    /* build a ptr in memory to store all the frames in later. */
    LLVMTypeRef hdl_ptr_type = LLVMPointerType(LLVMInt8TypeInContext(gallivm->context), 0);
    LLVMValueRef coro_mem = LLVMBuildAlloca(gallivm->builder, hdl_ptr_type, "coro_mem");
    LLVMBuildStore(builder, LLVMConstNull(hdl_ptr_type), coro_mem);
 
-   LLVMValueRef coro_hdls = LLVMBuildArrayAlloca(gallivm->builder, hdl_ptr_type, coro_num_hdls, "coro_hdls");
+   LLVMValueRef coro_hdls = LLVMBuildArrayAlloca(gallivm->builder, hdl_ptr_type, num_subgroup_loop, "coro_hdls");
 
    unsigned end_coroutine = INT_MAX;
 
@@ -585,22 +586,17 @@ generate_compute(struct llvmpipe_context *lp,
     * and calls the coroutine main entrypoint on the first pass, but in subsequent
     * passes it checks if the coroutine has completed and resumes it if not.
     */
-   /* take x_width - round up to type.length width */
-   lp_build_loop_begin(&loop_state[3], gallivm,
-                       lp_build_const_int32(gallivm, 0)); /* coroutine reentry loop */
-   lp_build_loop_begin(&loop_state[2], gallivm,
-                       lp_build_const_int32(gallivm, 0)); /* z loop */
    lp_build_loop_begin(&loop_state[1], gallivm,
-                       lp_build_const_int32(gallivm, 0)); /* y loop */
+                       lp_build_const_int32(gallivm, 0)); /* coroutine reentry loop */
    lp_build_loop_begin(&loop_state[0], gallivm,
-                       lp_build_const_int32(gallivm, 0)); /* x loop */
+                       lp_build_const_int32(gallivm, 0)); /* subgroup loop */
    {
       LLVMValueRef args[CS_ARG_MAX];
       args[CS_ARG_CONTEXT] = context_ptr;
       args[CS_ARG_RESOURCES] = resources_ptr;
-      args[CS_ARG_BLOCK_X_SIZE] = loop_state[0].counter;
-      args[CS_ARG_BLOCK_Y_SIZE] = loop_state[1].counter;
-      args[CS_ARG_BLOCK_Z_SIZE] = loop_state[2].counter;
+      args[CS_ARG_BLOCK_X_SIZE] = LLVMGetUndef(int32_type);
+      args[CS_ARG_BLOCK_Y_SIZE] = LLVMGetUndef(int32_type);
+      args[CS_ARG_BLOCK_Z_SIZE] = LLVMGetUndef(int32_type);
       args[CS_ARG_GRID_X] = grid_x_arg;
       args[CS_ARG_GRID_Y] = grid_y_arg;
       args[CS_ARG_GRID_Z] = grid_z_arg;
@@ -611,34 +607,25 @@ generate_compute(struct llvmpipe_context *lp,
       args[CS_ARG_DRAW_ID] = draw_id_arg;
       args[CS_ARG_VERTEX_DATA] = io_ptr;
       args[CS_ARG_PER_THREAD_DATA] = thread_data_ptr;
-      args[CS_ARG_CORO_X_LOOPS] = num_x_loop;
+      args[CS_ARG_CORO_SUBGROUP_COUNT] = num_subgroup_loop;
       args[CS_ARG_CORO_PARTIALS] = partials;
       args[CS_ARG_CORO_BLOCK_X_SIZE] = block_x_size_arg;
       args[CS_ARG_CORO_BLOCK_Y_SIZE] = block_y_size_arg;
       args[CS_ARG_CORO_BLOCK_Z_SIZE] = block_z_size_arg;
 
-      /* idx = (z * (size_x * size_y) + y * size_x + x */
-      LLVMValueRef coro_hdl_idx = LLVMBuildMul(gallivm->builder, loop_state[2].counter,
-                                               LLVMBuildMul(gallivm->builder, num_x_loop, block_y_size_arg, ""), "");
-      coro_hdl_idx = LLVMBuildAdd(gallivm->builder, coro_hdl_idx,
-                                  LLVMBuildMul(gallivm->builder, loop_state[1].counter,
-                                               num_x_loop, ""), "");
-      coro_hdl_idx = LLVMBuildAdd(gallivm->builder, coro_hdl_idx,
-                                  loop_state[0].counter, "");
-
-      args[CS_ARG_CORO_IDX] = coro_hdl_idx;
+      args[CS_ARG_CORO_IDX] = loop_state[0].counter;
 
       args[CS_ARG_CORO_MEM] = coro_mem;
 
       if (is_mesh)
          args[CS_ARG_CORO_OUTPUTS] = output_array;
 
-      LLVMValueRef coro_entry = LLVMBuildGEP2(gallivm->builder, hdl_ptr_type, coro_hdls, &coro_hdl_idx, 1, "");
+      LLVMValueRef coro_entry = LLVMBuildGEP2(gallivm->builder, hdl_ptr_type, coro_hdls, &loop_state[0].counter, 1, "");
 
       LLVMValueRef coro_hdl = LLVMBuildLoad2(gallivm->builder, hdl_ptr_type, coro_entry, "coro_hdl");
 
       struct lp_build_if_state ifstate;
-      LLVMValueRef cmp = LLVMBuildICmp(gallivm->builder, LLVMIntEQ, loop_state[3].counter,
+      LLVMValueRef cmp = LLVMBuildICmp(gallivm->builder, LLVMIntEQ, loop_state[1].counter,
                                        lp_build_const_int32(gallivm, 0), "");
       /* first time here - call the coroutine function entry point */
       lp_build_if(&ifstate, gallivm, cmp);
@@ -651,24 +638,18 @@ generate_compute(struct llvmpipe_context *lp,
       lp_build_if(&ifstate2, gallivm, coro_done);
       /* if done destroy and force loop exit */
       lp_build_coro_destroy(gallivm, coro_hdl);
-      lp_build_loop_force_set_counter(&loop_state[3], lp_build_const_int32(gallivm, end_coroutine - 1));
+      lp_build_loop_force_set_counter(&loop_state[1], lp_build_const_int32(gallivm, end_coroutine - 1));
       lp_build_else(&ifstate2);
       /* otherwise resume the coroutine */
       lp_build_coro_resume(gallivm, coro_hdl);
       lp_build_endif(&ifstate2);
       lp_build_endif(&ifstate);
-      lp_build_loop_force_reload_counter(&loop_state[3]);
+      lp_build_loop_force_reload_counter(&loop_state[1]);
    }
    lp_build_loop_end_cond(&loop_state[0],
-                          num_x_loop,
+                          num_subgroup_loop,
                           NULL,  LLVMIntUGE);
    lp_build_loop_end_cond(&loop_state[1],
-                          block_y_size_arg,
-                          NULL,  LLVMIntUGE);
-   lp_build_loop_end_cond(&loop_state[2],
-                          block_z_size_arg,
-                          NULL,  LLVMIntUGE);
-   lp_build_loop_end_cond(&loop_state[3],
                           lp_build_const_int32(gallivm, end_coroutine),
                           NULL, LLVMIntEQ);
 
@@ -680,12 +661,8 @@ generate_compute(struct llvmpipe_context *lp,
    LLVMBuildRetVoid(builder);
 
    /* This is stage (b) - generate the compute shader code inside the coroutine. */
-   LLVMValueRef x_size_arg, y_size_arg, z_size_arg;
    context_ptr  = LLVMGetParam(coro, CS_ARG_CONTEXT);
    resources_ptr = LLVMGetParam(coro, CS_ARG_RESOURCES);
-   x_size_arg = LLVMGetParam(coro, CS_ARG_BLOCK_X_SIZE);
-   y_size_arg = LLVMGetParam(coro, CS_ARG_BLOCK_Y_SIZE);
-   z_size_arg = LLVMGetParam(coro, CS_ARG_BLOCK_Z_SIZE);
    grid_x_arg = LLVMGetParam(coro, CS_ARG_GRID_X);
    grid_y_arg = LLVMGetParam(coro, CS_ARG_GRID_Y);
    grid_z_arg = LLVMGetParam(coro, CS_ARG_GRID_Z);
@@ -696,12 +673,12 @@ generate_compute(struct llvmpipe_context *lp,
    draw_id_arg = LLVMGetParam(coro, CS_ARG_DRAW_ID);
    io_ptr = LLVMGetParam(coro, CS_ARG_VERTEX_DATA);
    thread_data_ptr  = LLVMGetParam(coro, CS_ARG_PER_THREAD_DATA);
-   num_x_loop = LLVMGetParam(coro, CS_ARG_CORO_X_LOOPS);
+   num_subgroup_loop = LLVMGetParam(coro, CS_ARG_CORO_SUBGROUP_COUNT);
    partials = LLVMGetParam(coro, CS_ARG_CORO_PARTIALS);
    block_x_size_arg = LLVMGetParam(coro, CS_ARG_CORO_BLOCK_X_SIZE);
    block_y_size_arg = LLVMGetParam(coro, CS_ARG_CORO_BLOCK_Y_SIZE);
    block_z_size_arg = LLVMGetParam(coro, CS_ARG_CORO_BLOCK_Z_SIZE);
-   LLVMValueRef coro_idx = LLVMGetParam(coro, CS_ARG_CORO_IDX);
+   LLVMValueRef subgroup_id = LLVMGetParam(coro, CS_ARG_CORO_IDX);
    coro_mem = LLVMGetParam(coro, CS_ARG_CORO_MEM);
    if (is_mesh)
       output_array = LLVMGetParam(coro, CS_ARG_CORO_OUTPUTS);
@@ -730,27 +707,32 @@ generate_compute(struct llvmpipe_context *lp,
                                                   variant->jit_cs_thread_data_type,
                                                   thread_data_ptr);
 
-      LLVMValueRef coro_num_hdls = LLVMBuildMul(gallivm->builder, num_x_loop, block_y_size_arg, "");
-      coro_num_hdls = LLVMBuildMul(gallivm->builder, coro_num_hdls, block_z_size_arg, "");
-
       /* these are coroutine entrypoint necessities */
       LLVMValueRef coro_id = lp_build_coro_id(gallivm);
-      LLVMValueRef coro_entry = lp_build_coro_alloc_mem_array(gallivm, coro_mem, coro_idx, coro_num_hdls);
+      LLVMValueRef coro_entry = lp_build_coro_alloc_mem_array(gallivm, coro_mem, subgroup_id, num_subgroup_loop);
       LLVMTypeRef mem_ptr_type = LLVMInt8TypeInContext(gallivm->context);
       LLVMValueRef alloced_ptr = LLVMBuildLoad2(gallivm->builder, hdl_ptr_type, coro_mem, "");
       alloced_ptr = LLVMBuildGEP2(gallivm->builder, mem_ptr_type, alloced_ptr, &coro_entry, 1, "");
       LLVMValueRef coro_hdl = lp_build_coro_begin(gallivm, coro_id, alloced_ptr);
       LLVMValueRef has_partials = LLVMBuildICmp(gallivm->builder, LLVMIntNE, partials, lp_build_const_int32(gallivm, 0), "");
-      LLVMValueRef tids_x[LP_MAX_VECTOR_LENGTH], tids_y[LP_MAX_VECTOR_LENGTH], tids_z[LP_MAX_VECTOR_LENGTH];
-      LLVMValueRef base_val = LLVMBuildMul(gallivm->builder, x_size_arg, vec_length, "");
-      for (i = 0; i < cs_type.length; i++) {
-         tids_x[i] = LLVMBuildAdd(gallivm->builder, base_val, lp_build_const_int32(gallivm, i), "");
-         tids_y[i] = y_size_arg;
-         tids_z[i] = z_size_arg;
-      }
-      system_values.thread_id[0] = lp_build_gather_values(gallivm, tids_x, cs_type.length);
-      system_values.thread_id[1] = lp_build_gather_values(gallivm, tids_y, cs_type.length);
-      system_values.thread_id[2] = lp_build_gather_values(gallivm, tids_z, cs_type.length);
+
+      struct lp_build_context bld;
+      lp_build_context_init(&bld, gallivm, lp_uint_type(cs_type));
+
+      LLVMValueRef base_val = LLVMBuildMul(gallivm->builder, subgroup_id, vec_length, "");
+      LLVMValueRef invocation_indices[LP_MAX_VECTOR_LENGTH];
+      for (i = 0; i < cs_type.length; i++)
+         invocation_indices[i] = LLVMBuildAdd(gallivm->builder, base_val, lp_build_const_int32(gallivm, i), "");
+      LLVMValueRef invocation_index = lp_build_gather_values(gallivm, invocation_indices, cs_type.length);
+
+      LLVMValueRef block_x_size_vec = lp_build_broadcast_scalar(&bld, block_x_size_arg);
+      LLVMValueRef block_y_size_vec = lp_build_broadcast_scalar(&bld, block_y_size_arg);
+
+      system_values.thread_id[0] = LLVMBuildURem(gallivm->builder, invocation_index, block_x_size_vec, "");
+      system_values.thread_id[1] = LLVMBuildUDiv(gallivm->builder, invocation_index, block_x_size_vec, "");
+      system_values.thread_id[1] = LLVMBuildURem(gallivm->builder, system_values.thread_id[1], block_y_size_vec, "");
+      system_values.thread_id[2] = LLVMBuildUDiv(gallivm->builder, invocation_index, block_x_size_vec, "");
+      system_values.thread_id[2] = LLVMBuildUDiv(gallivm->builder, system_values.thread_id[2], block_y_size_vec, "");
 
       system_values.block_id[0] = grid_x_arg;
       system_values.block_id[1] = grid_y_arg;
@@ -763,38 +745,15 @@ generate_compute(struct llvmpipe_context *lp,
       system_values.work_dim = work_dim_arg;
       system_values.draw_id = draw_id_arg;
 
-      /* subgroup_id = ((z * block_size_x * block_size_y) + (y * block_size_x) + x) / subgroup_size
-       *
-       * this breaks if z or y is zero, so distribute the division to preserve ids
-       *
-       * subgroup_id = ((z * block_size_x * block_size_y) / subgroup_size) + ((y * block_size_x) / subgroup_size) + (x / subgroup_size)
-       *
-       * except "x" is pre-divided here
-       *
-       * subgroup_id = ((z * block_size_x * block_size_y) / subgroup_size) + ((y * block_size_x) / subgroup_size) + x
-       */
-      LLVMValueRef subgroup_id = LLVMBuildUDiv(builder,
-                                               LLVMBuildMul(gallivm->builder, z_size_arg, LLVMBuildMul(gallivm->builder, block_x_size_arg, block_y_size_arg, ""), ""),
-                                               vec_length, "");
-      subgroup_id = LLVMBuildAdd(gallivm->builder,
-                                 subgroup_id,
-                                 LLVMBuildUDiv(builder, LLVMBuildMul(gallivm->builder, y_size_arg, block_x_size_arg, ""), vec_length, ""),
-                                 "");
-      subgroup_id = LLVMBuildAdd(gallivm->builder, subgroup_id, x_size_arg, "");
       system_values.subgroup_id = subgroup_id;
-      LLVMValueRef num_subgroups = LLVMBuildUDiv(builder,
-                                                 LLVMBuildMul(builder, block_x_size_arg,
-                                                              LLVMBuildMul(builder, block_y_size_arg, block_z_size_arg, ""), ""),
-                                                 vec_length, "");
-      LLVMValueRef subgroup_cmp = LLVMBuildICmp(gallivm->builder, LLVMIntEQ, num_subgroups, lp_build_const_int32(gallivm, 0), "");
-      system_values.num_subgroups = LLVMBuildSelect(builder, subgroup_cmp, lp_build_const_int32(gallivm, 1), num_subgroups, "");
+      system_values.num_subgroups = num_subgroup_loop;
 
       system_values.block_size[0] = block_x_size_arg;
       system_values.block_size[1] = block_y_size_arg;
       system_values.block_size[2] = block_z_size_arg;
 
-      LLVMValueRef last_x_loop = LLVMBuildICmp(gallivm->builder, LLVMIntEQ, x_size_arg, LLVMBuildSub(gallivm->builder, num_x_loop, lp_build_const_int32(gallivm, 1), ""), "");
-      LLVMValueRef use_partial_mask = LLVMBuildAnd(gallivm->builder, last_x_loop, has_partials, "");
+      LLVMValueRef last_loop = LLVMBuildICmp(gallivm->builder, LLVMIntEQ, subgroup_id, LLVMBuildSub(gallivm->builder, num_subgroup_loop, lp_build_const_int32(gallivm, 1), ""), "");
+      LLVMValueRef use_partial_mask = LLVMBuildAnd(gallivm->builder, last_loop, has_partials, "");
       struct lp_build_if_state if_state;
       LLVMTypeRef mask_type = LLVMVectorType(int32_type, cs_type.length);
       LLVMValueRef mask_val = lp_build_alloca(gallivm, mask_type, "mask");
@@ -866,7 +825,7 @@ generate_compute(struct llvmpipe_context *lp,
                                                         lp_int_type(cs_type), 0);
 
          struct lp_build_if_state iter0state;
-         LLVMValueRef is_iter0 = LLVMBuildICmp(gallivm->builder, LLVMIntEQ, coro_idx,
+         LLVMValueRef is_iter0 = LLVMBuildICmp(gallivm->builder, LLVMIntEQ, subgroup_id,
                                                lp_build_const_int32(gallivm, 0), "");
          LLVMValueRef vertex_count = LLVMBuildLoad2(gallivm->builder, i32t, mesh_iface.vertex_count, "");
          LLVMValueRef prim_count = LLVMBuildLoad2(gallivm->builder, i32t, mesh_iface.prim_count, "");
diff --git a/src/gallium/drivers/nouveau/ci/gitlab-ci-inc.yml b/src/gallium/drivers/nouveau/ci/gitlab-ci-inc.yml
index a2fb4821bc4..5d041b61282 100644
--- a/src/gallium/drivers/nouveau/ci/gitlab-ci-inc.yml
+++ b/src/gallium/drivers/nouveau/ci/gitlab-ci-inc.yml
@@ -70,20 +70,3 @@
   tags:
     - google-nouveau-jetson-nano
 
-# Single Jetson Nano board at anholt's house.
-.gm20b-gles-full:
-  extends:
-    - .gm20b-test
-    - .nouveau-manual-rules
-  timeout: 2h
-  variables:
-    HWCI_TEST_SCRIPT: "/install/deqp-runner.sh"
-    DEQP_SUITE: nouveau-gm20b
-    TEST_PHASE_TIMEOUT: 120
-
-.gm20b-gles:
-  extends:
-    - .gm20b-gles-full
-  timeout: 30m
-  variables:
-    DEQP_FRACTION: 10
diff --git a/src/gallium/drivers/nouveau/ci/gitlab-ci.yml b/src/gallium/drivers/nouveau/ci/gitlab-ci.yml
index 90ee2b58d82..673d77eaee2 100644
--- a/src/gallium/drivers/nouveau/ci/gitlab-ci.yml
+++ b/src/gallium/drivers/nouveau/ci/gitlab-ci.yml
@@ -12,3 +12,21 @@ gk20a-gles:
     DEQP_SUITE: nouveau-gk20a
     FARM : anholt
     DEVICE_TYPE: anholt-jetson
+
+# Single Jetson Nano board at anholt's house.
+.gm20b-gles-full:
+  extends:
+    - .gm20b-test
+    - .nouveau-manual-rules
+  timeout: 2h
+  variables:
+    HWCI_TEST_SCRIPT: "/install/deqp-runner.sh"
+    DEQP_SUITE: nouveau-gm20b
+    TEST_PHASE_TIMEOUT: 120
+
+.gm20b-gles:
+  extends:
+    - .gm20b-gles-full
+  timeout: 30m
+  variables:
+    DEQP_FRACTION: 10
diff --git a/src/gallium/drivers/nouveau/nouveau_screen.c b/src/gallium/drivers/nouveau/nouveau_screen.c
index 22924204514..e03ab87790d 100644
--- a/src/gallium/drivers/nouveau/nouveau_screen.c
+++ b/src/gallium/drivers/nouveau/nouveau_screen.c
@@ -80,7 +80,8 @@ nouveau_screen_fence_ref(struct pipe_screen *pscreen,
                          struct pipe_fence_handle **ptr,
                          struct pipe_fence_handle *pfence)
 {
-   nouveau_fence_ref(nouveau_fence(pfence), (struct nouveau_fence **)ptr);
+   nouveau_fence_ref((pfence ? nouveau_fence(pfence) : NULL),
+                     (ptr ? (struct nouveau_fence **)ptr : NULL));
 }
 
 static bool
@@ -291,6 +292,8 @@ nouveau_screen_init(struct nouveau_screen *screen, struct nouveau_device *dev)
    void *data;
    union nouveau_bo_config mm_config;
 
+   glsl_type_singleton_init_or_ref();
+
    char *nv_dbg = getenv("NOUVEAU_MESA_DEBUG");
    if (nv_dbg)
       nouveau_mesa_debug = atoi(nv_dbg);
@@ -442,8 +445,6 @@ nouveau_screen_init(struct nouveau_screen *screen, struct nouveau_device *dev)
                                        &mm_config);
    screen->mm_VRAM = nouveau_mm_create(dev, NOUVEAU_BO_VRAM, &mm_config);
 
-   glsl_type_singleton_init_or_ref();
-
    return 0;
 
 err:
diff --git a/src/gallium/drivers/panfrost/pan_cmdstream.c b/src/gallium/drivers/panfrost/pan_cmdstream.c
index 2930a546d51..3565588f113 100644
--- a/src/gallium/drivers/panfrost/pan_cmdstream.c
+++ b/src/gallium/drivers/panfrost/pan_cmdstream.c
@@ -622,6 +622,7 @@ panfrost_emit_frag_shader_meta(struct panfrost_batch *batch)
    struct panfrost_compiled_shader *ss = ctx->prog[PIPE_SHADER_FRAGMENT];
 
    panfrost_batch_add_bo(batch, ss->bin.bo, PIPE_SHADER_FRAGMENT);
+   panfrost_batch_add_bo(batch, ss->state.bo, PIPE_SHADER_FRAGMENT);
 
    struct panfrost_ptr xfer;
 
@@ -1380,7 +1381,8 @@ panfrost_emit_const_buf(struct panfrost_batch *batch,
             PAN_SYSVAL_TYPE(ss->sysvals.sysvals[sysval_idx]);
          mali_ptr ptr = push_transfer.gpu + (4 * i);
 
-         if (sysval_type == PAN_SYSVAL_NUM_WORK_GROUPS)
+         if (sysval_type == PAN_SYSVAL_NUM_WORK_GROUPS &&
+             sysval_comp < ARRAY_SIZE(batch->num_wg_sysval))
             batch->num_wg_sysval[sysval_comp] = ptr;
       }
       /* Map the UBO, this should be cheap. For some buffers this may
@@ -2495,6 +2497,8 @@ panfrost_initialize_surface(struct panfrost_batch *batch,
    if (surf) {
       struct panfrost_resource *rsrc = pan_resource(surf->texture);
       BITSET_SET(rsrc->valid.data, surf->u.tex.level);
+      if (rsrc->separate_stencil)
+         BITSET_SET(rsrc->separate_stencil->valid.data, surf->u.tex.level);
    }
 }
 
@@ -3021,6 +3025,11 @@ panfrost_launch_grid_on_batch(struct pipe_context *pipe,
    mali_ptr saved_tls = batch->tls.gpu;
    batch->tls.gpu = panfrost_emit_shared_memory(batch, info);
 
+   /* if indirect, mark the indirect buffer as being read */
+   if (info->indirect)
+      panfrost_batch_read_rsrc(batch, pan_resource(info->indirect), PIPE_SHADER_COMPUTE);
+
+   /* launch it */
    JOBX(launch_grid)(batch, info);
    batch->compute_count++;
    batch->tls.gpu = saved_tls;
diff --git a/src/gallium/drivers/panfrost/pan_job.c b/src/gallium/drivers/panfrost/pan_job.c
index 26f9be55060..c1baacc034d 100644
--- a/src/gallium/drivers/panfrost/pan_job.c
+++ b/src/gallium/drivers/panfrost/pan_job.c
@@ -635,16 +635,19 @@ panfrost_batch_submit(struct panfrost_context *ctx,
       struct pipe_surface *surf = batch->key.zsbuf;
       struct panfrost_resource *z_rsrc = pan_resource(surf->texture);
 
-      /* Shared depth/stencil resources are not supported, and would
-       * break this optimisation. */
-      assert(!(z_rsrc->base.bind & PAN_BIND_SHARED_MASK));
-
-      if (batch->clear & PIPE_CLEAR_STENCIL) {
-         z_rsrc->stencil_value = batch->clear_stencil;
-         z_rsrc->constant_stencil = true;
-      } else if (z_rsrc->constant_stencil) {
-         batch->clear_stencil = z_rsrc->stencil_value;
-         batch->clear |= PIPE_CLEAR_STENCIL;
+      /* if there are multiple levels or layers, we optimize only the first */
+      if (surf->u.tex.level == 0 && surf->u.tex.first_layer == 0) {
+         /* Shared depth/stencil resources are not supported, and would
+          * break this optimisation. */
+         assert(!(z_rsrc->base.bind & PAN_BIND_SHARED_MASK));
+
+         if (batch->clear & PIPE_CLEAR_STENCIL) {
+            z_rsrc->stencil_value = batch->clear_stencil;
+            z_rsrc->constant_stencil = true;
+         } else if (z_rsrc->constant_stencil) {
+            batch->clear_stencil = z_rsrc->stencil_value;
+            batch->clear |= PIPE_CLEAR_STENCIL;
+         }
       }
 
       if (batch->draws & PIPE_CLEAR_STENCIL)
diff --git a/src/gallium/drivers/panfrost/pan_resource.c b/src/gallium/drivers/panfrost/pan_resource.c
index aa2974eb5eb..7bf25155e51 100644
--- a/src/gallium/drivers/panfrost/pan_resource.c
+++ b/src/gallium/drivers/panfrost/pan_resource.c
@@ -1181,7 +1181,7 @@ panfrost_ptr_map(struct pipe_context *pctx, struct pipe_resource *resource,
    bool create_new_bo = usage & PIPE_MAP_DISCARD_WHOLE_RESOURCE;
    bool copy_resource = false;
 
-   if (!create_new_bo && !(usage & PIPE_MAP_UNSYNCHRONIZED) &&
+   if (!(usage & PIPE_MAP_UNSYNCHRONIZED) &&
        !(resource->flags & PIPE_RESOURCE_FLAG_MAP_PERSISTENT) &&
        (usage & PIPE_MAP_WRITE) && panfrost_any_batch_reads_rsrc(ctx, rsrc)) {
       /* When a resource to be modified is already being used by a
diff --git a/src/gallium/drivers/panfrost/pan_screen.c b/src/gallium/drivers/panfrost/pan_screen.c
index 1a3497b1b88..9ddce2b7e82 100644
--- a/src/gallium/drivers/panfrost/pan_screen.c
+++ b/src/gallium/drivers/panfrost/pan_screen.c
@@ -301,9 +301,7 @@ panfrost_get_param(struct pipe_screen *screen, enum pipe_cap param)
       return 4;
 
    case PIPE_CAP_MAX_VARYINGS:
-      /* Return the GLSL maximum. The internal maximum
-       * PAN_MAX_VARYINGS accommodates internal varyings. */
-      return MAX_VARYING;
+      return dev->arch >= 9 ? 16 : 32;
 
    /* Removed in v6 (Bifrost) */
    case PIPE_CAP_GL_CLAMP:
diff --git a/src/gallium/drivers/panfrost/pan_shader.c b/src/gallium/drivers/panfrost/pan_shader.c
index 27716bd64c1..fc6e6d546dd 100644
--- a/src/gallium/drivers/panfrost/pan_shader.c
+++ b/src/gallium/drivers/panfrost/pan_shader.c
@@ -383,10 +383,6 @@ panfrost_create_shader_state(struct pipe_context *pctx,
    struct panfrost_context *ctx = pan_context(pctx);
 
    if (so->nir->xfb_info) {
-      nir_shader *xfb = nir_shader_clone(NULL, so->nir);
-      xfb->info.name = ralloc_asprintf(xfb, "%s@xfb", xfb->info.name);
-      xfb->info.internal = true;
-
       so->xfb = calloc(1, sizeof(struct panfrost_compiled_shader));
       so->xfb->key.vs_is_xfb = true;
 
diff --git a/src/gallium/drivers/r300/ci/r300-r480-fails.txt b/src/gallium/drivers/r300/ci/r300-r480-fails.txt
index d092bbe3b38..a18f099845b 100644
--- a/src/gallium/drivers/r300/ci/r300-r480-fails.txt
+++ b/src/gallium/drivers/r300/ci/r300-r480-fails.txt
@@ -3,7 +3,6 @@ dEQP-GLES2.functional.clipping.line.wide_line_clip_viewport_corner,Fail
 dEQP-GLES2.functional.clipping.point.wide_point_clip,Fail
 dEQP-GLES2.functional.clipping.point.wide_point_clip_viewport_center,Fail
 dEQP-GLES2.functional.clipping.point.wide_point_clip_viewport_corner,Fail
-dEQP-GLES2.functional.draw.draw_elements.indices.user_ptr.index_byte,Fail
 dEQP-GLES2.functional.fbo.completeness.renderable.texture.color0.rgb_half_float_oes,Fail
 dEQP-GLES2.functional.fbo.render.repeated_clear.tex2d_rgb,Fail
 dEQP-GLES2.functional.fbo.render.repeated_clear.tex2d_rgba,Fail
diff --git a/src/gallium/drivers/r300/ci/r300-rv370-fails.txt b/src/gallium/drivers/r300/ci/r300-rv370-fails.txt
index 0c43c21baff..cd0b1b7b85d 100644
--- a/src/gallium/drivers/r300/ci/r300-rv370-fails.txt
+++ b/src/gallium/drivers/r300/ci/r300-rv370-fails.txt
@@ -3,7 +3,6 @@ dEQP-GLES2.functional.clipping.line.wide_line_clip_viewport_corner,Fail
 dEQP-GLES2.functional.clipping.point.wide_point_clip,Fail
 dEQP-GLES2.functional.clipping.point.wide_point_clip_viewport_center,Fail
 dEQP-GLES2.functional.clipping.point.wide_point_clip_viewport_corner,Fail
-dEQP-GLES2.functional.draw.draw_elements.indices.user_ptr.index_byte,Fail
 dEQP-GLES2.functional.fbo.completeness.renderable.texture.color0.rgb_half_float_oes,Fail
 dEQP-GLES2.functional.fbo.completeness.renderable.texture.color0.rgba_half_float_oes,Fail
 dEQP-GLES2.functional.fbo.render.repeated_clear.tex2d_rgb,Fail
diff --git a/src/gallium/drivers/r300/ci/r300-rv530-nohiz-fails.txt b/src/gallium/drivers/r300/ci/r300-rv530-nohiz-fails.txt
index 07fa9e6435b..21a8043dcb8 100644
--- a/src/gallium/drivers/r300/ci/r300-rv530-nohiz-fails.txt
+++ b/src/gallium/drivers/r300/ci/r300-rv530-nohiz-fails.txt
@@ -5,7 +5,6 @@ dEQP-GLES2.functional.clipping.point.wide_point_clip_viewport_corner,Fail
 dEQP-GLES2.functional.clipping.line.wide_line_clip_viewport_center,Fail
 dEQP-GLES2.functional.clipping.line.wide_line_clip_viewport_corner,Fail
 
-dEQP-GLES2.functional.draw.draw_elements.indices.user_ptr.index_byte,Fail
 
 # "Framebuffer checked as complete, expected incomplete"
 dEQP-GLES2.functional.fbo.completeness.renderable.texture.color0.rgb_half_float_oes,Fail
diff --git a/src/gallium/drivers/r300/compiler/r300_nir.c b/src/gallium/drivers/r300/compiler/r300_nir.c
index 1fc240fa7b5..d87e6c165c2 100644
--- a/src/gallium/drivers/r300/compiler/r300_nir.c
+++ b/src/gallium/drivers/r300/compiler/r300_nir.c
@@ -80,6 +80,7 @@ r300_optimize_nir(struct nir_shader *s, struct pipe_screen *screen)
       NIR_PASS_V(s, nir_lower_vars_to_ssa);
 
       NIR_PASS(progress, s, nir_copy_prop);
+      NIR_PASS(progress, s, r300_nir_lower_flrp);
       NIR_PASS(progress, s, nir_opt_algebraic);
       if (s->info.stage == MESA_SHADER_VERTEX) {
          if (!is_r500)
diff --git a/src/gallium/drivers/r300/compiler/r3xx_vertprog.c b/src/gallium/drivers/r300/compiler/r3xx_vertprog.c
index a02147a8244..00b9928164a 100644
--- a/src/gallium/drivers/r300/compiler/r3xx_vertprog.c
+++ b/src/gallium/drivers/r300/compiler/r3xx_vertprog.c
@@ -689,6 +689,7 @@ static void allocate_temporary_registers(struct radeon_compiler *c, void *user)
 
 	if (!ra_allocate(graph)) {
 		rc_error(c, "Ran out of hardware temporaries\n");
+                ralloc_free(graph);
 		return;
 	}
 
diff --git a/src/gallium/drivers/r300/compiler/r500_nir_lower_fcsel.c b/src/gallium/drivers/r300/compiler/r500_nir_lower_fcsel.c
index ac280870850..2f3ccb48c19 100644
--- a/src/gallium/drivers/r300/compiler/r500_nir_lower_fcsel.c
+++ b/src/gallium/drivers/r300/compiler/r500_nir_lower_fcsel.c
@@ -1,3 +1,8 @@
+/*
+ * Copyright Pavel Ondračka <pavel.ondracka@gmail.com>
+ * SPDX-License-Identifier: MIT
+ */
+
 #include <stdbool.h>
 #include "r300_nir.h"
 #include "nir_builder.h"
diff --git a/src/gallium/drivers/r300/compiler/radeon_pair_regalloc.c b/src/gallium/drivers/r300/compiler/radeon_pair_regalloc.c
index e7cabfa9d24..5f947678453 100644
--- a/src/gallium/drivers/r300/compiler/radeon_pair_regalloc.c
+++ b/src/gallium/drivers/r300/compiler/radeon_pair_regalloc.c
@@ -357,6 +357,7 @@ static void do_advanced_regalloc(struct regalloc_state * s)
 
 	if (!ra_allocate(graph)) {
 		rc_error(s->C, "Ran out of hardware temporaries\n");
+                ralloc_free(graph);
 		return;
 	}
 
diff --git a/src/gallium/drivers/r300/r300_context.c b/src/gallium/drivers/r300/r300_context.c
index edad0071a5a..433f669b58a 100644
--- a/src/gallium/drivers/r300/r300_context.c
+++ b/src/gallium/drivers/r300/r300_context.c
@@ -86,10 +86,15 @@ static void r300_destroy_context(struct pipe_context* context)
     if (r300->draw)
         draw_destroy(r300->draw);
 
+    for (unsigned i = 0; i < r300->nr_vertex_buffers; i++)
+       pipe_vertex_buffer_unreference(&r300->vertex_buffer[i]);
+
     if (r300->uploader)
         u_upload_destroy(r300->uploader);
     if (r300->context.stream_uploader)
         u_upload_destroy(r300->context.stream_uploader);
+    if (r300->context.const_uploader)
+       u_upload_destroy(r300->context.const_uploader);
 
     /* XXX: This function assumes r300->query_list was initialized */
     r300_release_referenced_objects(r300);
@@ -99,6 +104,7 @@ static void r300_destroy_context(struct pipe_context* context)
         r300->rws->ctx_destroy(r300->ctx);
 
     rc_destroy_regalloc_state(&r300->fs_regalloc_state);
+    rc_destroy_regalloc_state(&r300->vs_regalloc_state);
 
     /* XXX: No way to tell if this was initialized or not? */
     slab_destroy_child(&r300->pool_transfers);
@@ -125,6 +131,9 @@ static void r300_destroy_context(struct pipe_context* context)
             FREE(r300->vertex_stream_state.state);
         }
     }
+
+    FREE(r300->stencilref_fallback);
+
     FREE(r300);
 }
 
diff --git a/src/gallium/drivers/r300/r300_context.h b/src/gallium/drivers/r300/r300_context.h
index d7a784ac083..ff0962e835e 100644
--- a/src/gallium/drivers/r300/r300_context.h
+++ b/src/gallium/drivers/r300/r300_context.h
@@ -744,7 +744,8 @@ void r300_translate_index_buffer(struct r300_context *r300,
                                  const struct pipe_draw_info *info,
                                  struct pipe_resource **out_index_buffer,
                                  unsigned *index_size, unsigned index_offset,
-                                 unsigned *start, unsigned count);
+                                 unsigned *start, unsigned count,
+                                 const uint8_t **export_ptr);
 
 /* r300_render_stencilref.c */
 void r300_plug_in_stencil_ref_fallback(struct r300_context *r300);
diff --git a/src/gallium/drivers/r300/r300_fs.c b/src/gallium/drivers/r300/r300_fs.c
index 5300c8bf047..3179e77ce71 100644
--- a/src/gallium/drivers/r300/r300_fs.c
+++ b/src/gallium/drivers/r300/r300_fs.c
@@ -525,6 +525,8 @@ static void r300_translate_fragment_shader(
             abort();
         }
 
+        free(compiler.code->constants.Constants);
+        free(compiler.code->constants_remap_table);
         rc_destroy(&compiler.Base);
         r300_dummy_fragment_shader(r300, shader);
         return;
diff --git a/src/gallium/drivers/r300/r300_public.h b/src/gallium/drivers/r300/r300_public.h
index bf54cc4dfc0..89e4a9e9cac 100644
--- a/src/gallium/drivers/r300/r300_public.h
+++ b/src/gallium/drivers/r300/r300_public.h
@@ -1,4 +1,4 @@
-
+// SPDX-License-Identifier: MIT
 #ifndef R300_PUBLIC_H
 #define R300_PUBLIC_H
 
diff --git a/src/gallium/drivers/r300/r300_render.c b/src/gallium/drivers/r300/r300_render.c
index 858d1798b48..978d7d6c59d 100644
--- a/src/gallium/drivers/r300/r300_render.c
+++ b/src/gallium/drivers/r300/r300_render.c
@@ -601,6 +601,7 @@ static void r300_draw_elements(struct r300_context *r300,
     unsigned short_count;
     int buffer_offset = 0, index_offset = 0; /* for index bias emulation */
     uint16_t indices3[3];
+    const uint8_t *local_ptr = info->index.user;
 
     if (draw->index_bias && !r300->screen->caps.is_r500) {
         r300_split_index_bias(r300, draw->index_bias, &buffer_offset,
@@ -608,7 +609,7 @@ static void r300_draw_elements(struct r300_context *r300,
     }
 
     r300_translate_index_buffer(r300, info, &indexBuffer,
-                                &indexSize, index_offset, &start, count);
+                                &indexSize, index_offset, &start, count, &local_ptr);
 
     /* Fallback for misaligned ushort indices. */
     if (indexSize == 2 && (start & 1) && indexBuffer) {
@@ -628,10 +629,18 @@ static void r300_draw_elements(struct r300_context *r300,
                                      count, (uint8_t*)ptr);
         }
     } else {
-        if (info->has_user_indices)
-            r300_upload_index_buffer(r300, &indexBuffer, indexSize,
+        if (info->has_user_indices) {
+           struct pipe_resource* indexSaved = indexBuffer;
+
+           if (local_ptr != info->index.user)
+              start = 0;
+
+           r300_upload_index_buffer(r300, &indexBuffer, indexSize,
                                      &start, count,
-                                     info->index.user);
+                                     local_ptr);
+
+           pipe_resource_reference(&indexSaved, NULL);
+        }
     }
 
     /* 19 dwords for emit_draw_elements. Give up if the function fails. */
diff --git a/src/gallium/drivers/r300/r300_render_translate.c b/src/gallium/drivers/r300/r300_render_translate.c
index f3749815773..32d6a2ec5a9 100644
--- a/src/gallium/drivers/r300/r300_render_translate.c
+++ b/src/gallium/drivers/r300/r300_render_translate.c
@@ -29,20 +29,21 @@ void r300_translate_index_buffer(struct r300_context *r300,
                                  const struct pipe_draw_info *info,
                                  struct pipe_resource **out_buffer,
                                  unsigned *index_size, unsigned index_offset,
-                                 unsigned *start, unsigned count)
+                                 unsigned *start, unsigned count,
+                                 const uint8_t **export_ptr)
 {
     unsigned out_offset;
-    void *ptr;
+    void **ptr = (void **)export_ptr;
 
     switch (*index_size) {
     case 1:
         *out_buffer = NULL;
         u_upload_alloc(r300->uploader, 0, count * 2, 4,
-                       &out_offset, out_buffer, &ptr);
+                       &out_offset, out_buffer, ptr);
 
         util_shorten_ubyte_elts_to_userptr(
                 &r300->context, info, PIPE_MAP_UNSYNCHRONIZED, index_offset,
-                *start, count, ptr);
+                *start, count, *ptr);
 
         *index_size = 2;
         *start = out_offset / 2;
@@ -52,12 +53,12 @@ void r300_translate_index_buffer(struct r300_context *r300,
         if (index_offset) {
             *out_buffer = NULL;
             u_upload_alloc(r300->uploader, 0, count * 2, 4,
-                           &out_offset, out_buffer, &ptr);
+                           &out_offset, out_buffer, ptr);
 
             util_rebuild_ushort_elts_to_userptr(&r300->context, info,
                                                 PIPE_MAP_UNSYNCHRONIZED,
                                                 index_offset, *start,
-                                                count, ptr);
+                                                count, *ptr);
 
             *start = out_offset / 2;
         }
@@ -67,12 +68,12 @@ void r300_translate_index_buffer(struct r300_context *r300,
         if (index_offset) {
             *out_buffer = NULL;
             u_upload_alloc(r300->uploader, 0, count * 4, 4,
-                           &out_offset, out_buffer, &ptr);
+                           &out_offset, out_buffer, ptr);
 
             util_rebuild_uint_elts_to_userptr(&r300->context, info,
                                               PIPE_MAP_UNSYNCHRONIZED,
                                               index_offset, *start,
-                                              count, ptr);
+                                              count, *ptr);
 
             *start = out_offset / 4;
         }
diff --git a/src/gallium/drivers/r300/r300_state.c b/src/gallium/drivers/r300/r300_state.c
index a0177280444..ff825c796a2 100644
--- a/src/gallium/drivers/r300/r300_state.c
+++ b/src/gallium/drivers/r300/r300_state.c
@@ -1119,6 +1119,8 @@ static void r300_delete_fs_state(struct pipe_context* pipe, void* shader)
     struct r300_fragment_shader* fs = (struct r300_fragment_shader*)shader;
     struct r300_fragment_shader_code *tmp, *ptr = fs->first;
 
+    free(fs->shader->code.constants_remap_table);
+
     while (ptr) {
         tmp = ptr;
         ptr = ptr->next;
diff --git a/src/gallium/drivers/r300/r300_texture.c b/src/gallium/drivers/r300/r300_texture.c
index 23b38490d45..5bdeca72af9 100644
--- a/src/gallium/drivers/r300/r300_texture.c
+++ b/src/gallium/drivers/r300/r300_texture.c
@@ -1223,7 +1223,8 @@ struct pipe_surface* r300_create_surface_custom(struct pipe_context * ctx,
                                                tex->b.nr_samples,
                                                tex->tex.microtile,
                                                tex->tex.macrotile[level],
-                                               DIM_HEIGHT, 0);
+                                               DIM_HEIGHT, 0,
+                                               tex->b.bind & PIPE_BIND_SCANOUT);
 
         surface->cbzb_height = align((surface->base.height + 1) / 2,
                                      tile_height);
diff --git a/src/gallium/drivers/r300/r300_texture_desc.c b/src/gallium/drivers/r300/r300_texture_desc.c
index 7b1b2e21d2a..d8ebcad3a48 100644
--- a/src/gallium/drivers/r300/r300_texture_desc.c
+++ b/src/gallium/drivers/r300/r300_texture_desc.c
@@ -33,7 +33,8 @@ unsigned r300_get_pixel_alignment(enum pipe_format format,
                                   unsigned num_samples,
                                   enum radeon_bo_layout microtile,
                                   enum radeon_bo_layout macrotile,
-                                  enum r300_dim dim, bool is_rs690)
+                                  enum r300_dim dim, bool is_rs690,
+                                  bool scanout)
 {
     static const unsigned table[2][5][3][2] =
     {
@@ -75,6 +76,13 @@ unsigned r300_get_pixel_alignment(enum pipe_format format,
             tile = align;
     }
 
+    if (scanout) {
+        if (microtile || macrotile)
+           tile = MAX2(tile, 256 / pixsize);
+        else
+           tile = MAX2(tile, 64);
+    }
+
     assert(tile);
     return tile;
 }
@@ -92,7 +100,9 @@ static bool r300_texture_macro_switch(struct r300_resource *tex,
     }
 
     tile = r300_get_pixel_alignment(tex->b.format, tex->b.nr_samples,
-                                    tex->tex.microtile, RADEON_LAYOUT_TILED, dim, 0);
+                                    tex->tex.microtile, RADEON_LAYOUT_TILED, dim, 0,
+                                    tex->b.bind & PIPE_BIND_SCANOUT);
+
     if (dim == DIM_WIDTH) {
         texdim = u_minify(tex->tex.width0, level);
     } else {
@@ -133,11 +143,16 @@ static unsigned r300_texture_get_stride(struct r300_screen *screen,
     width = u_minify(tex->tex.width0, level);
 
     if (util_format_is_plain(tex->b.format)) {
+        /* MSAA and mipmapping are incompatible with scanout. */
+        assert(!(tex->b.bind & PIPE_BIND_SCANOUT) ||
+               (tex->b.last_level == 0 && tex->b.nr_samples <= 1));
+
         tile_width = r300_get_pixel_alignment(tex->b.format,
                                               tex->b.nr_samples,
                                               tex->tex.microtile,
                                               tex->tex.macrotile[level],
-                                              DIM_WIDTH, is_rs690);
+                                              DIM_WIDTH, is_rs690,
+                                              tex->b.bind & PIPE_BIND_SCANOUT);
         width = align(width, tile_width);
 
         stride = util_format_get_stride(tex->b.format, width);
@@ -169,7 +184,8 @@ static unsigned r300_texture_get_nblocksy(struct r300_resource *tex,
                                                tex->b.nr_samples,
                                                tex->tex.microtile,
                                                tex->tex.macrotile[level],
-                                               DIM_HEIGHT, 0);
+                                               DIM_HEIGHT, 0,
+                                               tex->b.bind & PIPE_BIND_SCANOUT);
         height = align(height, tile_height);
 
         /* See if the CBZB clear can be used on the buffer,
@@ -259,6 +275,11 @@ static void r300_setup_miptree(struct r300_screen *screen,
         tex->tex.stride_in_bytes[i] = stride;
         tex->tex.cbzb_allowed[i] = tex->tex.cbzb_allowed[i] && aligned_for_cbzb;
 
+        if (tex->b.bind & PIPE_BIND_SCANOUT) {
+           assert(i == 0);
+           tex->tex.stride_in_bytes_override = stride;
+        }
+
         SCREEN_DBG(screen, DBG_TEXALLOC, "r300: Texture miptree: Level %d "
                 "(%dx%dx%d px, pitch %d bytes) %d bytes total, macrotiled %s\n",
                 i, u_minify(tex->tex.width0, i), u_minify(tex->tex.height0, i),
diff --git a/src/gallium/drivers/r300/r300_texture_desc.h b/src/gallium/drivers/r300/r300_texture_desc.h
index 767335c1b90..45b7219acda 100644
--- a/src/gallium/drivers/r300/r300_texture_desc.h
+++ b/src/gallium/drivers/r300/r300_texture_desc.h
@@ -41,7 +41,8 @@ unsigned r300_get_pixel_alignment(enum pipe_format format,
                                   unsigned num_samples,
                                   enum radeon_bo_layout microtile,
                                   enum radeon_bo_layout macrotile,
-                                  enum r300_dim dim, bool is_rs690);
+                                  enum r300_dim dim, bool is_rs690,
+                                  bool scanout);
 
 void r300_texture_desc_init(struct r300_screen *rscreen,
                             struct r300_resource *tex,
diff --git a/src/gallium/drivers/r600/evergreen_state.c b/src/gallium/drivers/r600/evergreen_state.c
index fe6cb10b1e9..511a74e3dd4 100644
--- a/src/gallium/drivers/r600/evergreen_state.c
+++ b/src/gallium/drivers/r600/evergreen_state.c
@@ -2137,7 +2137,8 @@ static void evergreen_emit_vertex_buffers(struct r600_context *rctx,
 {
 	struct radeon_cmdbuf *cs = &rctx->b.gfx.cs;
 	struct r600_fetch_shader *shader = (struct r600_fetch_shader*)rctx->vertex_fetch_shader.cso;
-	uint32_t dirty_mask = state->dirty_mask & shader->buffer_mask;
+	uint32_t buffer_mask = shader ? shader->buffer_mask : ~0;
+	uint32_t dirty_mask = state->dirty_mask & buffer_mask;
 
 	while (dirty_mask) {
 		struct pipe_vertex_buffer *vb;
@@ -2176,7 +2177,7 @@ static void evergreen_emit_vertex_buffers(struct r600_context *rctx,
 		radeon_emit(cs, radeon_add_to_buffer_list(&rctx->b, &rctx->b.gfx, rbuffer,
 						      RADEON_USAGE_READ | RADEON_PRIO_VERTEX_BUFFER));
 	}
-	state->dirty_mask &= ~shader->buffer_mask;
+	state->dirty_mask &= ~buffer_mask;
 }
 
 static void evergreen_fs_emit_vertex_buffers(struct r600_context *rctx, struct r600_atom * atom)
diff --git a/src/gallium/drivers/r600/r600_formats.h b/src/gallium/drivers/r600/r600_formats.h
index f36e5534f29..6ce342c0a80 100644
--- a/src/gallium/drivers/r600/r600_formats.h
+++ b/src/gallium/drivers/r600/r600_formats.h
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: MIT
 #ifndef R600_FORMATS_H
 #define R600_FORMATS_H
 
diff --git a/src/gallium/drivers/r600/r600_opcodes.h b/src/gallium/drivers/r600/r600_opcodes.h
index 93dda44283a..6c294183480 100644
--- a/src/gallium/drivers/r600/r600_opcodes.h
+++ b/src/gallium/drivers/r600/r600_opcodes.h
@@ -1,4 +1,4 @@
-
+// SPDX-License-Identifier: MIT
 #ifndef R600_OPCODES_H
 #define R600_OPCODES_H
 
diff --git a/src/gallium/drivers/r600/sfn/sfn_instr_alu.cpp b/src/gallium/drivers/r600/sfn/sfn_instr_alu.cpp
index 5344d0653b6..f57017590d4 100644
--- a/src/gallium/drivers/r600/sfn/sfn_instr_alu.cpp
+++ b/src/gallium/drivers/r600/sfn/sfn_instr_alu.cpp
@@ -2101,6 +2101,14 @@ emit_alu_op2_64bit(const nir_alu_instr& alu,
 
    int num_emit0 = opcode == op2_mul_64 ? 3 : 1;
 
+   std::array<std::array<PRegister, 4>,2> tmp;
+   for (unsigned k = 0; k < alu.def.num_components; ++k) {
+      tmp[k][0] = shader.emit_load_to_register(value_factory.src64(alu.src[order[0]], k, 1), 0);
+      tmp[k][1] = shader.emit_load_to_register(value_factory.src64(alu.src[order[1]], k, 1), 1);
+      tmp[k][2] = shader.emit_load_to_register(value_factory.src64(alu.src[order[0]], k, 0), 2);
+      tmp[k][3] = shader.emit_load_to_register(value_factory.src64(alu.src[order[1]], k, 0), 3);
+   }
+
    assert(num_emit0 == 1 || alu.def.num_components == 1);
 
    for (unsigned k = 0; k < alu.def.num_components; ++k) {
@@ -2111,8 +2119,8 @@ emit_alu_op2_64bit(const nir_alu_instr& alu,
 
          ir = new AluInstr(opcode,
                            dest,
-                           value_factory.src64(alu.src[order[0]], k, 1),
-                           value_factory.src64(alu.src[order[1]], k, 1),
+                           tmp[k][0],
+                           tmp[k][1],
                            i < 2 ? AluInstr::write : AluInstr::empty);
          group->add_instruction(ir);
       }
@@ -2122,8 +2130,8 @@ emit_alu_op2_64bit(const nir_alu_instr& alu,
 
       ir = new AluInstr(opcode,
                         dest,
-                        value_factory.src64(alu.src[order[0]], k, 0),
-                        value_factory.src64(alu.src[order[1]], k, 0),
+                        tmp[k][2],
+                        tmp[k][3],
                         i == 1 ? AluInstr::write : AluInstr::empty);
       group->add_instruction(ir);
    }
@@ -2222,27 +2230,22 @@ static bool
 emit_alu_b2f64(const nir_alu_instr& alu, Shader& shader)
 {
    auto& value_factory = shader.value_factory();
-   auto group = new AluGroup();
-   AluInstr *ir = nullptr;
 
    for (unsigned i = 0; i < alu.def.num_components; ++i) {
-      ir = new AluInstr(op2_and_int,
+      auto ir = new AluInstr(op2_and_int,
                         value_factory.dest(alu.def, 2 * i, pin_group),
                         value_factory.src(alu.src[0], i),
                         value_factory.zero(),
                         {alu_write});
-      group->add_instruction(ir);
+      shader.emit_instruction(ir);
 
       ir = new AluInstr(op2_and_int,
                         value_factory.dest(alu.def, 2 * i + 1, pin_group),
                         value_factory.src(alu.src[0], i),
                         value_factory.literal(0x3ff00000),
                         {alu_write});
-      group->add_instruction(ir);
+      shader.emit_instruction(ir);
    }
-   if (ir)
-      ir->set_alu_flag(alu_last_instr);
-   shader.emit_instruction(group);
    return true;
 }
 
diff --git a/src/gallium/drivers/r600/sfn/sfn_shader.cpp b/src/gallium/drivers/r600/sfn/sfn_shader.cpp
index 7af0000f45b..2e374bd3301 100644
--- a/src/gallium/drivers/r600/sfn/sfn_shader.cpp
+++ b/src/gallium/drivers/r600/sfn/sfn_shader.cpp
@@ -938,13 +938,14 @@ lds_op_from_intrinsic(nir_atomic_op op, bool ret)
 }
 
 PRegister
-Shader::emit_load_to_register(PVirtualValue src)
+Shader::emit_load_to_register(PVirtualValue src, int chan)
 {
    assert(src);
    PRegister dest = src->as_register();
 
-   if (!dest) {
-      dest = value_factory().temp_register();
+   if (!dest || chan >= 0) {
+      dest = value_factory().temp_register(chan);
+      dest->set_pin(pin_free);
       emit_instruction(new AluInstr(op1_mov, dest, src, AluInstr::last_write));
    }
    return dest;
@@ -1364,6 +1365,22 @@ void Shader::InstructionChain::visit(AluInstr *instr)
          }
       }
    }
+
+   if (instr->has_lds_access()) {
+      last_lds_access = instr;
+      if (last_group_barrier)
+         instr->add_required_instr(last_group_barrier);
+   }
+
+   if (!instr->has_alu_flag(alu_is_lds) &&
+       instr->opcode() == op0_group_barrier) {
+      last_group_barrier = instr;
+      if (last_lds_access)
+         instr->add_required_instr(last_group_barrier);
+      if (last_ssbo_instr)
+         instr->add_required_instr(last_ssbo_instr);
+   }
+
 }
 
 void
@@ -1402,6 +1419,9 @@ Shader::InstructionChain::visit(RatInstr *instr)
 
    if (last_kill_instr)
       instr->add_required_instr(last_kill_instr);
+
+   if (last_group_barrier)
+      instr->add_required_instr(last_group_barrier);
 }
 
 void
@@ -1464,13 +1484,9 @@ Shader::emit_group_barrier(nir_intrinsic_instr *intr)
 {
    assert(m_control_flow_depth == 0);
    (void)intr;
-   /* Put barrier into it's own block, so that optimizers and the
-    * scheduler don't move code */
-   start_new_block(0);
    auto op = new AluInstr(op0_group_barrier, 0);
    op->set_alu_flag(alu_last_instr);
    emit_instruction(op);
-   start_new_block(0);
    return true;
 }
 
diff --git a/src/gallium/drivers/r600/sfn/sfn_shader.h b/src/gallium/drivers/r600/sfn/sfn_shader.h
index 9b442bdab04..e60c0da754a 100644
--- a/src/gallium/drivers/r600/sfn/sfn_shader.h
+++ b/src/gallium/drivers/r600/sfn/sfn_shader.h
@@ -261,7 +261,7 @@ public:
       return m_rat_return_address;
    }
 
-   PRegister emit_load_to_register(PVirtualValue src);
+   PRegister emit_load_to_register(PVirtualValue src, int chan = -1);
 
    virtual unsigned image_size_const_offset() { return 0;}
 
@@ -413,6 +413,8 @@ private:
       Instr *last_gds_instr{nullptr};
       Instr *last_ssbo_instr{nullptr};
       Instr *last_kill_instr{nullptr};
+      Instr *last_lds_access{nullptr};
+      Instr *last_group_barrier{nullptr};
       std::unordered_map<int, Instr * > last_alu_with_indirect_reg;
       bool prepare_mem_barrier{false};
    };
diff --git a/src/gallium/drivers/r600/sfn/sfn_shader_gs.h b/src/gallium/drivers/r600/sfn/sfn_shader_gs.h
index 206e7713052..aa40b0f078c 100644
--- a/src/gallium/drivers/r600/sfn/sfn_shader_gs.h
+++ b/src/gallium/drivers/r600/sfn/sfn_shader_gs.h
@@ -1,3 +1,9 @@
+/*
+ * Copyright 2021 Collabora LTD
+ * Author: Gert Wollny <gert.wollny@collabora.com>
+ * SPDX-License-Identifier: MIT
+ */
+
 #ifndef SFN_GEOMETRYSHADER_H
 #define SFN_GEOMETRYSHADER_H
 
diff --git a/src/gallium/drivers/r600/sfn/sfn_virtualvalues.cpp b/src/gallium/drivers/r600/sfn/sfn_virtualvalues.cpp
index 804d1bac2e4..9c70915e217 100644
--- a/src/gallium/drivers/r600/sfn/sfn_virtualvalues.cpp
+++ b/src/gallium/drivers/r600/sfn/sfn_virtualvalues.cpp
@@ -1076,6 +1076,7 @@ LocalArrayValue::accept(ConstRegisterVisitor& vistor) const
 void
 LocalArrayValue::add_parent_to_array(Instr *instr)
 {
+   m_array.add_parent(instr);
    if (m_addr)
       m_array.add_parent_to_elements(chan(), instr);
 }
diff --git a/src/gallium/drivers/radeonsi/radeon_uvd_enc.c b/src/gallium/drivers/radeonsi/radeon_uvd_enc.c
index c6daee96366..7c32a9d5056 100644
--- a/src/gallium/drivers/radeonsi/radeon_uvd_enc.c
+++ b/src/gallium/drivers/radeonsi/radeon_uvd_enc.c
@@ -80,9 +80,9 @@ static void radeon_uvd_enc_get_param(struct radeon_uvd_encoder *enc,
          enc->enc_pic.crop_bottom = pic->seq.conf_win_bottom_offset;
    } else {
          enc->enc_pic.crop_left = 0;
-         enc->enc_pic.crop_right = (align(enc->base.width, 16) - enc->base.width) / 2;
+         enc->enc_pic.crop_right = 0;
          enc->enc_pic.crop_top = 0;
-         enc->enc_pic.crop_bottom = (align(enc->base.height, 16) - enc->base.height) / 2;
+         enc->enc_pic.crop_bottom = 0;
    }
 
    enc->enc_pic.general_tier_flag = pic->seq.general_tier_flag;
diff --git a/src/gallium/drivers/radeonsi/radeon_uvd_enc_1_1.c b/src/gallium/drivers/radeonsi/radeon_uvd_enc_1_1.c
index 4ecaa4405c8..04d7bd88a32 100644
--- a/src/gallium/drivers/radeonsi/radeon_uvd_enc_1_1.c
+++ b/src/gallium/drivers/radeonsi/radeon_uvd_enc_1_1.c
@@ -203,9 +203,9 @@ static void radeon_uvd_enc_session_init_hevc(struct radeon_uvd_encoder *enc)
    enc->enc_pic.session_init.aligned_picture_width = align(enc->base.width, 64);
    enc->enc_pic.session_init.aligned_picture_height = align(enc->base.height, 16);
    enc->enc_pic.session_init.padding_width =
-      enc->enc_pic.session_init.aligned_picture_width - enc->base.width;
+      (enc->enc_pic.crop_left + enc->enc_pic.crop_right) * 2;
    enc->enc_pic.session_init.padding_height =
-      enc->enc_pic.session_init.aligned_picture_height - enc->base.height;
+      (enc->enc_pic.crop_top + enc->enc_pic.crop_bottom) * 2;
    enc->enc_pic.session_init.pre_encode_mode = RENC_UVD_PREENCODE_MODE_NONE;
    enc->enc_pic.session_init.pre_encode_chroma_enabled = false;
 
diff --git a/src/gallium/drivers/radeonsi/radeon_vcn_dec.c b/src/gallium/drivers/radeonsi/radeon_vcn_dec.c
index c6fa84af78a..ee52d8c7f5a 100644
--- a/src/gallium/drivers/radeonsi/radeon_vcn_dec.c
+++ b/src/gallium/drivers/radeonsi/radeon_vcn_dec.c
@@ -239,15 +239,13 @@ static rvcn_dec_message_avc_t get_h264_msg(struct radeon_decoder *dec,
       }
    }
 
-   /* if reference picture exists, however no reference picture found at the end
-      curr_pic_ref_frame_num == 0, which is not reasonable, should be corrected. */
-   if (result.used_for_reference_flags && (result.curr_pic_ref_frame_num == 0)) {
-      for (i = 0; i < ARRAY_SIZE(result.ref_frame_list); i++) {
-         result.ref_frame_list[i] = pic->ref[i] ?
-                (uintptr_t)vl_video_buffer_get_associated_data(pic->ref[i], &dec->base) : 0xff;
-         if (result.ref_frame_list[i] != 0xff) {
+   /* need at least one reference for P/B frames */
+   if (result.curr_pic_ref_frame_num == 0 && pic->slice_parameter.slice_info_present) {
+      for (i = 0; i < pic->slice_count; i++) {
+         if (pic->slice_parameter.slice_type[i] % 5 != 2) {
             result.curr_pic_ref_frame_num++;
-            result.non_existing_frame_flags &= ~(1 << i);
+            result.ref_frame_list[0] = 0;
+            result.non_existing_frame_flags &= ~1;
             break;
          }
       }
@@ -279,7 +277,8 @@ static rvcn_dec_message_avc_t get_h264_msg(struct radeon_decoder *dec,
       dec->ref_codec.bts = CODEC_8_BITS;
       dec->ref_codec.index = result.decoded_pic_idx;
       dec->ref_codec.ref_size = 16;
-      memset(dec->ref_codec.ref_list, 0xff, sizeof(dec->ref_codec.ref_list));
+      dec->ref_codec.num_refs = result.curr_pic_ref_frame_num;
+      STATIC_ASSERT(sizeof(dec->ref_codec.ref_list) == sizeof(result.ref_frame_list));
       memcpy(dec->ref_codec.ref_list, result.ref_frame_list, sizeof(result.ref_frame_list));
    }
 
@@ -292,7 +291,7 @@ static rvcn_dec_message_hevc_t get_h265_msg(struct radeon_decoder *dec,
                                             struct pipe_h265_picture_desc *pic)
 {
    rvcn_dec_message_hevc_t result;
-   unsigned i, j;
+   unsigned i, j, num_refs = 0;
 
    memset(&result, 0, sizeof(result));
    result.sps_info_flags = 0;
@@ -307,15 +306,15 @@ static rvcn_dec_message_hevc_t get_h265_msg(struct radeon_decoder *dec,
    result.sps_info_flags |= pic->pps->sps->separate_colour_plane_flag << 8;
    if (((struct si_screen *)dec->screen)->info.family == CHIP_CARRIZO)
       result.sps_info_flags |= 1 << 9;
-   if (pic->UseRefPicList == true)
+   if (pic->UseRefPicList == true) {
       result.sps_info_flags |= 1 << 10;
+      result.sps_info_flags |= 1 << 12;
+   }
    if (pic->UseStRpsBits == true && pic->pps->st_rps_bits != 0) {
       result.sps_info_flags |= 1 << 11;
       result.st_rps_bits = pic->pps->st_rps_bits;
    }
 
-   result.sps_info_flags |= 1 << 12;
-
    result.chroma_format = pic->pps->sps->chroma_format_idc;
    result.bit_depth_luma_minus8 = pic->pps->sps->bit_depth_luma_minus8;
    result.bit_depth_chroma_minus8 = pic->pps->sps->bit_depth_chroma_minus8;
@@ -413,9 +412,10 @@ static rvcn_dec_message_hevc_t get_h265_msg(struct radeon_decoder *dec,
 
       result.poc_list[i] = pic->PicOrderCntVal[i];
 
-      if (ref)
+      if (ref) {
          ref_pic = (uintptr_t)vl_video_buffer_get_associated_data(ref, &dec->base);
-      else
+         num_refs++;
+      } else
          ref_pic = 0x7F;
       result.ref_pic_list[i] = ref_pic;
    }
@@ -469,7 +469,8 @@ static rvcn_dec_message_hevc_t get_h265_msg(struct radeon_decoder *dec,
          CODEC_10_BITS : CODEC_8_BITS;
       dec->ref_codec.index = result.curr_idx;
       dec->ref_codec.ref_size = 15;
-      memset(dec->ref_codec.ref_list, 0x7f, sizeof(dec->ref_codec.ref_list));
+      dec->ref_codec.num_refs = num_refs;
+      STATIC_ASSERT(sizeof(dec->ref_codec.ref_list) == sizeof(result.ref_pic_list));
       memcpy(dec->ref_codec.ref_list, result.ref_pic_list, sizeof(result.ref_pic_list));
    }
    return result;
@@ -507,7 +508,7 @@ static rvcn_dec_message_vp9_t get_vp9_msg(struct radeon_decoder *dec,
                                           struct pipe_vp9_picture_desc *pic)
 {
    rvcn_dec_message_vp9_t result;
-   unsigned i ,j;
+   unsigned i, j, num_refs = 0;
 
    memset(&result, 0, sizeof(result));
 
@@ -641,9 +642,13 @@ static rvcn_dec_message_vp9_t get_vp9_msg(struct radeon_decoder *dec,
    get_current_pic_index(dec, target, &result.curr_pic_idx);
 
    for (i = 0; i < 8; i++) {
-      result.ref_frame_map[i] =
-         (pic->ref[i]) ? (uintptr_t)vl_video_buffer_get_associated_data(pic->ref[i], &dec->base)
-                       : 0x7f;
+      uintptr_t ref_frame;
+      if (pic->ref[i]) {
+         ref_frame = (uintptr_t)vl_video_buffer_get_associated_data(pic->ref[i], &dec->base);
+         num_refs++;
+      } else
+         ref_frame = 0x7f;
+      result.ref_frame_map[i] = ref_frame;
    }
 
    result.frame_refs[0] = result.ref_frame_map[pic->picture_parameter.pic_fields.last_ref_frame];
@@ -669,6 +674,7 @@ static rvcn_dec_message_vp9_t get_vp9_msg(struct radeon_decoder *dec,
          CODEC_10_BITS : CODEC_8_BITS;
       dec->ref_codec.index = result.curr_pic_idx;
       dec->ref_codec.ref_size = 8;
+      dec->ref_codec.num_refs = num_refs;
       memset(dec->ref_codec.ref_list, 0x7f, sizeof(dec->ref_codec.ref_list));
       memcpy(dec->ref_codec.ref_list, result.ref_frame_map, sizeof(result.ref_frame_map));
    }
@@ -959,7 +965,7 @@ static rvcn_dec_message_av1_t get_av1_msg(struct radeon_decoder *dec,
                                           struct pipe_av1_picture_desc *pic)
 {
    rvcn_dec_message_av1_t result;
-   unsigned i, j;
+   unsigned i, j, num_refs = 0;
    uint16_t tile_count = pic->picture_parameter.tile_cols * pic->picture_parameter.tile_rows;
 
    memset(&result, 0, sizeof(result));
@@ -1151,9 +1157,13 @@ static rvcn_dec_message_av1_t get_av1_msg(struct radeon_decoder *dec,
    result.order_hint_bits = pic->picture_parameter.order_hint_bits_minus_1 + 1;
 
    for (i = 0; i < NUM_AV1_REFS; ++i) {
-      result.ref_frame_map[i] =
-         (pic->ref[i]) ? (uintptr_t)vl_video_buffer_get_associated_data(pic->ref[i], &dec->base)
-                       : 0x7f;
+      uintptr_t ref_frame;
+      if (pic->ref[i]) {
+         ref_frame = (uintptr_t)vl_video_buffer_get_associated_data(pic->ref[i], &dec->base);
+         num_refs++;
+      } else
+         ref_frame = 0x7f;
+      result.ref_frame_map[i] = ref_frame;
    }
    for (i = 0; i < NUM_AV1_REFS_PER_FRAME; ++i)
        result.frame_refs[i] = result.ref_frame_map[pic->picture_parameter.ref_frame_idx[i]];
@@ -1300,6 +1310,7 @@ static rvcn_dec_message_av1_t get_av1_msg(struct radeon_decoder *dec,
       dec->ref_codec.bts = pic->picture_parameter.bit_depth_idx ? CODEC_10_BITS : CODEC_8_BITS;
       dec->ref_codec.index = result.curr_pic_idx;
       dec->ref_codec.ref_size = 8;
+      dec->ref_codec.num_refs = num_refs;
       memset(dec->ref_codec.ref_list, 0x7f, sizeof(dec->ref_codec.ref_list));
       memcpy(dec->ref_codec.ref_list, result.ref_frame_map, sizeof(result.ref_frame_map));
    }
@@ -1816,6 +1827,7 @@ static unsigned rvcn_dec_dynamic_dpb_t2_message(struct radeon_decoder *dec, rvcn
       size = size * 3 / 2;
 
    list_for_each_entry_safe(struct rvcn_dec_dynamic_dpb_t2, d, &dec->dpb_ref_list, list) {
+      bool found = false;
       for (i = 0; i < dec->ref_codec.ref_size; ++i) {
          if (((dec->ref_codec.ref_list[i] & 0x7f) != 0x7f) && (d->index == (dec->ref_codec.ref_list[i] & 0x7f))) {
             if (!dummy)
@@ -1829,10 +1841,10 @@ static unsigned rvcn_dec_dynamic_dpb_t2_message(struct radeon_decoder *dec, rvcn
             dynamic_dpb_t2->dpbAddrLo[i] = addr;
             dynamic_dpb_t2->dpbAddrHi[i] = addr >> 32;
             ++dynamic_dpb_t2->dpbArraySize;
-            break;
+            found = true;
          }
       }
-      if (i == dec->ref_codec.ref_size) {
+      if (!found) {
          if (d->dpb.res->b.b.width0 * d->dpb.res->b.b.height0 != size) {
             list_del(&d->list);
             list_addtail(&d->list, &dec->dpb_unref_list);
@@ -1887,6 +1899,23 @@ static unsigned rvcn_dec_dynamic_dpb_t2_message(struct radeon_decoder *dec, rvcn
       list_addtail(&dpb->list, &dec->dpb_ref_list);
    }
 
+   if (dynamic_dpb_t2->dpbArraySize < dec->ref_codec.num_refs) {
+      struct rvcn_dec_dynamic_dpb_t2 *d =
+         list_first_entry(&dec->dpb_ref_list, struct rvcn_dec_dynamic_dpb_t2, list);
+      addr = dec->ws->buffer_get_virtual_address(d->dpb.res->buf);
+      if (!addr && dummy)
+         addr = dec->ws->buffer_get_virtual_address(dummy->dpb.res->buf);
+      assert(addr);
+      for (i = 0; i < dec->ref_codec.num_refs; ++i) {
+         if (dynamic_dpb_t2->dpbAddrLo[i] || dynamic_dpb_t2->dpbAddrHi[i])
+            continue;
+         dynamic_dpb_t2->dpbAddrLo[i] = addr;
+         dynamic_dpb_t2->dpbAddrHi[i] = addr >> 32;
+         ++dynamic_dpb_t2->dpbArraySize;
+      }
+      assert(dynamic_dpb_t2->dpbArraySize == dec->ref_codec.num_refs);
+   }
+
    dec->ws->cs_add_buffer(&dec->cs, dpb->dpb.res->buf,
       RADEON_USAGE_READWRITE | RADEON_USAGE_SYNCHRONIZED, RADEON_DOMAIN_VRAM);
    addr = dec->ws->buffer_get_virtual_address(dpb->dpb.res->buf);
@@ -2873,9 +2902,6 @@ static void radeon_dec_decode_bitstream(struct pipe_video_codec *decoder,
    if (!dec->bs_ptr)
       return;
 
-   if (dec->bs_size && dec->stream_type == RDECODE_CODEC_AV1)
-      return;
-
    unsigned long total_bs_size = dec->bs_size;
    for (i = 0; i < num_buffers; ++i)
       total_bs_size += sizes[i];
diff --git a/src/gallium/drivers/radeonsi/radeon_vcn_dec.h b/src/gallium/drivers/radeonsi/radeon_vcn_dec.h
index 1be65c7ded8..3d502ce7b66 100644
--- a/src/gallium/drivers/radeonsi/radeon_vcn_dec.h
+++ b/src/gallium/drivers/radeonsi/radeon_vcn_dec.h
@@ -120,6 +120,7 @@ struct radeon_decoder {
       } bts;
       uint8_t index;
       unsigned ref_size;
+      unsigned num_refs;
       uint8_t ref_list[16];
    } ref_codec;
 
diff --git a/src/gallium/drivers/radeonsi/radeon_vcn_enc.c b/src/gallium/drivers/radeonsi/radeon_vcn_enc.c
index 9d0aa41e342..fb5ad4822af 100644
--- a/src/gallium/drivers/radeonsi/radeon_vcn_enc.c
+++ b/src/gallium/drivers/radeonsi/radeon_vcn_enc.c
@@ -196,9 +196,9 @@ static void radeon_vcn_enc_h264_get_cropping_param(struct radeon_encoder *enc,
       enc->enc_pic.crop_bottom = pic->seq.enc_frame_crop_bottom_offset;
    } else {
       enc->enc_pic.crop_left = 0;
-      enc->enc_pic.crop_right = (align(enc->base.width, 16) - enc->base.width) / 2;
+      enc->enc_pic.crop_right = 0;
       enc->enc_pic.crop_top = 0;
-      enc->enc_pic.crop_bottom = (align(enc->base.height, 16) - enc->base.height) / 2;
+      enc->enc_pic.crop_bottom = 0;
    }
 }
 
@@ -238,7 +238,6 @@ static void radeon_vcn_enc_h264_get_rc_param(struct radeon_encoder *enc,
    uint32_t frame_rate_den, frame_rate_num;
 
    enc->enc_pic.num_temporal_layers = pic->seq.num_temporal_layers ? pic->seq.num_temporal_layers : 1;
-   enc->enc_pic.temporal_id = 0;
    for (int i = 0; i < enc->enc_pic.num_temporal_layers; i++) {
       enc->enc_pic.rc_layer_init[i].target_bit_rate = pic->rate_ctrl[i].target_bitrate;
       enc->enc_pic.rc_layer_init[i].peak_bit_rate = pic->rate_ctrl[i].peak_bitrate;
@@ -446,9 +445,9 @@ static void radeon_vcn_enc_hevc_get_cropping_param(struct radeon_encoder *enc,
       enc->enc_pic.crop_bottom = pic->seq.conf_win_bottom_offset;
    } else {
       enc->enc_pic.crop_left = 0;
-      enc->enc_pic.crop_right = (align(enc->base.width, 16) - enc->base.width) / 2;
+      enc->enc_pic.crop_right = 0;
       enc->enc_pic.crop_top = 0;
-      enc->enc_pic.crop_bottom = (align(enc->base.height, 16) - enc->base.height) / 2;
+      enc->enc_pic.crop_bottom = 0;
    }
 }
 
@@ -1119,7 +1118,7 @@ static void radeon_enc_begin_frame(struct pipe_video_codec *encoder,
       enc->si = CALLOC_STRUCT(rvid_buffer);
       if (!enc->si ||
           !enc->stream_handle ||
-          !si_vid_create_buffer(enc->screen, enc->si, 128 * 1024, PIPE_USAGE_STAGING)) {
+          !si_vid_create_buffer(enc->screen, enc->si, 128 * 1024, PIPE_USAGE_DEFAULT)) {
          RVID_ERR("Can't create session buffer.\n");
          goto error;
       }
diff --git a/src/gallium/drivers/radeonsi/radeon_vcn_enc_1_2.c b/src/gallium/drivers/radeonsi/radeon_vcn_enc_1_2.c
index cb1b2957c49..b1d4d40808e 100644
--- a/src/gallium/drivers/radeonsi/radeon_vcn_enc_1_2.c
+++ b/src/gallium/drivers/radeonsi/radeon_vcn_enc_1_2.c
@@ -81,10 +81,12 @@ static void radeon_enc_session_init(struct radeon_encoder *enc)
       enc->enc_pic.session_init.aligned_picture_width = align(enc->base.width, 64);
    }
    enc->enc_pic.session_init.aligned_picture_height = align(enc->base.height, 16);
+
    enc->enc_pic.session_init.padding_width =
-      enc->enc_pic.session_init.aligned_picture_width - enc->base.width;
+      (enc->enc_pic.crop_left + enc->enc_pic.crop_right) * 2;
    enc->enc_pic.session_init.padding_height =
-      enc->enc_pic.session_init.aligned_picture_height - enc->base.height;
+      (enc->enc_pic.crop_top + enc->enc_pic.crop_bottom) * 2;
+
    enc->enc_pic.session_init.display_remote = 0;
    enc->enc_pic.session_init.pre_encode_mode = enc->enc_pic.quality_modes.pre_encode_mode;
    enc->enc_pic.session_init.pre_encode_chroma_enabled = !!(enc->enc_pic.quality_modes.pre_encode_mode);
@@ -114,8 +116,6 @@ static void radeon_enc_layer_control(struct radeon_encoder *enc)
 
 static void radeon_enc_layer_select(struct radeon_encoder *enc)
 {
-   enc->enc_pic.layer_sel.temporal_layer_index = enc->enc_pic.temporal_id;
-
    RADEON_ENC_BEGIN(enc->cmd.layer_select);
    RADEON_ENC_CS(enc->enc_pic.layer_sel.temporal_layer_index);
    RADEON_ENC_END();
@@ -183,7 +183,7 @@ static void radeon_enc_rc_session_init(struct radeon_encoder *enc)
 
 static void radeon_enc_rc_layer_init(struct radeon_encoder *enc)
 {
-   unsigned int i = enc->enc_pic.temporal_id;
+   unsigned int i = enc->enc_pic.layer_sel.temporal_layer_index;
    RADEON_ENC_BEGIN(enc->cmd.rc_layer_init);
    RADEON_ENC_CS(enc->enc_pic.rc_layer_init[i].target_bit_rate);
    RADEON_ENC_CS(enc->enc_pic.rc_layer_init[i].peak_bit_rate);
@@ -221,7 +221,9 @@ static void radeon_enc_deblocking_filter_hevc(struct radeon_encoder *enc)
 
 static void radeon_enc_quality_params(struct radeon_encoder *enc)
 {
-   enc->enc_pic.quality_params.vbaq_mode = enc->enc_pic.quality_modes.vbaq_mode;
+   enc->enc_pic.quality_params.vbaq_mode =
+      enc->enc_pic.rc_session_init.rate_control_method != RENCODE_RATE_CONTROL_METHOD_NONE ?
+      enc->enc_pic.quality_modes.vbaq_mode : 0;
    enc->enc_pic.quality_params.scene_change_sensitivity = 0;
    enc->enc_pic.quality_params.scene_change_min_idr_interval = 0;
    enc->enc_pic.quality_params.two_pass_search_center_map_mode =
@@ -779,7 +781,12 @@ static void radeon_enc_nalu_vps(struct radeon_encoder *enc)
    radeon_enc_code_fixed_bits(enc, 0x0, 2);
    radeon_enc_code_fixed_bits(enc, enc->enc_pic.general_tier_flag, 1);
    radeon_enc_code_fixed_bits(enc, enc->enc_pic.general_profile_idc, 5);
-   radeon_enc_code_fixed_bits(enc, 0x60000000, 32);
+
+   if (enc->enc_pic.general_profile_idc == 2)
+      radeon_enc_code_fixed_bits(enc, 0x20000000, 32);
+   else
+      radeon_enc_code_fixed_bits(enc, 0x60000000, 32);
+
    radeon_enc_code_fixed_bits(enc, 0xb0000000, 32);
    radeon_enc_code_fixed_bits(enc, 0x0, 16);
    radeon_enc_code_fixed_bits(enc, enc->enc_pic.general_level_idc, 8);
@@ -1350,7 +1357,7 @@ static void begin(struct radeon_encoder *enc)
 
    i = 0;
    do {
-      enc->enc_pic.temporal_id = i;
+      enc->enc_pic.layer_sel.temporal_layer_index = i;
       enc->layer_select(enc);
       enc->rc_layer_init(enc);
       enc->layer_select(enc);
@@ -1402,7 +1409,7 @@ static void encode(struct radeon_encoder *enc)
    if (enc->need_rate_control) {
       i = 0;
       do {
-         enc->enc_pic.temporal_id = i;
+         enc->enc_pic.layer_sel.temporal_layer_index = i;
          enc->layer_select(enc);
          enc->rc_layer_init(enc);
       } while (++i < enc->enc_pic.num_temporal_layers);
diff --git a/src/gallium/drivers/radeonsi/radeon_vcn_enc_2_0.c b/src/gallium/drivers/radeonsi/radeon_vcn_enc_2_0.c
index aefbd3192e8..3ecf88e2142 100644
--- a/src/gallium/drivers/radeonsi/radeon_vcn_enc_2_0.c
+++ b/src/gallium/drivers/radeonsi/radeon_vcn_enc_2_0.c
@@ -68,7 +68,9 @@ static void radeon_enc_op_preset(struct radeon_encoder *enc)
 
 static void radeon_enc_quality_params(struct radeon_encoder *enc)
 {
-   enc->enc_pic.quality_params.vbaq_mode = enc->enc_pic.quality_modes.vbaq_mode;
+   enc->enc_pic.quality_params.vbaq_mode =
+      enc->enc_pic.rc_session_init.rate_control_method != RENCODE_RATE_CONTROL_METHOD_NONE ?
+      enc->enc_pic.quality_modes.vbaq_mode : 0;
    enc->enc_pic.quality_params.scene_change_sensitivity = 0;
    enc->enc_pic.quality_params.scene_change_min_idr_interval = 0;
    enc->enc_pic.quality_params.two_pass_search_center_map_mode =
@@ -515,7 +517,7 @@ static void encode(struct radeon_encoder *enc)
    if (enc->need_rate_control) {
       i = 0;
       do {
-         enc->enc_pic.temporal_id = i;
+         enc->enc_pic.layer_sel.temporal_layer_index = i;
          enc->layer_select(enc);
          enc->rc_layer_init(enc);
       } while (++i < enc->enc_pic.num_temporal_layers);
diff --git a/src/gallium/drivers/radeonsi/radeon_vcn_enc_3_0.c b/src/gallium/drivers/radeonsi/radeon_vcn_enc_3_0.c
index ce94f221c1c..f2e217884d4 100644
--- a/src/gallium/drivers/radeonsi/radeon_vcn_enc_3_0.c
+++ b/src/gallium/drivers/radeonsi/radeon_vcn_enc_3_0.c
@@ -98,7 +98,9 @@ static void radeon_enc_encode_params_h264(struct radeon_encoder *enc)
 
 static void radeon_enc_quality_params(struct radeon_encoder *enc)
 {
-   enc->enc_pic.quality_params.vbaq_mode = enc->enc_pic.quality_modes.vbaq_mode;
+   enc->enc_pic.quality_params.vbaq_mode =
+      enc->enc_pic.rc_session_init.rate_control_method != RENCODE_RATE_CONTROL_METHOD_NONE ?
+      enc->enc_pic.quality_modes.vbaq_mode : 0;
    enc->enc_pic.quality_params.scene_change_sensitivity = 0;
    enc->enc_pic.quality_params.scene_change_min_idr_interval = 0;
    enc->enc_pic.quality_params.two_pass_search_center_map_mode =
@@ -506,10 +508,12 @@ static void radeon_enc_session_init(struct radeon_encoder *enc)
       enc->enc_pic.session_init.aligned_picture_width = align(enc->base.width, 64);
    }
    enc->enc_pic.session_init.aligned_picture_height = align(enc->base.height, 16);
+
    enc->enc_pic.session_init.padding_width =
-      enc->enc_pic.session_init.aligned_picture_width - enc->base.width;
+      (enc->enc_pic.crop_left + enc->enc_pic.crop_right) * 2;
    enc->enc_pic.session_init.padding_height =
-      enc->enc_pic.session_init.aligned_picture_height - enc->base.height;
+      (enc->enc_pic.crop_top + enc->enc_pic.crop_bottom) * 2;
+
    enc->enc_pic.session_init.slice_output_enabled = 0;
    enc->enc_pic.session_init.display_remote = 0;
    enc->enc_pic.session_init.pre_encode_mode = enc->enc_pic.quality_modes.pre_encode_mode;
diff --git a/src/gallium/drivers/radeonsi/radeon_vcn_enc_4_0.c b/src/gallium/drivers/radeonsi/radeon_vcn_enc_4_0.c
index 5255922a555..f15ea7930ba 100644
--- a/src/gallium/drivers/radeonsi/radeon_vcn_enc_4_0.c
+++ b/src/gallium/drivers/radeonsi/radeon_vcn_enc_4_0.c
@@ -81,7 +81,6 @@ static void radeon_enc_op_preset(struct radeon_encoder *enc)
 
 static void radeon_enc_session_init(struct radeon_encoder *enc)
 {
-   bool av1_encoding = false;
    uint32_t av1_height = enc->enc_pic.pic_height_in_luma_samples;
 
    switch (u_reduce_video_profile(enc->base.profile)) {
@@ -89,11 +88,20 @@ static void radeon_enc_session_init(struct radeon_encoder *enc)
          enc->enc_pic.session_init.encode_standard = RENCODE_ENCODE_STANDARD_H264;
          enc->enc_pic.session_init.aligned_picture_width = align(enc->base.width, 16);
          enc->enc_pic.session_init.aligned_picture_height = align(enc->base.height, 16);
+
+         enc->enc_pic.session_init.padding_width =
+            (enc->enc_pic.crop_left + enc->enc_pic.crop_right) * 2;
+         enc->enc_pic.session_init.padding_height =
+            (enc->enc_pic.crop_top + enc->enc_pic.crop_bottom) * 2;
          break;
       case PIPE_VIDEO_FORMAT_HEVC:
          enc->enc_pic.session_init.encode_standard = RENCODE_ENCODE_STANDARD_HEVC;
          enc->enc_pic.session_init.aligned_picture_width = align(enc->base.width, 64);
          enc->enc_pic.session_init.aligned_picture_height = align(enc->base.height, 16);
+         enc->enc_pic.session_init.padding_width =
+            (enc->enc_pic.crop_left + enc->enc_pic.crop_right) * 2;
+         enc->enc_pic.session_init.padding_height =
+            (enc->enc_pic.crop_top + enc->enc_pic.crop_bottom) * 2;
          break;
       case PIPE_VIDEO_FORMAT_AV1:
          enc->enc_pic.session_init.encode_standard = RENCODE_ENCODE_STANDARD_AV1;
@@ -104,33 +112,24 @@ static void radeon_enc_session_init(struct radeon_encoder *enc)
          if (!(av1_height % 8) && (av1_height % 16) && !(enc->enc_pic.enable_render_size))
             enc->enc_pic.session_init.aligned_picture_height = av1_height + 2;
 
-         av1_encoding = true;
+         enc->enc_pic.session_init.padding_width =
+            enc->enc_pic.session_init.aligned_picture_width -
+            enc->enc_pic.pic_width_in_luma_samples;
+         enc->enc_pic.session_init.padding_height =
+            enc->enc_pic.session_init.aligned_picture_height - av1_height;
+
+         if (enc->enc_pic.enable_render_size)
+            enc->enc_pic.enable_render_size =
+                           (enc->enc_pic.session_init.aligned_picture_width !=
+                            enc->enc_pic.render_width) ||
+                           (enc->enc_pic.session_init.aligned_picture_height !=
+                            enc->enc_pic.render_height);
          break;
       default:
          assert(0);
          break;
    }
 
-   enc->enc_pic.session_init.padding_width =
-      enc->enc_pic.session_init.aligned_picture_width - enc->base.width;
-   enc->enc_pic.session_init.padding_height =
-      enc->enc_pic.session_init.aligned_picture_height - enc->base.height;
-
-   if (av1_encoding) {
-      enc->enc_pic.session_init.padding_width =
-         enc->enc_pic.session_init.aligned_picture_width -
-         enc->enc_pic.pic_width_in_luma_samples;
-      enc->enc_pic.session_init.padding_height =
-         enc->enc_pic.session_init.aligned_picture_height - av1_height;
-
-      if (enc->enc_pic.enable_render_size)
-         enc->enc_pic.enable_render_size =
-                        (enc->enc_pic.session_init.aligned_picture_width !=
-                         enc->enc_pic.render_width) ||
-                        (enc->enc_pic.session_init.aligned_picture_height !=
-                         enc->enc_pic.render_height);
-   }
-
    enc->enc_pic.session_init.slice_output_enabled = 0;
    enc->enc_pic.session_init.display_remote = 0;
    enc->enc_pic.session_init.pre_encode_mode = enc->enc_pic.quality_modes.pre_encode_mode;
diff --git a/src/gallium/drivers/radeonsi/si_blit.c b/src/gallium/drivers/radeonsi/si_blit.c
index 81142d75560..6bc6511adea 100644
--- a/src/gallium/drivers/radeonsi/si_blit.c
+++ b/src/gallium/drivers/radeonsi/si_blit.c
@@ -962,6 +962,13 @@ void si_resource_copy_region(struct pipe_context *ctx, struct pipe_resource *dst
                              src_box, SI_OP_SYNC_BEFORE_AFTER))
       return;
 
+   /* If the blitter isn't available fail here instead of crashing. */
+   if (!sctx->blitter) {
+      fprintf(stderr, "si_resource_copy_region failed src_format: %s dst_format: %s\n",
+              util_format_name(src->format), util_format_name(dst->format));
+      return;
+   }
+
    assert(u_max_sample(dst) == u_max_sample(src));
 
    /* The driver doesn't decompress resources automatically while
diff --git a/src/gallium/drivers/radeonsi/si_buffer.c b/src/gallium/drivers/radeonsi/si_buffer.c
index 85d0ea5dffc..f4dfd27881f 100644
--- a/src/gallium/drivers/radeonsi/si_buffer.c
+++ b/src/gallium/drivers/radeonsi/si_buffer.c
@@ -176,6 +176,15 @@ bool si_alloc_resource(struct si_screen *sscreen, struct si_resource *res)
    util_range_set_empty(&res->valid_buffer_range);
    res->TC_L2_dirty = false;
 
+   if (res->b.b.target != PIPE_BUFFER && !(res->b.b.flags & SI_RESOURCE_AUX_PLANE)) {
+      /* The buffer is shared with other planes. */
+      struct si_resource *plane = (struct si_resource *)res->b.b.next;
+      for (; plane; plane = (struct si_resource *)plane->b.b.next) {
+         radeon_bo_reference(sscreen->ws, &plane->buf, res->buf);
+         plane->gpu_address = res->gpu_address;
+      }
+   }
+
    /* Print debug information. */
    if (sscreen->debug_flags & DBG(VM) && res->b.b.target == PIPE_BUFFER) {
       fprintf(stderr, "VM start=0x%" PRIX64 "  end=0x%" PRIX64 " | Buffer %" PRIu64 " bytes | Flags: ",
diff --git a/src/gallium/drivers/radeonsi/si_compute_blit.c b/src/gallium/drivers/radeonsi/si_compute_blit.c
index cdcf255b187..66dee97c643 100644
--- a/src/gallium/drivers/radeonsi/si_compute_blit.c
+++ b/src/gallium/drivers/radeonsi/si_compute_blit.c
@@ -659,12 +659,30 @@ bool si_compute_copy_image(struct si_context *sctx, struct pipe_resource *dst, u
     */
    if (!util_format_is_compressed(src->format) &&
        !util_format_is_compressed(dst->format) &&
-       !util_format_is_subsampled_422(src->format) &&
-       (!si_can_use_compute_blit(sctx, dst->format, dst->nr_samples, true,
-                                 vi_dcc_enabled(sdst, dst_level)) ||
-        !si_can_use_compute_blit(sctx, src->format, src->nr_samples, false,
-                                 vi_dcc_enabled(ssrc, src_level))))
-      return false;
+       !util_format_is_subsampled_422(src->format)) {
+      bool src_can_use_compute_blit =
+         si_can_use_compute_blit(sctx, src->format, src->nr_samples, false,
+                                 vi_dcc_enabled(ssrc, src_level));
+
+      if (!src_can_use_compute_blit)
+         return false;
+
+      bool dst_can_use_compute_blit =
+         si_can_use_compute_blit(sctx, dst->format, dst->nr_samples, true,
+                                 vi_dcc_enabled(sdst, dst_level));
+
+      if (!dst_can_use_compute_blit && !sctx->has_graphics &&
+          si_can_use_compute_blit(sctx, dst->format, dst->nr_samples, false,
+                                  vi_dcc_enabled(sdst, dst_level))) {
+         /* Non-graphics context don't have a blitter, so try harder to do
+          * a compute blit by disabling dcc on the destination texture.
+          */
+         dst_can_use_compute_blit = si_texture_disable_dcc(sctx, sdst);
+      }
+
+      if (!dst_can_use_compute_blit)
+         return false;
+   }
 
    enum pipe_format src_format = util_format_linear(src->format);
    enum pipe_format dst_format = util_format_linear(dst->format);
diff --git a/src/gallium/drivers/radeonsi/si_get.c b/src/gallium/drivers/radeonsi/si_get.c
index 93c9606fc62..a8926026455 100644
--- a/src/gallium/drivers/radeonsi/si_get.c
+++ b/src/gallium/drivers/radeonsi/si_get.c
@@ -838,6 +838,18 @@ static int si_get_video_param(struct pipe_screen *screen, enum pipe_video_profil
          }
          else
             return 0;
+      case PIPE_VIDEO_CAP_ENC_SURFACE_ALIGNMENT:
+           if (profile == PIPE_VIDEO_PROFILE_HEVC_MAIN ||
+               profile == PIPE_VIDEO_PROFILE_HEVC_MAIN_10) {
+            union pipe_enc_cap_surface_alignment attrib;
+            attrib.value = 0;
+
+            attrib.bits.log2_width_alignment = RADEON_ENC_HEVC_SURFACE_LOG2_WIDTH_ALIGNMENT;
+            attrib.bits.log2_height_alignment = RADEON_ENC_HEVC_SURFACE_LOG2_HEIGHT_ALIGNMENT;
+            return attrib.value;
+         }
+         else
+            return 0;
 
       default:
          return 0;
diff --git a/src/gallium/drivers/radeonsi/si_shader.c b/src/gallium/drivers/radeonsi/si_shader.c
index 4eed0367932..e40a15dac67 100644
--- a/src/gallium/drivers/radeonsi/si_shader.c
+++ b/src/gallium/drivers/radeonsi/si_shader.c
@@ -1770,6 +1770,11 @@ static bool kill_ps_outputs_cb(struct nir_builder *b, nir_instr *instr, void *_k
    assert(nir_intrinsic_component(intr) == 0);
    unsigned cb_shader_mask = ac_get_cb_shader_mask(key->ps.part.epilog.spi_shader_col_format);
 
+   /* Preserve alpha if ALPHA_TESTING is enabled. */
+   if (key->ps.part.epilog.alpha_func != PIPE_FUNC_ALWAYS ||
+       key->ps.part.epilog.alpha_to_coverage_via_mrtz)
+      cb_shader_mask |= 1 << 3;
+
    /* If COLOR is broadcasted to multiple color buffers, combine their masks. */
    if (location == FRAG_RESULT_COLOR) {
       for (unsigned i = 1; i <= key->ps.part.epilog.last_cbuf; i++)
diff --git a/src/gallium/drivers/radeonsi/si_shader_llvm.c b/src/gallium/drivers/radeonsi/si_shader_llvm.c
index 77b9c024974..6f9821c233a 100644
--- a/src/gallium/drivers/radeonsi/si_shader_llvm.c
+++ b/src/gallium/drivers/radeonsi/si_shader_llvm.c
@@ -239,7 +239,6 @@ void si_llvm_optimize_module(struct si_shader_context *ctx)
 
    /* Run the pass */
    LLVMRunPassManager(ctx->compiler->passmgr, ctx->ac.module);
-   LLVMDisposeBuilder(ctx->ac.builder);
 }
 
 void si_llvm_dispose(struct si_shader_context *ctx)
diff --git a/src/gallium/drivers/radeonsi/si_shader_nir.c b/src/gallium/drivers/radeonsi/si_shader_nir.c
index 027d9c410b8..3b03b3eaecb 100644
--- a/src/gallium/drivers/radeonsi/si_shader_nir.c
+++ b/src/gallium/drivers/radeonsi/si_shader_nir.c
@@ -208,7 +208,7 @@ static void si_late_optimize_16bit_samplers(struct si_screen *sscreen, nir_shade
       },
    };
    struct nir_fold_16bit_tex_image_options fold_16bit_options = {
-      .rounding_mode = nir_rounding_mode_rtz,
+      .rounding_mode = nir_rounding_mode_undef,
       .fold_tex_dest_types = nir_type_float,
       .fold_image_dest_types = nir_type_float,
       .fold_image_store_data = true,
diff --git a/src/gallium/drivers/radeonsi/si_shaderlib_nir.c b/src/gallium/drivers/radeonsi/si_shaderlib_nir.c
index 1ab8a0a5a65..51375878004 100644
--- a/src/gallium/drivers/radeonsi/si_shaderlib_nir.c
+++ b/src/gallium/drivers/radeonsi/si_shaderlib_nir.c
@@ -697,13 +697,15 @@ void *si_create_dma_compute_shader(struct si_context *sctx, unsigned num_dwords_
     * the 2nd store writes into 1 * wavesize + tid,
     * the 3rd store writes into 2 * wavesize + tid, etc.
     */
-   nir_def *store_address = get_global_ids(&b, 1);
+   nir_def *store_address =
+      nir_iadd(&b, nir_imul_imm(&b, nir_channel(&b, nir_load_workgroup_id(&b), 0),
+                                default_wave_size * num_mem_ops),
+               nir_channel(&b, nir_load_local_invocation_id(&b), 0));
 
    /* Convert from a "store size unit" into bytes. */
    store_address = nir_imul_imm(&b, store_address, 4 * inst_dwords[0]);
 
-   nir_def *load_address = store_address, *value, *values[num_mem_ops];
-   value = nir_undef(&b, 1, 32);
+   nir_def *load_address = store_address, *value = NULL, *values[num_mem_ops];
 
    if (is_copy) {
       b.shader->info.num_ssbos++;
@@ -723,7 +725,7 @@ void *si_create_dma_compute_shader(struct si_context *sctx, unsigned num_dwords_
             load_address = nir_iadd(&b, load_address,
                                     nir_imm_int(&b, 4 * inst_dwords[i] * default_wave_size));
          }
-         values[i] = nir_load_ssbo(&b, 4, 32, nir_imm_int(&b, 1),load_address,
+         values[i] = nir_load_ssbo(&b, inst_dwords[i], 32, nir_imm_int(&b, 1), load_address,
                                    .access = load_qualifier);
       }
 
diff --git a/src/gallium/drivers/radeonsi/si_state.c b/src/gallium/drivers/radeonsi/si_state.c
index 47485a23feb..7075acfba2d 100644
--- a/src/gallium/drivers/radeonsi/si_state.c
+++ b/src/gallium/drivers/radeonsi/si_state.c
@@ -6286,11 +6286,10 @@ static void gfx10_init_gfx_preamble_state(struct si_context *sctx)
 
    /* Compute registers. */
    si_pm4_set_reg(pm4, R_00B834_COMPUTE_PGM_HI, S_00B834_DATA(sscreen->info.address32_hi >> 8));
-   si_pm4_set_reg(pm4, R_00B858_COMPUTE_STATIC_THREAD_MGMT_SE0, compute_cu_en);
-   si_pm4_set_reg(pm4, R_00B85C_COMPUTE_STATIC_THREAD_MGMT_SE1, compute_cu_en);
 
-   si_pm4_set_reg(pm4, R_00B864_COMPUTE_STATIC_THREAD_MGMT_SE2, compute_cu_en);
-   si_pm4_set_reg(pm4, R_00B868_COMPUTE_STATIC_THREAD_MGMT_SE3, compute_cu_en);
+   for (unsigned i = 0; i < 4; ++i)
+      si_pm4_set_reg(pm4, R_00B858_COMPUTE_STATIC_THREAD_MGMT_SE0 + i * 4,
+                     i < sscreen->info.max_se ? compute_cu_en : 0x0);
 
    si_pm4_set_reg(pm4, R_00B890_COMPUTE_USER_ACCUM_0, 0);
    si_pm4_set_reg(pm4, R_00B894_COMPUTE_USER_ACCUM_1, 0);
@@ -6298,10 +6297,9 @@ static void gfx10_init_gfx_preamble_state(struct si_context *sctx)
    si_pm4_set_reg(pm4, R_00B89C_COMPUTE_USER_ACCUM_3, 0);
 
    if (sctx->gfx_level >= GFX11) {
-      si_pm4_set_reg(pm4, R_00B8AC_COMPUTE_STATIC_THREAD_MGMT_SE4, compute_cu_en);
-      si_pm4_set_reg(pm4, R_00B8B0_COMPUTE_STATIC_THREAD_MGMT_SE5, compute_cu_en);
-      si_pm4_set_reg(pm4, R_00B8B4_COMPUTE_STATIC_THREAD_MGMT_SE6, compute_cu_en);
-      si_pm4_set_reg(pm4, R_00B8B8_COMPUTE_STATIC_THREAD_MGMT_SE7, compute_cu_en);
+      for (unsigned i = 4; i < 8; ++i)
+         si_pm4_set_reg(pm4, R_00B8AC_COMPUTE_STATIC_THREAD_MGMT_SE4 + (i - 4) * 4,
+                        i < sscreen->info.max_se ? compute_cu_en : 0x0);
 
       /* How many threads should go to 1 SE before moving onto the next. Think of GL1 cache hits.
        * Only these values are valid: 0 (disabled), 64, 128, 256, 512
@@ -6392,6 +6390,7 @@ static void gfx10_init_gfx_preamble_state(struct si_context *sctx)
                   (sctx->gfx_level >= GFX11 ?
                       S_028410_DCC_WR_POLICY_GFX11(meta_write_policy) |
                       S_028410_COLOR_WR_POLICY_GFX11(V_028410_CACHE_STREAM) |
+                      S_028410_DCC_RD_POLICY(meta_read_policy) |
                       S_028410_COLOR_RD_POLICY(V_028410_CACHE_NOA_GFX11)
                     :
                       S_028410_CMASK_WR_POLICY(meta_write_policy) |
@@ -6401,7 +6400,7 @@ static void gfx10_init_gfx_preamble_state(struct si_context *sctx)
                       S_028410_CMASK_RD_POLICY(meta_read_policy) |
                       S_028410_FMASK_RD_POLICY(V_028410_CACHE_NOA_GFX10) |
                       S_028410_COLOR_RD_POLICY(V_028410_CACHE_NOA_GFX10)) |
-                  S_028410_DCC_RD_POLICY(meta_read_policy));
+                      S_028410_DCC_RD_POLICY(meta_read_policy));
    si_pm4_set_reg(pm4, R_028708_SPI_SHADER_IDX_FORMAT,
                   S_028708_IDX0_EXPORT_FORMAT(V_028708_SPI_SHADER_1COMP));
 
@@ -6445,8 +6444,10 @@ static void gfx10_init_gfx_preamble_state(struct si_context *sctx)
                      S_028B50_DONUT_SPLIT_GFX9(24) |
                      S_028B50_TRAP_SPLIT(6));
 
+   /* GFX11+ shouldn't subtract 1 from pbb_max_alloc_count.  */
+   unsigned gfx10_one = sctx->gfx_level < GFX11;
    si_pm4_set_reg(pm4, R_028C48_PA_SC_BINNER_CNTL_1,
-                  S_028C48_MAX_ALLOC_COUNT(sscreen->info.pbb_max_alloc_count - 1) |
+                  S_028C48_MAX_ALLOC_COUNT(sscreen->info.pbb_max_alloc_count - gfx10_one) |
                   S_028C48_MAX_PRIM_PER_BATCH(1023));
 
    if (sctx->gfx_level >= GFX11_5)
diff --git a/src/gallium/drivers/radeonsi/si_state.h b/src/gallium/drivers/radeonsi/si_state.h
index 096f94cee39..c2e9e9d7d79 100644
--- a/src/gallium/drivers/radeonsi/si_state.h
+++ b/src/gallium/drivers/radeonsi/si_state.h
@@ -342,7 +342,7 @@ enum si_tracked_reg
 
    /* The slots below can be reused by other generations. */
    SI_TRACKED_VGT_ESGS_RING_ITEMSIZE,        /* GFX6-8 (GFX9+ can reuse this slot) */
-   SI_TRACKED_VGT_REUSE_OFF,                 /* GFX6-8 (GFX9+ can reuse this slot) */
+   SI_TRACKED_VGT_REUSE_OFF,                 /* GFX6-8,10.3 */
    SI_TRACKED_IA_MULTI_VGT_PARAM,            /* GFX6-8 (GFX9+ can reuse this slot) */
 
    SI_TRACKED_VGT_GS_MAX_PRIMS_PER_SUBGROUP, /* GFX9 - the slots above can be reused */
diff --git a/src/gallium/drivers/radeonsi/si_state_binning.c b/src/gallium/drivers/radeonsi/si_state_binning.c
index 39a97ebd0ce..984586eb864 100644
--- a/src/gallium/drivers/radeonsi/si_state_binning.c
+++ b/src/gallium/drivers/radeonsi/si_state_binning.c
@@ -393,6 +393,9 @@ static void si_emit_dpbb_disable(struct si_context *sctx)
    if (sctx->gfx_level >= GFX10) {
       struct uvec2 bin_size = {};
       struct uvec2 bin_size_extend = {};
+      unsigned binning_disabled =
+         sctx->gfx_level >= GFX11_5 ? V_028C44_BINNING_DISABLED
+                                    : V_028C44_DISABLE_BINNING_USE_NEW_SC;
 
       bin_size.x = 128;
       bin_size.y = sctx->framebuffer.min_bytes_per_pixel <= 4 ? 128 : 64;
@@ -404,7 +407,7 @@ static void si_emit_dpbb_disable(struct si_context *sctx)
 
       radeon_opt_set_context_reg(sctx, R_028C44_PA_SC_BINNER_CNTL_0,
                                  SI_TRACKED_PA_SC_BINNER_CNTL_0,
-                                 S_028C44_BINNING_MODE(V_028C44_DISABLE_BINNING_USE_NEW_SC) |
+                                 S_028C44_BINNING_MODE(binning_disabled) |
                                  S_028C44_BIN_SIZE_X(bin_size.x == 16) |
                                  S_028C44_BIN_SIZE_Y(bin_size.y == 16) |
                                  S_028C44_BIN_SIZE_X_EXTEND(bin_size_extend.x) |
diff --git a/src/gallium/drivers/radeonsi/si_state_shaders.cpp b/src/gallium/drivers/radeonsi/si_state_shaders.cpp
index 9faa8693b3b..11f483aef58 100644
--- a/src/gallium/drivers/radeonsi/si_state_shaders.cpp
+++ b/src/gallium/drivers/radeonsi/si_state_shaders.cpp
@@ -3856,6 +3856,8 @@ static void si_destroy_shader_selector(struct pipe_context *ctx, void *cso)
       si_delete_shader(sctx, sel->main_shader_part_es);
    if (sel->main_shader_part_ngg)
       si_delete_shader(sctx, sel->main_shader_part_ngg);
+   if (sel->main_shader_part_ngg_es)
+      si_delete_shader(sctx, sel->main_shader_part_ngg_es);
 
    free(sel->keys);
    free(sel->variants);
@@ -4343,6 +4345,15 @@ static void si_emit_vgt_pipeline_state(struct si_context *sctx, unsigned index)
    radeon_begin(cs);
    radeon_opt_set_context_reg(sctx, R_028B54_VGT_SHADER_STAGES_EN, SI_TRACKED_VGT_SHADER_STAGES_EN,
                               sctx->vgt_shader_stages_en);
+   if (sctx->gfx_level == GFX10_3) {
+      /* Legacy Tess+GS should disable reuse to prevent hangs on GFX10.3. */
+      bool has_legacy_tess_gs = G_028B54_HS_EN(sctx->vgt_shader_stages_en) &&
+                                G_028B54_GS_EN(sctx->vgt_shader_stages_en) &&
+                                !G_028B54_PRIMGEN_EN(sctx->vgt_shader_stages_en); /* !NGG */
+
+      radeon_opt_set_context_reg(sctx, R_028AB4_VGT_REUSE_OFF, SI_TRACKED_VGT_REUSE_OFF,
+                                 S_028AB4_REUSE_OFF(has_legacy_tess_gs));
+   }
    radeon_end_update_context_roll(sctx);
 
    if (sctx->gfx_level >= GFX10) {
diff --git a/src/gallium/drivers/svga/svga_draw.c b/src/gallium/drivers/svga/svga_draw.c
index 16865ba06c3..fdf498cda7e 100644
--- a/src/gallium/drivers/svga/svga_draw.c
+++ b/src/gallium/drivers/svga/svga_draw.c
@@ -1006,6 +1006,7 @@ draw_vgpu10(struct svga_hwtnl *hwtnl,
    struct svga_context *svga = hwtnl->svga;
    struct svga_winsys_surface *indirect_handle;
    enum pipe_error ret;
+   bool is_instanced_draw = instance_count > 1 || start_instance > 0;
 
    assert(svga_have_vgpu10(svga));
    assert(hwtnl->cmd.prim_count == 0);
@@ -1096,7 +1097,7 @@ draw_vgpu10(struct svga_hwtnl *hwtnl,
                                                        indirect_handle,
                                                        indirect->offset);
       }
-      else if (instance_count > 1) {
+      else if (is_instanced_draw) {
          ret = SVGA3D_vgpu10_DrawIndexedInstanced(svga->swc,
                                                   vcount,
                                                   instance_count,
@@ -1139,7 +1140,7 @@ draw_vgpu10(struct svga_hwtnl *hwtnl,
                                                 indirect_handle,
                                                 indirect->offset);
       }
-      else if (instance_count > 1) {
+      else if (is_instanced_draw) {
          ret = SVGA3D_vgpu10_DrawInstanced(svga->swc,
                                            vcount,
                                            instance_count,
diff --git a/src/gallium/drivers/v3d/v3d_cl.c b/src/gallium/drivers/v3d/v3d_cl.c
index d8ee4ffc206..42cfd6282bf 100644
--- a/src/gallium/drivers/v3d/v3d_cl.c
+++ b/src/gallium/drivers/v3d/v3d_cl.c
@@ -32,6 +32,16 @@
 #include "broadcom/common/v3d_macros.h"
 #include "broadcom/cle/v3dx_pack.h"
 
+/* The Control List Executor (CLE) pre-fetches V3D_CLE_READAHEAD bytes from
+ * the Control List buffer. The usage of these last bytes should be avoided or
+ * the CLE would pre-fetch the data after the end of the CL buffer, reporting
+ * the kernel "MMU error from client CLE".
+ */
+#define V3D42_CLE_READAHEAD 256u
+#define V3D42_CLE_BUFFER_MIN_SIZE 4096u
+#define V3D71_CLE_READAHEAD 1024u
+#define V3D71_CLE_BUFFER_MIN_SIZE 16384u
+
 void
 v3d_init_cl(struct v3d_job *job, struct v3d_cl *cl)
 {
@@ -50,9 +60,12 @@ v3d_cl_ensure_space(struct v3d_cl *cl, uint32_t space, uint32_t alignment)
                 cl->next = cl->base + offset;
                 return offset;
         }
-
+        struct v3d_device_info *devinfo = &cl->job->v3d->screen->devinfo;
+        uint32_t cle_buffer_min_size = V3DV_X(devinfo, CLE_BUFFER_MIN_SIZE);
         v3d_bo_unreference(&cl->bo);
-        cl->bo = v3d_bo_alloc(cl->job->v3d->screen, align(space, 4096), "CL");
+        cl->bo = v3d_bo_alloc(cl->job->v3d->screen,
+                              align(space, cle_buffer_min_size),
+                              "CL");
         cl->base = v3d_bo_map(cl->bo);
         cl->size = cl->bo->size;
         cl->next = cl->base;
@@ -63,14 +76,30 @@ v3d_cl_ensure_space(struct v3d_cl *cl, uint32_t space, uint32_t alignment)
 void
 v3d_cl_ensure_space_with_branch(struct v3d_cl *cl, uint32_t space)
 {
-        if (cl_offset(cl) + space + cl_packet_length(BRANCH) <= cl->size)
+        if (cl_offset(cl) + space  <= cl->size)
                 return;
 
-        struct v3d_bo *new_bo = v3d_bo_alloc(cl->job->v3d->screen, space, "CL");
-        assert(space <= new_bo->size);
+        /* The last V3D_CLE_READAHEAD bytes of the buffer are unusable, so we
+         * need to take them into account when allocating a new BO for the
+         * CL. We have to be sure that we have room for a BRANCH packet so we
+         * can always chain a next BO if needed. We will need to increase
+         * cl->size by the packet length before calling cl_summit to use this
+         * reserved space.
+         */
+        struct v3d_device_info *devinfo = &cl->job->v3d->screen->devinfo;
+        uint32_t cle_readahead = V3DV_X(devinfo, CLE_READAHEAD);
+        uint32_t cle_buffer_min_size = V3DV_X(devinfo, CLE_BUFFER_MIN_SIZE);
+        uint32_t unusable_size = cle_readahead + cl_packet_length(BRANCH);
+        struct v3d_bo *new_bo = v3d_bo_alloc(cl->job->v3d->screen,
+                                             align(space + unusable_size,
+                                                   cle_buffer_min_size),
+                                             "CL");
+        assert(space + unusable_size <= new_bo->size);
 
         /* Chain to the new BO from the old one. */
         if (cl->bo) {
+                cl->size += cl_packet_length(BRANCH);
+                assert(cl->size + cle_readahead <= cl->bo->size);
                 cl_emit(cl, BRANCH, branch) {
                         branch.address = cl_address(new_bo, 0);
                 }
@@ -82,7 +111,11 @@ v3d_cl_ensure_space_with_branch(struct v3d_cl *cl, uint32_t space)
 
         cl->bo = new_bo;
         cl->base = v3d_bo_map(cl->bo);
-        cl->size = cl->bo->size;
+        /* Take only into account the usable size of the BO to guarantee that
+         * we never write in the last bytes of the CL buffer because of the
+         * readahead of the CLE
+         */
+        cl->size = cl->bo->size - unusable_size;
         cl->next = cl->base;
 }
 
diff --git a/src/gallium/drivers/v3d/v3d_cl.h b/src/gallium/drivers/v3d/v3d_cl.h
index de966d2baad..76d8c3aa300 100644
--- a/src/gallium/drivers/v3d/v3d_cl.h
+++ b/src/gallium/drivers/v3d/v3d_cl.h
@@ -234,6 +234,7 @@ cl_get_emit_space(struct v3d_cl_out **cl, size_t size)
                 cl_advance(&cl_out, cl_packet_length(packet));   \
                 cl_end(cl, cl_out);                              \
                 _loop_terminate = NULL;                          \
+                assert(cl_offset(cl) <= (cl)->size);             \
         }))                                                      \
 
 #define cl_emit_with_prepacked(cl, packet, prepacked, name)      \
@@ -253,9 +254,10 @@ cl_get_emit_space(struct v3d_cl_out **cl, size_t size)
                 _loop_terminate = NULL;                          \
         }))                                                      \
 
-#define cl_emit_prepacked_sized(cl, packet, size) do {                \
-        memcpy((cl)->next, packet, size);             \
-        cl_advance(&(cl)->next, size);                \
+#define cl_emit_prepacked_sized(cl, packet, psize) do {          \
+        memcpy((cl)->next, packet, psize);                       \
+        cl_advance(&(cl)->next, psize);                          \
+        assert(cl_offset(cl) <= (cl)->size);                     \
 } while (0)
 
 #define cl_emit_prepacked(cl, packet) \
diff --git a/src/gallium/drivers/v3d/v3dx_draw.c b/src/gallium/drivers/v3d/v3dx_draw.c
index feba9080ec2..128104c213b 100644
--- a/src/gallium/drivers/v3d/v3dx_draw.c
+++ b/src/gallium/drivers/v3d/v3dx_draw.c
@@ -709,6 +709,9 @@ v3d_emit_gl_shader_state(struct v3d_context *v3d,
         }
 
         bool cs_loaded_any = false;
+        const bool cs_uses_builtins = v3d->prog.cs->prog_data.vs->uses_iid ||
+                                      v3d->prog.cs->prog_data.vs->uses_biid ||
+                                      v3d->prog.cs->prog_data.vs->uses_vid;
         for (int i = 0; i < vtx->num_elements; i++) {
                 struct pipe_vertex_element *elem = &vtx->pipe[i];
                 struct pipe_vertex_buffer *vb =
@@ -738,11 +741,18 @@ v3d_emit_gl_shader_state(struct v3d_context *v3d,
                          * inputs.  (Since CS is just dead-code-elimination
                          * compared to VS, we can't have CS loading but not
                          * VS).
+                         *
+                         * GFXH-1602: first attribute must be active if using
+                         * builtins.
                          */
                         if (v3d->prog.cs->prog_data.vs->vattr_sizes[i])
                                 cs_loaded_any = true;
-                        if (i == vtx->num_elements - 1 && !cs_loaded_any) {
+                        if (i == 0 && cs_uses_builtins && !cs_loaded_any) {
+                                attr.number_of_values_read_by_coordinate_shader = 1;
+                                cs_loaded_any = true;
+                        } else if (i == vtx->num_elements - 1 && !cs_loaded_any) {
                                 attr.number_of_values_read_by_coordinate_shader = 1;
+                                cs_loaded_any = true;
                         }
                         attr.maximum_index = 0xffffff;
                 }
@@ -1390,7 +1400,7 @@ v3d_launch_grid(struct pipe_context *pctx, const struct pipe_grid_info *info)
                 v3d->compute_shared_memory =
                         v3d_bo_alloc(v3d->screen,
                                      v3d->prog.compute->prog_data.compute->shared_size *
-                                     wgs_per_sg,
+                                     num_wgs,
                                      "shared_vars");
         }
 
diff --git a/src/gallium/drivers/virgl/ci/gitlab-ci-inc.yml b/src/gallium/drivers/virgl/ci/gitlab-ci-inc.yml
index 7326998c325..61b56188ed4 100644
--- a/src/gallium/drivers/virgl/ci/gitlab-ci-inc.yml
+++ b/src/gallium/drivers/virgl/ci/gitlab-ci-inc.yml
@@ -67,7 +67,7 @@
 .virgl-iris-test:
   extends:
     - .lava-piglit-traces:x86_64
-    - .lava-asus-cx9400-volteer:x86_64
+    - .lava-acer-cp514-2h-1160g7-volteer:x86_64
   variables:
     HWCI_KERNEL_MODULES: vhost_vsock
     HWCI_KVM: "true"
diff --git a/src/gallium/drivers/virgl/ci/traces-virgl.yml b/src/gallium/drivers/virgl/ci/traces-virgl.yml
index bf5fcc8539b..44f04175082 100644
--- a/src/gallium/drivers/virgl/ci/traces-virgl.yml
+++ b/src/gallium/drivers/virgl/ci/traces-virgl.yml
@@ -12,7 +12,7 @@ traces:
       checksum: 57ddd36b117adc9216c65c10d914a37e
   gputest/pixmark-piano-v2.trace:
     gl-virgl:
-      checksum: cbe50265c2d1a114fd75bf12407fbad9
+      checksum: 3b760606c18aebda1ad0eff6eb03203a
   gputest/triangle-v2.trace:
     gl-virgl:
       checksum: 7812de00011a3a059892e36cea19c696
diff --git a/src/gallium/drivers/zink/ci/gitlab-ci-inc.yml b/src/gallium/drivers/zink/ci/gitlab-ci-inc.yml
index c54a6bc7dee..ed4ed87566e 100644
--- a/src/gallium/drivers/zink/ci/gitlab-ci-inc.yml
+++ b/src/gallium/drivers/zink/ci/gitlab-ci-inc.yml
@@ -121,7 +121,8 @@
 
 .zink-anv-test:
   extends:
-    - .anv-tgl-test
+    - .lava-acer-cp514-2h-1160g7-volteer:x86_64
+    - .anv-test
     - .zink-anv-rules
     - .zink-test
   variables:
diff --git a/src/gallium/drivers/zink/ci/traces-zink-restricted.yml b/src/gallium/drivers/zink/ci/traces-zink-restricted.yml
index c45b40947f3..37eff63598a 100644
--- a/src/gallium/drivers/zink/ci/traces-zink-restricted.yml
+++ b/src/gallium/drivers/zink/ci/traces-zink-restricted.yml
@@ -60,7 +60,7 @@ traces:
       text: https://gitlab.freedesktop.org/mesa/mesa/-/issues/8986
   TheRavenRemastered/Raven-f10900-v2.trace:
     gl-zink-anv-tgl:
-      checksum: db7b901177e7ac00cc489e0e13f71f76
+      checksum: e910141d9520739c653fa7de0d8a1c9b
   TombRaider2013/TombRaider-f1430-v2.trace:
     gl-zink-anv-tgl:
       label: [crash]
diff --git a/src/gallium/drivers/zink/ci/traces-zink.yml b/src/gallium/drivers/zink/ci/traces-zink.yml
index b1af198f019..0882be78295 100644
--- a/src/gallium/drivers/zink/ci/traces-zink.yml
+++ b/src/gallium/drivers/zink/ci/traces-zink.yml
@@ -30,7 +30,7 @@ traces:
       checksum: 433b69bea68cfe81914b857bbdc60ea5
   gputest/pixmark-piano-v2.trace:
     gl-zink-anv-tgl:
-      checksum: 4c7afcce5d87ec2bced65e92a1c9a41c
+      checksum: 9e7b3f2d38e6cea705af8161cfd41465
   gputest/triangle-v2.trace:
     gl-zink-anv-tgl:
       checksum: 5f694874b15bcd7a3689b387c143590b
diff --git a/src/gallium/drivers/zink/ci/zink-anv-tgl-fails.txt b/src/gallium/drivers/zink/ci/zink-anv-tgl-fails.txt
index f683854124e..1f4373e869d 100644
--- a/src/gallium/drivers/zink/ci/zink-anv-tgl-fails.txt
+++ b/src/gallium/drivers/zink/ci/zink-anv-tgl-fails.txt
@@ -96,10 +96,6 @@ glx@glx_arb_create_context_es2_profile@invalid opengl es version,Fail
 glx@glx_arb_create_context_no_error@no error,Fail
 glx@glx_arb_create_context_robustness@invalid reset notification strategy,Fail
 
-# See also the EGL buffer age failures
-glx@glx-buffer-age,Fail
-glx@glx-buffer-age vblank_mode=0,Fail
-
 glx@glx-swap-pixmap-bad,Fail
 
 # ../src/gallium/drivers/zink/zink_kopper.c:859: zink_kopper_update: Assertion `pres->bind & PIPE_BIND_DISPLAY_TARGET' failed.
@@ -156,10 +152,6 @@ spec@!opengl 2.1@polygon-stipple-fs,Fail
 
 spec@!opengl 3.0@clearbuffer-depth-cs-probe,Fail
 
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-mixed-const-non-const-uniform-index,Fail
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-mixed-const-non-const-uniform-index2,Fail
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-non-const-uniform-index,Fail
-
 spec@arb_framebuffer_object@fbo-blit-scaled-linear,Fail
 
 
@@ -530,10 +522,6 @@ spec@arb_sample_shading@samplemask 8@sample mask_in_one,Fail
 
 spec@arb_shader_image_load_store@early-z,Fail
 spec@arb_shader_image_load_store@early-z@occlusion query test/early-z pass,Fail
-spec@arb_shader_image_load_store@host-mem-barrier,Fail
-spec@arb_shader_image_load_store@host-mem-barrier@Transform feedback/WaW/one bit barrier test/16x16,Fail
-spec@arb_shader_image_load_store@host-mem-barrier@Transform feedback/WaW/one bit barrier test/4x4,Fail
-spec@arb_shader_image_load_store@host-mem-barrier@Transform feedback/WaW/one bit barrier test/64x64,Fail
 
 spec@arb_shader_texture_lod@execution@arb_shader_texture_lod-texgradcube,Fail
 
@@ -680,3 +668,8 @@ KHR-GL46.sparse_texture_tests.SparseTextureCommitment,Crash
 # Assertion `size % ZINK_SPARSE_BUFFER_PAGE_SIZE == 0 || offset + size == bo->base.size' failed.
 spec@arb_sparse_buffer@basic,Crash
 spec@arb_sparse_buffer@buffer-data,Crash
+
+# Failing on a bunch of drivers with an assert on the weak_ref
+# pipeline cache, seems to be a zink issue not destroying a
+# shader/pipeline before calling vkDestroyPipeline()
+spec@ext_external_objects@vk-vert-buf-reuse,Crash
diff --git a/src/gallium/drivers/zink/ci/zink-lvp-fails.txt b/src/gallium/drivers/zink/ci/zink-lvp-fails.txt
index dfb56f0c41b..5a1e8d3c5bf 100644
--- a/src/gallium/drivers/zink/ci/zink-lvp-fails.txt
+++ b/src/gallium/drivers/zink/ci/zink-lvp-fails.txt
@@ -143,10 +143,6 @@ spec@!opengl 1.0@rasterpos,Fail
 spec@!opengl 1.0@rasterpos@glsl_vs_gs_linked,Fail
 spec@!opengl 1.0@rasterpos@glsl_vs_tes_linked,Fail
 
-
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-mixed-const-non-const-uniform-index,Fail
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-mixed-const-non-const-uniform-index2,Fail
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-non-const-uniform-index,Fail
 spec@arb_gpu_shader_fp64@execution@conversion@frag-conversion-explicit-dmat2-mat2,Fail
 spec@arb_gpu_shader_fp64@execution@conversion@frag-conversion-explicit-dmat2x3-mat2x3,Fail
 spec@arb_gpu_shader_fp64@execution@conversion@frag-conversion-explicit-dmat2x4-mat2x4,Fail
diff --git a/src/gallium/drivers/zink/ci/zink-radv-navi10-fails.txt b/src/gallium/drivers/zink/ci/zink-radv-navi10-fails.txt
index 248257a7d79..c04e59251c6 100644
--- a/src/gallium/drivers/zink/ci/zink-radv-navi10-fails.txt
+++ b/src/gallium/drivers/zink/ci/zink-radv-navi10-fails.txt
@@ -13,13 +13,8 @@ glx@extension string sanity,Fail
 # #6322
 spec@arb_framebuffer_object@fbo-blit-scaled-linear,Fail
 
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-mixed-const-non-const-uniform-index2,Fail
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-mixed-const-non-const-uniform-index,Fail
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-non-const-uniform-index,Fail
 spec@arb_bindless_texture@compiler@samplers@arith-bound-sampler-texture2d.frag,Crash
 
-spec@arb_framebuffer_no_attachments@arb_framebuffer_no_attachments-query,Fail
-spec@arb_framebuffer_no_attachments@arb_framebuffer_no_attachments-query@MS8,Fail
 spec@arb_gpu_shader_fp64@execution@arb_gpu_shader_fp64-tf-separate,Fail
 spec@arb_gpu_shader_fp64@execution@conversion@frag-conversion-explicit-dmat2-mat2,Fail
 spec@arb_gpu_shader_fp64@execution@conversion@frag-conversion-explicit-dmat2x3-mat2x3,Fail
@@ -187,62 +182,6 @@ spec@!opengl 3.0@clearbuffer-depth-cs-probe,Fail
 spec@!opengl 1.0@rasterpos@glsl_vs_gs_linked,Fail
 spec@!opengl 1.0@rasterpos@glsl_vs_tes_linked,Fail
 
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT_10_10_10_2,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT_2_10_10_10_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT_8_8_8_8,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT_8_8_8_8_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_SHORT_4_4_4_4,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_SHORT_4_4_4_4_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT_10_10_10_2,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT_2_10_10_10_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT_8_8_8_8,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT_8_8_8_8_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_SHORT_4_4_4_4,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_SHORT_4_4_4_4_REV,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats-s3tc,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats-s3tc@GL_COMPRESSED_SLUMINANCE_ALPHA,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats-s3tc@GL_COMPRESSED_SLUMINANCE_ALPHA NPOT,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats@GL_SLUMINANCE8_ALPHA8,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats@GL_SLUMINANCE8_ALPHA8 NPOT,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats@GL_SLUMINANCE_ALPHA,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats@GL_SLUMINANCE_ALPHA NPOT,Fail
 spec@ext_framebuffer_blit@fbo-blit-check-limits,Fail
 
 # Introduced with the uprev of piglit (70ce1dcacc92 - "ci: Update piglit with s3 support")
diff --git a/src/gallium/drivers/zink/ci/zink-radv-navi31-fails.txt b/src/gallium/drivers/zink/ci/zink-radv-navi31-fails.txt
index a826438ae27..4a92af95939 100644
--- a/src/gallium/drivers/zink/ci/zink-radv-navi31-fails.txt
+++ b/src/gallium/drivers/zink/ci/zink-radv-navi31-fails.txt
@@ -13,14 +13,8 @@ glx@extension string sanity,Fail
 # #6322
 spec@arb_framebuffer_object@fbo-blit-scaled-linear,Fail
 
-
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-mixed-const-non-const-uniform-index2,Fail
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-mixed-const-non-const-uniform-index,Fail
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-non-const-uniform-index,Fail
 spec@arb_bindless_texture@compiler@samplers@arith-bound-sampler-texture2d.frag,Crash
 
-spec@arb_framebuffer_no_attachments@arb_framebuffer_no_attachments-query,Fail
-spec@arb_framebuffer_no_attachments@arb_framebuffer_no_attachments-query@MS8,Fail
 spec@arb_gpu_shader_fp64@execution@arb_gpu_shader_fp64-tf-separate,Fail
 spec@arb_gpu_shader_fp64@execution@conversion@frag-conversion-explicit-dmat2-mat2,Fail
 spec@arb_gpu_shader_fp64@execution@conversion@frag-conversion-explicit-dmat2x3-mat2x3,Fail
@@ -187,62 +181,6 @@ spec@!opengl 3.0@clearbuffer-depth-cs-probe,Fail
 spec@!opengl 1.0@rasterpos@glsl_vs_gs_linked,Fail
 spec@!opengl 1.0@rasterpos@glsl_vs_tes_linked,Fail
 
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT_10_10_10_2,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT_2_10_10_10_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT_8_8_8_8,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT_8_8_8_8_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_SHORT_4_4_4_4,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_SHORT_4_4_4_4_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT_10_10_10_2,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT_2_10_10_10_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT_8_8_8_8,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT_8_8_8_8_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_SHORT_4_4_4_4,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_SHORT_4_4_4_4_REV,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats-s3tc,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats-s3tc@GL_COMPRESSED_SLUMINANCE_ALPHA,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats-s3tc@GL_COMPRESSED_SLUMINANCE_ALPHA NPOT,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats@GL_SLUMINANCE8_ALPHA8,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats@GL_SLUMINANCE8_ALPHA8 NPOT,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats@GL_SLUMINANCE_ALPHA,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats@GL_SLUMINANCE_ALPHA NPOT,Fail
 spec@ext_framebuffer_blit@fbo-blit-check-limits,Fail
 
 # Introduced with the uprev of piglit (70ce1dcacc92 - "ci: Update piglit with s3 support")
diff --git a/src/gallium/drivers/zink/ci/zink-radv-polaris10-fails.txt b/src/gallium/drivers/zink/ci/zink-radv-polaris10-fails.txt
index 1c4088c67f2..b7fa3416124 100644
--- a/src/gallium/drivers/zink/ci/zink-radv-polaris10-fails.txt
+++ b/src/gallium/drivers/zink/ci/zink-radv-polaris10-fails.txt
@@ -13,13 +13,8 @@ glx@extension string sanity,Fail
 # #6322
 spec@arb_framebuffer_object@fbo-blit-scaled-linear,Fail
 
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-mixed-const-non-const-uniform-index2,Fail
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-mixed-const-non-const-uniform-index,Fail
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-non-const-uniform-index,Fail
 spec@arb_bindless_texture@compiler@samplers@arith-bound-sampler-texture2d.frag,Crash
 
-spec@arb_framebuffer_no_attachments@arb_framebuffer_no_attachments-query,Fail
-spec@arb_framebuffer_no_attachments@arb_framebuffer_no_attachments-query@MS8,Fail
 spec@arb_gpu_shader_fp64@execution@arb_gpu_shader_fp64-tf-separate,Fail
 spec@arb_gpu_shader_fp64@execution@conversion@frag-conversion-explicit-dmat2-mat2,Fail
 spec@arb_gpu_shader_fp64@execution@conversion@frag-conversion-explicit-dmat2x3-mat2x3,Fail
@@ -198,64 +193,8 @@ spec@!opengl 3.0@clearbuffer-depth-cs-probe,Fail
 spec@!opengl 1.0@rasterpos@glsl_vs_gs_linked,Fail
 spec@!opengl 1.0@rasterpos@glsl_vs_tes_linked,Fail
 
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT_10_10_10_2,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT_2_10_10_10_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT_8_8_8_8,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT_8_8_8_8_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_SHORT_4_4_4_4,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_SHORT_4_4_4_4_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT_10_10_10_2,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT_2_10_10_10_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT_8_8_8_8,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT_8_8_8_8_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_SHORT_4_4_4_4,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_SHORT_4_4_4_4_REV,Fail
 spec@ext_image_dma_buf_import@ext_image_dma_buf_import-sample_yuv420,Fail
 spec@ext_image_dma_buf_import@ext_image_dma_buf_import-sample_yvu420,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats-s3tc,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats-s3tc@GL_COMPRESSED_SLUMINANCE_ALPHA,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats-s3tc@GL_COMPRESSED_SLUMINANCE_ALPHA NPOT,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats@GL_SLUMINANCE8_ALPHA8,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats@GL_SLUMINANCE8_ALPHA8 NPOT,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats@GL_SLUMINANCE_ALPHA,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats@GL_SLUMINANCE_ALPHA NPOT,Fail
 spec@ext_framebuffer_blit@fbo-blit-check-limits,Fail
 
 # Introduced with the uprev of piglit (70ce1dcacc92 - "ci: Update piglit with s3 support")
@@ -380,69 +319,11 @@ dEQP-GLES3.functional.texture.specification.texsubimage2d_depth.depth24_stencil8
 dEQP-GLES3.functional.texture.specification.texsubimage2d_depth.depth32f_stencil8,Fail
 dEQP-GLES3.functional.texture.specification.texsubimage3d_depth.depth24_stencil8_2d_array,Fail
 dEQP-GLES3.functional.texture.specification.texsubimage3d_depth.depth32f_stencil8_2d_array,Fail
-dEQP-GLES31.functional.fbo.no_attachments.maximums.all,Fail
-dEQP-GLES31.functional.fbo.no_attachments.maximums.height,Fail
-dEQP-GLES31.functional.fbo.no_attachments.maximums.samples,Fail
-dEQP-GLES31.functional.fbo.no_attachments.maximums.size,Fail
-dEQP-GLES31.functional.fbo.no_attachments.maximums.width,Fail
-dEQP-GLES31.functional.fbo.no_attachments.multisample.samples0,Fail
-dEQP-GLES31.functional.fbo.no_attachments.multisample.samples1,Fail
-dEQP-GLES31.functional.fbo.no_attachments.multisample.samples2,Fail
-dEQP-GLES31.functional.fbo.no_attachments.multisample.samples3,Fail
-dEQP-GLES31.functional.fbo.no_attachments.multisample.samples4,Fail
-dEQP-GLES31.functional.fbo.no_attachments.npot_size.1023x1023,Fail
-dEQP-GLES31.functional.fbo.no_attachments.npot_size.1025x1025,Fail
-dEQP-GLES31.functional.fbo.no_attachments.npot_size.127x127,Fail
-dEQP-GLES31.functional.fbo.no_attachments.npot_size.127x15,Fail
-dEQP-GLES31.functional.fbo.no_attachments.npot_size.129x127,Fail
-dEQP-GLES31.functional.fbo.no_attachments.npot_size.129x129,Fail
 dEQP-GLES31.functional.fbo.no_attachments.npot_size.15x15,Fail
-dEQP-GLES31.functional.fbo.no_attachments.npot_size.15x511,Fail
 dEQP-GLES31.functional.fbo.no_attachments.npot_size.17x17,Fail
 dEQP-GLES31.functional.fbo.no_attachments.npot_size.1x1,Fail
-dEQP-GLES31.functional.fbo.no_attachments.npot_size.2047x1025,Fail
-dEQP-GLES31.functional.fbo.no_attachments.npot_size.2047x2047,Fail
-dEQP-GLES31.functional.fbo.no_attachments.npot_size.255x255,Fail
-dEQP-GLES31.functional.fbo.no_attachments.npot_size.257x257,Fail
 dEQP-GLES31.functional.fbo.no_attachments.npot_size.31x31,Fail
-dEQP-GLES31.functional.fbo.no_attachments.npot_size.33x33,Fail
 dEQP-GLES31.functional.fbo.no_attachments.npot_size.3x3,Fail
-dEQP-GLES31.functional.fbo.no_attachments.npot_size.511x127,Fail
-dEQP-GLES31.functional.fbo.no_attachments.npot_size.511x511,Fail
-dEQP-GLES31.functional.fbo.no_attachments.npot_size.513x513,Fail
-dEQP-GLES31.functional.fbo.no_attachments.npot_size.63x63,Fail
-dEQP-GLES31.functional.fbo.no_attachments.npot_size.65x65,Fail
-dEQP-GLES31.functional.fbo.no_attachments.random.0,Fail
-dEQP-GLES31.functional.fbo.no_attachments.random.1,Fail
-dEQP-GLES31.functional.fbo.no_attachments.random.10,Fail
-dEQP-GLES31.functional.fbo.no_attachments.random.11,Fail
-dEQP-GLES31.functional.fbo.no_attachments.random.12,Fail
-dEQP-GLES31.functional.fbo.no_attachments.random.13,Fail
-dEQP-GLES31.functional.fbo.no_attachments.random.14,Fail
-dEQP-GLES31.functional.fbo.no_attachments.random.15,Fail
-dEQP-GLES31.functional.fbo.no_attachments.random.2,Fail
-dEQP-GLES31.functional.fbo.no_attachments.random.3,Fail
-dEQP-GLES31.functional.fbo.no_attachments.random.4,Fail
-dEQP-GLES31.functional.fbo.no_attachments.random.5,Fail
-dEQP-GLES31.functional.fbo.no_attachments.random.6,Fail
-dEQP-GLES31.functional.fbo.no_attachments.random.7,Fail
-dEQP-GLES31.functional.fbo.no_attachments.random.8,Fail
-dEQP-GLES31.functional.fbo.no_attachments.random.9,Fail
-dEQP-GLES31.functional.fbo.no_attachments.size.1024x1024,Fail
-dEQP-GLES31.functional.fbo.no_attachments.size.1024x16,Fail
-dEQP-GLES31.functional.fbo.no_attachments.size.1024x256,Fail
-dEQP-GLES31.functional.fbo.no_attachments.size.1024x64,Fail
-dEQP-GLES31.functional.fbo.no_attachments.size.16x1024,Fail
-dEQP-GLES31.functional.fbo.no_attachments.size.16x256,Fail
-dEQP-GLES31.functional.fbo.no_attachments.size.16x64,Fail
-dEQP-GLES31.functional.fbo.no_attachments.size.256x1024,Fail
-dEQP-GLES31.functional.fbo.no_attachments.size.256x16,Fail
-dEQP-GLES31.functional.fbo.no_attachments.size.256x256,Fail
-dEQP-GLES31.functional.fbo.no_attachments.size.256x64,Fail
-dEQP-GLES31.functional.fbo.no_attachments.size.64x1024,Fail
-dEQP-GLES31.functional.fbo.no_attachments.size.64x16,Fail
-dEQP-GLES31.functional.fbo.no_attachments.size.64x256,Fail
-dEQP-GLES31.functional.fbo.no_attachments.size.64x64,Fail
 dEQP-GLES31.functional.texture.specification.texstorage3d.format.depth24_stencil8_cube_array,Fail
 dEQP-GLES31.functional.texture.specification.texstorage3d.format.depth32f_stencil8_cube_array,Fail
 dEQP-GLES31.functional.texture.specification.texsubimage3d_depth.depth24_stencil8_cube_array,Fail
@@ -488,10 +369,6 @@ spec@arb_depth_buffer_float@fbo-clear-formats,Fail
 spec@arb_depth_buffer_float@fbo-clear-formats@GL_DEPTH32F_STENCIL8,Fail
 spec@arb_es2_compatibility@texwrap formats bordercolor-swizzled,Fail
 spec@arb_es2_compatibility@texwrap formats bordercolor-swizzled@GL_RGB565- swizzled- border color only,Fail
-spec@arb_framebuffer_no_attachments@arb_framebuffer_no_attachments-query@Basic,Fail
-spec@arb_framebuffer_no_attachments@arb_framebuffer_no_attachments-query@discard,Fail
-spec@arb_framebuffer_no_attachments@arb_framebuffer_no_attachments-query@fb resize,Fail
-spec@arb_framebuffer_no_attachments@arb_framebuffer_no_attachments-query@glViewport,Fail
 spec@arb_sample_locations@test,Fail
 spec@arb_sample_locations@test@MSAA: 1- X: 0- Y: 0- Grid: false,Fail
 spec@arb_sample_locations@test@MSAA: 1- X: 0- Y: 0- Grid: true,Fail
diff --git a/src/gallium/drivers/zink/ci/zink-radv-vangogh-fails.txt b/src/gallium/drivers/zink/ci/zink-radv-vangogh-fails.txt
index 03e0bacfb29..9ff3b55ff8d 100644
--- a/src/gallium/drivers/zink/ci/zink-radv-vangogh-fails.txt
+++ b/src/gallium/drivers/zink/ci/zink-radv-vangogh-fails.txt
@@ -13,13 +13,8 @@ glx@extension string sanity,Fail
 # #6322
 spec@arb_framebuffer_object@fbo-blit-scaled-linear,Fail
 
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-mixed-const-non-const-uniform-index2,Fail
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-mixed-const-non-const-uniform-index,Fail
-spec@arb_arrays_of_arrays@execution@image_store@basic-imagestore-non-const-uniform-index,Fail
 spec@arb_bindless_texture@compiler@samplers@arith-bound-sampler-texture2d.frag,Crash
 
-spec@arb_framebuffer_no_attachments@arb_framebuffer_no_attachments-query,Fail
-spec@arb_framebuffer_no_attachments@arb_framebuffer_no_attachments-query@MS8,Fail
 spec@arb_gpu_shader_fp64@execution@arb_gpu_shader_fp64-tf-separate,Fail
 spec@arb_gpu_shader_fp64@execution@conversion@frag-conversion-explicit-dmat2-mat2,Fail
 spec@arb_gpu_shader_fp64@execution@conversion@frag-conversion-explicit-dmat2x3-mat2x3,Fail
@@ -186,62 +181,6 @@ spec@!opengl 3.0@clearbuffer-depth-cs-probe,Fail
 spec@!opengl 1.0@rasterpos@glsl_vs_gs_linked,Fail
 spec@!opengl 1.0@rasterpos@glsl_vs_tes_linked,Fail
 
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ABGR_EXT and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_ALPHA and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT_10_10_10_2,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT_2_10_10_10_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT_8_8_8_8,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_INT_8_8_8_8_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_SHORT_4_4_4_4,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_BGRA and GL_UNSIGNED_SHORT_4_4_4_4_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_LUMINANCE_ALPHA and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_FLOAT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_BYTE,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT_10_10_10_2,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT_2_10_10_10_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT_8_8_8_8,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_INT_8_8_8_8_REV,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_SHORT,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_SHORT_4_4_4_4,Fail
-spec@!opengl 1.1@teximage-colors gl_sluminance8_alpha8@GL_SLUMINANCE8_ALPHA8 texture with GL_RGBA and GL_UNSIGNED_SHORT_4_4_4_4_REV,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats-s3tc,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats-s3tc@GL_COMPRESSED_SLUMINANCE_ALPHA,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats-s3tc@GL_COMPRESSED_SLUMINANCE_ALPHA NPOT,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats@GL_SLUMINANCE8_ALPHA8,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats@GL_SLUMINANCE8_ALPHA8 NPOT,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats@GL_SLUMINANCE_ALPHA,Fail
-spec@ext_texture_srgb@fbo-generatemipmap-formats@GL_SLUMINANCE_ALPHA NPOT,Fail
 spec@ext_framebuffer_blit@fbo-blit-check-limits,Fail
 
 # Introduced with the uprev of piglit (70ce1dcacc92 - "ci: Update piglit with s3 support")
diff --git a/src/gallium/drivers/zink/ci/zink-tu-a630-flakes.txt b/src/gallium/drivers/zink/ci/zink-tu-a630-flakes.txt
index c729c1415d7..a9107058df7 100644
--- a/src/gallium/drivers/zink/ci/zink-tu-a630-flakes.txt
+++ b/src/gallium/drivers/zink/ci/zink-tu-a630-flakes.txt
@@ -13,3 +13,4 @@ dEQP-GLES31.functional.copy_image.non_compressed.viewclass_16_bits.rg8ui_rg8ui.t
 dEQP-GLES31.functional.copy_image.non_compressed.viewclass_32_bits.rg16i_rgb10_a2.cubemap_to_renderbuffer
 dEQP-GLES3.functional.texture.specification.texstorage3d.format.depth_component16_2d_array
 dEQP-GLES3.functional.texture.specification.texstorage2d.format.rgb565_cube
+dEQP-GLES31.functional.fbo.color.texcubearray.r16f
diff --git a/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.c b/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.c
index c30426aba83..e59df2b9ce4 100644
--- a/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.c
+++ b/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.c
@@ -114,6 +114,7 @@ struct ntv_context {
          subgroup_size_var;
 
    SpvId discard_func;
+   SpvId float_array_type[2];
 };
 
 static SpvId
@@ -1166,6 +1167,8 @@ get_bare_image_type(struct ntv_context *ctx, struct nir_variable *var, bool is_s
    bool arrayed = glsl_sampler_type_is_array(type);
    if (dimension == SpvDimCube && arrayed)
       spirv_builder_emit_cap(&ctx->builder, SpvCapabilityImageCubeArray);
+   if (arrayed && !is_sampler && is_ms)
+      spirv_builder_emit_cap(&ctx->builder, SpvCapabilityImageMSArray);
 
    SpvId result_type = get_glsl_basetype(ctx, glsl_get_sampler_result_type(type));
    return spirv_builder_type_image(&ctx->builder, result_type,
@@ -1930,13 +1933,7 @@ emit_alu(struct ntv_context *ctx, nir_alu_instr *alu)
       result = emit_builtin_unop(ctx, GLSLstd450PackHalf2x16, get_def_type(ctx, &alu->def, nir_type_uint), src[0]);
       break;
 
-   case nir_op_unpack_64_2x32:
-      assert(nir_op_infos[alu->op].num_inputs == 1);
-      result = emit_builtin_unop(ctx, GLSLstd450UnpackDouble2x32, get_def_type(ctx, &alu->def, nir_type_uint), src[0]);
-      break;
-
    BUILTIN_UNOPF(nir_op_unpack_half_2x16, GLSLstd450UnpackHalf2x16)
-   BUILTIN_UNOPF(nir_op_pack_64_2x32, GLSLstd450PackDouble2x32)
 #undef BUILTIN_UNOP
 #undef BUILTIN_UNOPF
 
@@ -2122,9 +2119,11 @@ emit_alu(struct ntv_context *ctx, nir_alu_instr *alu)
    /* those are all simple bitcasts, we could do better, but it doesn't matter */
    case nir_op_pack_32_4x8:
    case nir_op_pack_32_2x16:
+   case nir_op_pack_64_2x32:
    case nir_op_pack_64_4x16:
    case nir_op_unpack_32_4x8:
    case nir_op_unpack_32_2x16:
+   case nir_op_unpack_64_2x32:
    case nir_op_unpack_64_4x16: {
       result = emit_bitcast(ctx, dest_type, src[0]);
       break;
@@ -2737,6 +2736,8 @@ emit_interpolate(struct ntv_context *ctx, nir_intrinsic_instr *intr)
    case nir_intrinsic_interp_deref_at_sample:
       op = GLSLstd450InterpolateAtSample;
       src1 = get_src(ctx, &intr->src[1], &atype);
+      if (atype != nir_type_int)
+         src1 = emit_bitcast(ctx, get_ivec_type(ctx, 32, 1), src1);
       break;
    case nir_intrinsic_interp_deref_at_offset:
       op = GLSLstd450InterpolateAtOffset;
@@ -2780,6 +2781,16 @@ emit_deref_atomic_intrinsic(struct ntv_context *ctx, nir_intrinsic_instr *intr)
    nir_alu_type atype;
    nir_alu_type ret_type = nir_atomic_op_type(nir_intrinsic_atomic_op(intr)) == nir_type_float ? nir_type_float : nir_type_uint;
    SpvId ptr = get_src(ctx, &intr->src[0], &atype);
+   if (atype != ret_type && ret_type == nir_type_float) {
+      unsigned bit_size = nir_src_bit_size(intr->src[0]);
+      SpvId *float_array_type = &ctx->float_array_type[bit_size == 32 ? 0 : 1];
+      if (!*float_array_type) {
+         *float_array_type = spirv_builder_type_pointer(&ctx->builder, SpvStorageClassStorageBuffer,
+                                                        spirv_builder_type_float(&ctx->builder, bit_size));
+      }
+      ptr = emit_unop(ctx, SpvOpBitcast, *float_array_type, ptr);
+   }
+
    SpvId param = get_src(ctx, &intr->src[1], &atype);
    if (atype != ret_type)
       param = cast_src_to_type(ctx, param, intr->src[1], ret_type);
@@ -3208,6 +3219,11 @@ emit_barrier(struct ntv_context *ctx, nir_intrinsic_instr *intr)
       if (modes & (nir_var_shader_out | nir_var_mem_task_payload))
          semantics |= SpvMemorySemanticsOutputMemoryMask;
 
+      if (!modes)
+         semantics = SpvMemorySemanticsWorkgroupMemoryMask |
+                     SpvMemorySemanticsUniformMemoryMask |
+                     SpvMemorySemanticsImageMemoryMask |
+                     SpvMemorySemanticsCrossWorkgroupMemoryMask;
       semantics |= SpvMemorySemanticsAcquireReleaseMask;
    }
 
@@ -3533,6 +3549,9 @@ tex_instr_is_lod_allowed(nir_tex_instr *tex)
            tex->sampler_dim == GLSL_SAMPLER_DIM_2D ||
            tex->sampler_dim == GLSL_SAMPLER_DIM_3D ||
            tex->sampler_dim == GLSL_SAMPLER_DIM_CUBE ||
+           /* External images are interpreted as 2D in type_to_dim,
+            * so LOD is allowed */
+           tex->sampler_dim == GLSL_SAMPLER_DIM_EXTERNAL ||
            /* RECT will always become 2D, so this is fine */
            tex->sampler_dim == GLSL_SAMPLER_DIM_RECT);
 }
@@ -3965,8 +3984,11 @@ emit_deref_array(struct ntv_context *ctx, nir_deref_instr *deref)
    }
 
    SpvStorageClass storage_class = get_storage_class(var);
-   SpvId base, type;
+   SpvId type;
    nir_alu_type atype = nir_type_uint;
+
+   SpvId base = get_src(ctx, &deref->parent, &atype);
+
    switch (var->data.mode) {
 
    case nir_var_mem_ubo:
@@ -4008,6 +4030,26 @@ emit_deref_array(struct ntv_context *ctx, nir_deref_instr *deref)
    if (itype == nir_type_float)
       index = emit_bitcast(ctx, get_uvec_type(ctx, 32, 1), index);
 
+   if (var->data.mode == nir_var_uniform || var->data.mode == nir_var_image) {
+      nir_deref_instr *aoa_deref = nir_src_as_deref(deref->parent);
+      uint32_t inner_stride = glsl_array_size(aoa_deref->type);
+
+      while (aoa_deref->deref_type != nir_deref_type_var) {
+         assert(aoa_deref->deref_type == nir_deref_type_array);
+
+         SpvId aoa_index = get_src(ctx, &aoa_deref->arr.index, &itype);
+         if (itype == nir_type_float)
+            aoa_index = emit_bitcast(ctx, get_uvec_type(ctx, 32, 1), aoa_index);
+
+         aoa_deref = nir_src_as_deref(aoa_deref->parent);
+
+         uint32_t stride = glsl_get_aoa_size(aoa_deref->type) / inner_stride;
+         aoa_index = emit_binop(ctx, SpvOpIMul, get_uvec_type(ctx, 32, 1), aoa_index,
+                                emit_uint_const(ctx, 32, stride));
+         index = emit_binop(ctx, SpvOpIAdd, get_uvec_type(ctx, 32, 1), index, aoa_index);
+      }
+   }
+
    SpvId ptr_type = spirv_builder_type_pointer(&ctx->builder,
                                                storage_class,
                                                type);
@@ -4774,7 +4816,7 @@ nir_to_spirv(struct nir_shader *s, const struct zink_shader_info *sinfo, uint32_
       /* this could be huge, so only alloc if needed since it's extremely unlikely to
        * ever be used by anything except cts
        */
-      ctx.resident_defs = ralloc_array_size(ctx.mem_ctx,
+      ctx.resident_defs = rzalloc_array_size(ctx.mem_ctx,
                                             sizeof(SpvId), entry->ssa_alloc);
       if (!ctx.resident_defs)
          goto fail;
diff --git a/src/gallium/drivers/zink/zink_batch.c b/src/gallium/drivers/zink/zink_batch.c
index 769ba4f212c..b8abb68e53d 100644
--- a/src/gallium/drivers/zink/zink_batch.c
+++ b/src/gallium/drivers/zink/zink_batch.c
@@ -309,6 +309,11 @@ zink_batch_state_destroy(struct zink_screen *screen, struct zink_batch_state *bs
    util_dynarray_fini(&bs->bindless_releases[0]);
    util_dynarray_fini(&bs->bindless_releases[1]);
    util_dynarray_fini(&bs->acquires);
+   util_dynarray_fini(&bs->signal_semaphores);
+   util_dynarray_fini(&bs->wait_semaphores);
+   util_dynarray_fini(&bs->wait_semaphore_stages);
+   util_dynarray_fini(&bs->fd_wait_semaphores);
+   util_dynarray_fini(&bs->fd_wait_semaphore_stages);
    util_dynarray_fini(&bs->acquire_flags);
    unsigned num_mfences = util_dynarray_num_elements(&bs->fence.mfences, void *);
    struct zink_tc_fence **mfence = bs->fence.mfences.data;
@@ -458,10 +463,13 @@ get_batch_state(struct zink_context *ctx, struct zink_batch *batch)
       }
       simple_mtx_unlock(&screen->free_batch_states_lock);
    }
-   if (!bs && ctx->batch_states) {
-      /* states are stored sequentially, so if the first one doesn't work, none of them will */
-      if (zink_screen_check_last_finished(screen, ctx->batch_states->fence.batch_id) ||
-          find_unused_state(ctx->batch_states)) {
+   /* states are stored sequentially, so if the first one doesn't work, none of them will */
+   if (!bs && ctx->batch_states && ctx->batch_states->next) {
+      /* only a submitted state can be reused */
+      if (p_atomic_read(&ctx->batch_states->fence.submitted) &&
+          /* a submitted state must have completed before it can be reused */
+          (zink_screen_check_last_finished(screen, ctx->batch_states->fence.batch_id) ||
+           p_atomic_read(&ctx->batch_states->fence.completed))) {
          bs = ctx->batch_states;
          pop_batch_state(ctx);
       }
@@ -754,7 +762,7 @@ submit_queue(void *data, void *gdata, int thread_index)
 
    unsigned i = 0;
    VkSemaphore *sem = bs->signal_semaphores.data;
-   set_foreach_remove(&bs->dmabuf_exports, entry) {
+   set_foreach(&bs->dmabuf_exports, entry) {
       struct zink_resource *res = (void*)entry->key;
       for (; res; res = zink_resource(res->base.b.next))
          zink_screen_import_dmabuf_semaphore(screen, res, sem[i++]);
@@ -762,6 +770,7 @@ submit_queue(void *data, void *gdata, int thread_index)
       struct pipe_resource *pres = (void*)entry->key;
       pipe_resource_reference(&pres, NULL);
    }
+   _mesa_set_clear(&bs->dmabuf_exports, NULL);
 
    bs->usage.submit_count++;
 end:
diff --git a/src/gallium/drivers/zink/zink_blit.c b/src/gallium/drivers/zink/zink_blit.c
index 43ab2db5649..966b22a5755 100644
--- a/src/gallium/drivers/zink/zink_blit.c
+++ b/src/gallium/drivers/zink/zink_blit.c
@@ -362,13 +362,20 @@ zink_blit(struct pipe_context *pctx,
    bool stencil_blit = false;
    if (!util_blitter_is_blit_supported(ctx->blitter, info)) {
       if (util_format_is_depth_or_stencil(info->src.resource->format)) {
-         struct pipe_blit_info depth_blit = *info;
-         depth_blit.mask = PIPE_MASK_Z;
-         stencil_blit = util_blitter_is_blit_supported(ctx->blitter, &depth_blit);
-         if (stencil_blit) {
-            zink_blit_begin(ctx, ZINK_BLIT_SAVE_FB | ZINK_BLIT_SAVE_FS | ZINK_BLIT_SAVE_TEXTURES);
-            util_blitter_blit(ctx->blitter, &depth_blit);
+         if (info->mask & PIPE_MASK_Z) {
+            struct pipe_blit_info depth_blit = *info;
+            depth_blit.mask = PIPE_MASK_Z;
+            if (util_blitter_is_blit_supported(ctx->blitter, &depth_blit)) {
+               zink_blit_begin(ctx, ZINK_BLIT_SAVE_FB | ZINK_BLIT_SAVE_FS | ZINK_BLIT_SAVE_TEXTURES);
+               util_blitter_blit(ctx->blitter, &depth_blit);
+            } else {
+               mesa_loge("ZINK: depth blit unsupported %s -> %s",
+                         util_format_short_name(info->src.resource->format),
+                         util_format_short_name(info->dst.resource->format));
+            }
          }
+         if (info->mask & PIPE_MASK_S)
+            stencil_blit = true;
       }
       if (!stencil_blit) {
          mesa_loge("ZINK: blit unsupported %s -> %s",
diff --git a/src/gallium/drivers/zink/zink_bo.c b/src/gallium/drivers/zink/zink_bo.c
index b5b4f070765..2c5e78c1b7a 100644
--- a/src/gallium/drivers/zink/zink_bo.c
+++ b/src/gallium/drivers/zink/zink_bo.c
@@ -548,7 +548,7 @@ bo_sparse_create(struct zink_screen *screen, uint64_t size)
    bo->base.base.alignment_log2 = util_logbase2(ZINK_SPARSE_BUFFER_PAGE_SIZE);
    bo->base.base.size = size;
    bo->base.vtbl = &bo_sparse_vtbl;
-   unsigned placement = zink_mem_type_idx_from_bits(screen, ZINK_HEAP_DEVICE_LOCAL_SPARSE, VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT);
+   unsigned placement = zink_mem_type_idx_from_types(screen, ZINK_HEAP_DEVICE_LOCAL_SPARSE, UINT32_MAX);
    assert(placement != UINT32_MAX);
    bo->base.base.placement = placement;
    bo->unique_id = p_atomic_inc_return(&screen->pb.next_bo_unique_id);
@@ -622,6 +622,8 @@ zink_bo_create(struct zink_screen *screen, uint64_t size, unsigned alignment, en
             low_bound *= 2; //nvidia has fat textures or something
          unsigned vk_heap_idx = screen->info.mem_props.memoryTypes[mem_type_idx].heapIndex;
          reclaim_all = screen->info.mem_props.memoryHeaps[vk_heap_idx].size <= low_bound;
+         if (reclaim_all)
+            reclaim_all = clean_up_buffer_managers(screen);
       }
       entry = pb_slab_alloc_reclaimed(slabs, alloc_size, mem_type_idx, reclaim_all);
       if (!entry) {
@@ -636,6 +638,8 @@ zink_bo_create(struct zink_screen *screen, uint64_t size, unsigned alignment, en
       assert(bo->base.base.placement == mem_type_idx);
       pipe_reference_init(&bo->base.base.reference, 1);
       bo->base.base.size = size;
+      memset(&bo->reads, 0, sizeof(bo->reads));
+      memset(&bo->writes, 0, sizeof(bo->writes));
       assert(alignment <= 1 << bo->base.base.alignment_log2);
 
       return &bo->base;
@@ -664,8 +668,11 @@ no_slab:
        bo = (struct zink_bo*)
             pb_cache_reclaim_buffer(&screen->pb.bo_cache, size, alignment, 0, mem_type_idx);
        assert(!bo || bo->base.base.placement == mem_type_idx);
-       if (bo)
+       if (bo) {
+          memset(&bo->reads, 0, sizeof(bo->reads));
+          memset(&bo->writes, 0, sizeof(bo->writes));
           return &bo->base;
+       }
    }
 
    /* Create a new one. */
diff --git a/src/gallium/drivers/zink/zink_bo.h b/src/gallium/drivers/zink/zink_bo.h
index f747df6d869..cd7338aff5a 100644
--- a/src/gallium/drivers/zink/zink_bo.h
+++ b/src/gallium/drivers/zink/zink_bo.h
@@ -94,10 +94,10 @@ zink_heap_from_domain_flags(VkMemoryPropertyFlags domains, enum zink_alloc_flag
 }
 
 static ALWAYS_INLINE unsigned
-zink_mem_type_idx_from_bits(struct zink_screen *screen, enum zink_heap heap, uint32_t bits)
+zink_mem_type_idx_from_types(struct zink_screen *screen, enum zink_heap heap, uint32_t types)
 {
    for (unsigned i = 0; i < screen->heap_count[heap]; i++) {
-      if (bits & BITFIELD_BIT(screen->heap_map[heap][i])) {
+      if (types & BITFIELD_BIT(screen->heap_map[heap][i])) {
          return screen->heap_map[heap][i];
       }
    }
diff --git a/src/gallium/drivers/zink/zink_clear.c b/src/gallium/drivers/zink/zink_clear.c
index dc2ca1d1c11..dd1684a33f9 100644
--- a/src/gallium/drivers/zink/zink_clear.c
+++ b/src/gallium/drivers/zink/zink_clear.c
@@ -98,8 +98,8 @@ clear_in_rp(struct pipe_context *pctx,
          return;
       cr.rect.offset.x = scissor_state->minx;
       cr.rect.offset.y = scissor_state->miny;
-      cr.rect.extent.width = MIN2(fb->width, scissor_state->maxx - scissor_state->minx);
-      cr.rect.extent.height = MIN2(fb->height, scissor_state->maxy - scissor_state->miny);
+      cr.rect.extent.width = MIN2(fb->width - cr.rect.offset.x, scissor_state->maxx - scissor_state->minx);
+      cr.rect.extent.height = MIN2(fb->height - cr.rect.offset.y, scissor_state->maxy - scissor_state->miny);
    } else {
       cr.rect.extent.width = fb->width;
       cr.rect.extent.height = fb->height;
@@ -644,6 +644,8 @@ zink_clear_depth_stencil(struct pipe_context *pctx, struct pipe_surface *dst,
                          bool render_condition_enabled)
 {
    struct zink_context *ctx = zink_context(pctx);
+   /* check for stencil fallback */
+   bool blitting = ctx->blitting;
    zink_flush_dgc_if_enabled(ctx);
    bool render_condition_active = ctx->render_condition_active;
    if (!render_condition_enabled && render_condition_active) {
@@ -656,14 +658,16 @@ zink_clear_depth_stencil(struct pipe_context *pctx, struct pipe_surface *dst,
        dsty + height > ctx->fb_state.height)
       cur_attachment = false;
    if (!cur_attachment) {
-      util_blitter_save_framebuffer(ctx->blitter, &ctx->fb_state);
-      set_clear_fb(pctx, NULL, dst);
-      zink_blit_barriers(ctx, NULL, zink_resource(dst->texture), false);
-      ctx->blitting = true;
+      if (!blitting) {
+         util_blitter_save_framebuffer(ctx->blitter, &ctx->fb_state);
+         set_clear_fb(pctx, NULL, dst);
+         zink_blit_barriers(ctx, NULL, zink_resource(dst->texture), false);
+         ctx->blitting = true;
+      }
    }
    struct pipe_scissor_state scissor = {dstx, dsty, dstx + width, dsty + height};
    pctx->clear(pctx, clear_flags, &scissor, NULL, depth, stencil);
-   if (!cur_attachment) {
+   if (!cur_attachment && !blitting) {
       util_blitter_restore_fb_state(ctx->blitter);
       ctx->blitting = false;
    }
diff --git a/src/gallium/drivers/zink/zink_compiler.c b/src/gallium/drivers/zink/zink_compiler.c
index 3674f412062..fce8f51b7ea 100644
--- a/src/gallium/drivers/zink/zink_compiler.c
+++ b/src/gallium/drivers/zink/zink_compiler.c
@@ -2671,13 +2671,11 @@ fill_zero_reads(nir_builder *b, nir_intrinsic_instr *intr, void *data)
    if (intr->def.bit_size == 64)
       num_components *= 2;
    nir_src *src_offset = nir_get_io_offset_src(intr);
-   if (nir_src_is_const(*src_offset)) {
-      unsigned slot_offset = nir_src_as_uint(*src_offset);
-      if (s.location + slot_offset != wc->slot)
-         return false;
-   } else if (s.location > wc->slot || s.location + s.num_slots <= wc->slot) {
+   if (!nir_src_is_const(*src_offset))
+      return false;
+   unsigned slot_offset = nir_src_as_uint(*src_offset);
+   if (s.location + slot_offset != wc->slot)
       return false;
-   }
    uint32_t readmask = BITFIELD_MASK(intr->num_components) << c;
    if (intr->def.bit_size == 64)
       readmask |= readmask << (intr->num_components + c);
@@ -3543,6 +3541,88 @@ invert_point_coord(nir_shader *nir)
                                      nir_metadata_dominance, NULL);
 }
 
+static bool
+is_residency_code(nir_def *src)
+{
+   nir_instr *parent = src->parent_instr;
+   while (1) {
+      if (parent->type == nir_instr_type_intrinsic) {
+         ASSERTED nir_intrinsic_instr *intr = nir_instr_as_intrinsic(parent);
+         assert(intr->intrinsic == nir_intrinsic_is_sparse_texels_resident);
+         return false;
+      }
+      if (parent->type == nir_instr_type_tex)
+         return true;
+      assert(parent->type == nir_instr_type_alu);
+      nir_alu_instr *alu = nir_instr_as_alu(parent);
+      parent = alu->src[0].src.ssa->parent_instr;
+   }
+}
+
+static bool
+lower_sparse_instr(nir_builder *b, nir_intrinsic_instr *instr, void *data)
+{
+   if (instr->intrinsic == nir_intrinsic_sparse_residency_code_and) {
+      b->cursor = nir_before_instr(&instr->instr);
+      nir_def *src0;
+      if (is_residency_code(instr->src[0].ssa))
+         src0 = nir_is_sparse_texels_resident(b, 1, instr->src[0].ssa);
+      else
+         src0 = instr->src[0].ssa;
+      nir_def *src1;
+      if (is_residency_code(instr->src[1].ssa))
+         src1 = nir_is_sparse_texels_resident(b, 1, instr->src[1].ssa);
+      else
+         src1 = instr->src[1].ssa;
+      nir_def *def = nir_iand(b, src0, src1);
+      nir_def_rewrite_uses_after(&instr->def, def, &instr->instr);
+      nir_instr_remove(&instr->instr);
+      return true;
+   }
+   if (instr->intrinsic != nir_intrinsic_is_sparse_texels_resident)
+      return false;
+
+   /* vulkan vec can only be a vec4, but this is (maybe) vec5,
+    * so just rewrite as the first component since ntv is going to use a different
+    * method for storing the residency value anyway
+    */
+   b->cursor = nir_before_instr(&instr->instr);
+   nir_instr *parent = instr->src[0].ssa->parent_instr;
+   if (is_residency_code(instr->src[0].ssa)) {
+      assert(parent->type == nir_instr_type_alu);
+      nir_alu_instr *alu = nir_instr_as_alu(parent);
+      nir_def_rewrite_uses_after(instr->src[0].ssa, nir_channel(b, alu->src[0].src.ssa, 0), parent);
+      nir_instr_remove(parent);
+   } else {
+      nir_def *src;
+      if (parent->type == nir_instr_type_intrinsic) {
+         nir_intrinsic_instr *intr = nir_instr_as_intrinsic(parent);
+         assert(intr->intrinsic == nir_intrinsic_is_sparse_texels_resident);
+         src = intr->src[0].ssa;
+      } else {
+         assert(parent->type == nir_instr_type_alu);
+         nir_alu_instr *alu = nir_instr_as_alu(parent);
+         src = alu->src[0].src.ssa;
+      }
+      if (instr->def.bit_size != 32) {
+         if (instr->def.bit_size == 1)
+            src = nir_ieq_imm(b, src, 1);
+         else
+            src = nir_u2uN(b, src, instr->def.bit_size);
+      }
+      nir_def_rewrite_uses(&instr->def, src);
+      nir_instr_remove(&instr->instr);
+   }
+   return true;
+}
+
+static bool
+lower_sparse(nir_shader *shader)
+{
+   return nir_shader_intrinsics_pass(shader, lower_sparse_instr,
+                                     nir_metadata_dominance, NULL);
+}
+
 static bool
 add_derefs_instr(nir_builder *b, nir_intrinsic_instr *intr, void *data)
 {
@@ -3551,6 +3631,8 @@ add_derefs_instr(nir_builder *b, nir_intrinsic_instr *intr, void *data)
    bool is_interp = false;
    if (!filter_io_instr(intr, &is_load, &is_input, &is_interp))
       return false;
+   bool is_special_io = (b->shader->info.stage == MESA_SHADER_VERTEX && is_input) ||
+                        (b->shader->info.stage == MESA_SHADER_FRAGMENT && !is_input);
    unsigned loc = nir_intrinsic_io_semantics(intr).location;
    nir_src *src_offset = nir_get_io_offset_src(intr);
    const unsigned slot_offset = src_offset && nir_src_is_const(*src_offset) ? nir_src_as_uint(*src_offset) : 0;
@@ -3579,9 +3661,8 @@ add_derefs_instr(nir_builder *b, nir_intrinsic_instr *intr, void *data)
       bool is_struct = glsl_type_is_struct(glsl_without_array(type));
       if (is_struct)
          size = get_slot_components(var, var->data.location + slot_offset, var->data.location);
-      else if ((var->data.mode == nir_var_shader_out && var->data.location < VARYING_SLOT_VAR0) ||
-          (var->data.mode == nir_var_shader_in && var->data.location < (b->shader->info.stage == MESA_SHADER_VERTEX ? VERT_ATTRIB_GENERIC0 : VARYING_SLOT_VAR0)))
-         size = glsl_type_is_array(type) ? glsl_get_aoa_size(type) : glsl_get_vector_elements(type);
+      else if (!is_special_io && var->data.compact)
+         size = glsl_get_aoa_size(type);
       else
          size = glsl_get_vector_elements(glsl_without_array(type));
       assert(size);
@@ -3679,7 +3760,7 @@ add_derefs_instr(nir_builder *b, nir_intrinsic_instr *intr, void *data)
          }
          /* filter needed components */
          if (intr->num_components < load->num_components)
-            load = nir_channels(b, load, BITFIELD_MASK(intr->num_components) << c);
+            load = nir_channels(b, load, BITFIELD_MASK(intr->num_components) << (c - var->data.location_frac));
          nir_def_rewrite_uses(&intr->def, load);
       } else {
          nir_def *store = intr->src[0].ssa;
@@ -3936,6 +4017,7 @@ zink_shader_compile(struct zink_screen *screen, bool can_shobj, struct zink_shad
          zs->can_inline = false;
    } else if (need_optimize)
       optimize_nir(nir, zs, true);
+   NIR_PASS_V(nir, lower_sparse);
    
    struct zink_shader_object obj = compile_module(screen, zs, nir, can_shobj, pg);
    ralloc_free(nir);
@@ -4570,88 +4652,6 @@ scan_nir(struct zink_screen *screen, nir_shader *shader, struct zink_shader *zs)
    }
 }
 
-static bool
-is_residency_code(nir_def *src)
-{
-   nir_instr *parent = src->parent_instr;
-   while (1) {
-      if (parent->type == nir_instr_type_intrinsic) {
-         ASSERTED nir_intrinsic_instr *intr = nir_instr_as_intrinsic(parent);
-         assert(intr->intrinsic == nir_intrinsic_is_sparse_texels_resident);
-         return false;
-      }
-      if (parent->type == nir_instr_type_tex)
-         return true;
-      assert(parent->type == nir_instr_type_alu);
-      nir_alu_instr *alu = nir_instr_as_alu(parent);
-      parent = alu->src[0].src.ssa->parent_instr;
-   }
-}
-
-static bool
-lower_sparse_instr(nir_builder *b, nir_intrinsic_instr *instr, void *data)
-{
-   if (instr->intrinsic == nir_intrinsic_sparse_residency_code_and) {
-      b->cursor = nir_before_instr(&instr->instr);
-      nir_def *src0;
-      if (is_residency_code(instr->src[0].ssa))
-         src0 = nir_is_sparse_texels_resident(b, 1, instr->src[0].ssa);
-      else
-         src0 = instr->src[0].ssa;
-      nir_def *src1;
-      if (is_residency_code(instr->src[1].ssa))
-         src1 = nir_is_sparse_texels_resident(b, 1, instr->src[1].ssa);
-      else
-         src1 = instr->src[1].ssa;
-      nir_def *def = nir_iand(b, src0, src1);
-      nir_def_rewrite_uses_after(&instr->def, def, &instr->instr);
-      nir_instr_remove(&instr->instr);
-      return true;
-   }
-   if (instr->intrinsic != nir_intrinsic_is_sparse_texels_resident)
-      return false;
-
-   /* vulkan vec can only be a vec4, but this is (maybe) vec5,
-    * so just rewrite as the first component since ntv is going to use a different
-    * method for storing the residency value anyway
-    */
-   b->cursor = nir_before_instr(&instr->instr);
-   nir_instr *parent = instr->src[0].ssa->parent_instr;
-   if (is_residency_code(instr->src[0].ssa)) {
-      assert(parent->type == nir_instr_type_alu);
-      nir_alu_instr *alu = nir_instr_as_alu(parent);
-      nir_def_rewrite_uses_after(instr->src[0].ssa, nir_channel(b, alu->src[0].src.ssa, 0), parent);
-      nir_instr_remove(parent);
-   } else {
-      nir_def *src;
-      if (parent->type == nir_instr_type_intrinsic) {
-         nir_intrinsic_instr *intr = nir_instr_as_intrinsic(parent);
-         assert(intr->intrinsic == nir_intrinsic_is_sparse_texels_resident);
-         src = intr->src[0].ssa;
-      } else {
-         assert(parent->type == nir_instr_type_alu);
-         nir_alu_instr *alu = nir_instr_as_alu(parent);
-         src = alu->src[0].src.ssa;
-      }
-      if (instr->def.bit_size != 32) {
-         if (instr->def.bit_size == 1)
-            src = nir_ieq_imm(b, src, 1);
-         else
-            src = nir_u2uN(b, src, instr->def.bit_size);
-      }
-      nir_def_rewrite_uses(&instr->def, src);
-      nir_instr_remove(&instr->instr);
-   }
-   return true;
-}
-
-static bool
-lower_sparse(nir_shader *shader)
-{
-   return nir_shader_intrinsics_pass(shader, lower_sparse_instr,
-                                     nir_metadata_dominance, NULL);
-}
-
 static bool
 match_tex_dests_instr(nir_builder *b, nir_instr *in, void *data)
 {
@@ -4912,13 +4912,13 @@ fixup_io_locations(nir_shader *nir)
             else
                var->data.driver_location = var->data.location;
          }
-         return true;
+         continue;
       }
       /* i/o interface blocks are required to be EXACT matches between stages:
       * iterate over all locations and set locations incrementally
       */
       unsigned slot = 0;
-      for (unsigned i = 0; i < VARYING_SLOT_MAX; i++) {
+      for (unsigned i = 0; i < VARYING_SLOT_TESS_MAX; i++) {
          if (nir_slot_is_sysval_output(i, MESA_SHADER_NONE))
             continue;
          bool found = false;
@@ -5301,11 +5301,20 @@ mem_access_size_align_cb(nir_intrinsic_op intrin, uint8_t bytes,
 
    assert(util_is_power_of_two_nonzero(align));
 
-   return (nir_mem_access_size_align){
-      .num_components = MIN2(bytes / (bit_size / 8), 4),
-      .bit_size = bit_size,
-      .align = bit_size / 8,
-   };
+   /* simply drop the bit_size for unaligned load/stores */
+   if (align < (bit_size / 8)) {
+      return (nir_mem_access_size_align){
+         .num_components = MIN2(bytes / align, 4),
+         .bit_size = align * 8,
+         .align = align,
+      };
+   } else {
+      return (nir_mem_access_size_align){
+         .num_components = MIN2(bytes / (bit_size / 8), 4),
+         .bit_size = bit_size,
+         .align = bit_size / 8,
+      };
+   }
 }
 
 static nir_mem_access_size_align
@@ -5468,7 +5477,6 @@ zink_shader_create(struct zink_screen *screen, struct nir_shader *nir)
 
    NIR_PASS_V(nir, lower_basevertex);
    NIR_PASS_V(nir, lower_baseinstance);
-   NIR_PASS_V(nir, lower_sparse);
    NIR_PASS_V(nir, split_bitfields);
    NIR_PASS_V(nir, nir_lower_frexp); /* TODO: Use the spirv instructions for this. */
 
@@ -5651,6 +5659,7 @@ zink_shader_create(struct zink_screen *screen, struct nir_shader *nir)
    }
    zink_shader_serialize_blob(nir, &ret->blob);
    memcpy(&ret->info, &nir->info, sizeof(nir->info));
+   ret->info.name = ralloc_strdup(ret, nir->info.name);
 
    ret->can_inline = true;
 
@@ -5708,70 +5717,82 @@ zink_shader_free(struct zink_screen *screen, struct zink_shader *shader)
    ralloc_free(shader);
 }
 
+static bool
+gfx_shader_prune(struct zink_screen *screen, struct zink_shader *shader)
+{
+   /* this shader may still be precompiling, so access here must be locked and singular */
+   simple_mtx_lock(&shader->lock);
+   struct set_entry *entry = _mesa_set_next_entry(shader->programs, NULL);
+   struct zink_gfx_program *prog = (void*)(entry ? entry->key : NULL);
+   if (entry)
+      _mesa_set_remove(shader->programs, entry);
+   simple_mtx_unlock(&shader->lock);
+   if (!prog)
+      return false;
+   gl_shader_stage stage = shader->info.stage;
+   assert(stage < ZINK_GFX_SHADER_COUNT);
+   unsigned stages_present = prog->stages_present;
+   if (prog->shaders[MESA_SHADER_TESS_CTRL] &&
+         prog->shaders[MESA_SHADER_TESS_CTRL]->non_fs.is_generated)
+      stages_present &= ~BITFIELD_BIT(MESA_SHADER_TESS_CTRL);
+   unsigned idx = zink_program_cache_stages(stages_present);
+   if (!prog->base.removed && prog->stages_present == prog->stages_remaining &&
+         (stage == MESA_SHADER_FRAGMENT || !shader->non_fs.is_generated)) {
+      struct hash_table *ht = &prog->ctx->program_cache[idx];
+      simple_mtx_lock(&prog->ctx->program_lock[idx]);
+      struct hash_entry *he = _mesa_hash_table_search(ht, prog->shaders);
+      assert(he && he->data == prog);
+      _mesa_hash_table_remove(ht, he);
+      prog->base.removed = true;
+      simple_mtx_unlock(&prog->ctx->program_lock[idx]);
+      util_queue_fence_wait(&prog->base.cache_fence);
+
+      for (unsigned r = 0; r < ARRAY_SIZE(prog->pipelines); r++) {
+         for (int i = 0; i < ARRAY_SIZE(prog->pipelines[0]); ++i) {
+            hash_table_foreach(&prog->pipelines[r][i], entry) {
+               struct zink_gfx_pipeline_cache_entry *pc_entry = entry->data;
+
+               util_queue_fence_wait(&pc_entry->fence);
+            }
+         }
+      }
+   }
+   if (stage == MESA_SHADER_FRAGMENT || !shader->non_fs.is_generated) {
+      prog->shaders[stage] = NULL;
+      prog->stages_remaining &= ~BITFIELD_BIT(stage);
+   }
+   /* only remove generated tcs during parent tes destruction */
+   if (stage == MESA_SHADER_TESS_EVAL && shader->non_fs.generated_tcs)
+      prog->shaders[MESA_SHADER_TESS_CTRL] = NULL;
+   if (stage != MESA_SHADER_FRAGMENT &&
+      prog->shaders[MESA_SHADER_GEOMETRY] &&
+      prog->shaders[MESA_SHADER_GEOMETRY]->non_fs.parent ==
+      shader) {
+      prog->shaders[MESA_SHADER_GEOMETRY] = NULL;
+   }
+   zink_gfx_program_reference(screen, &prog, NULL);
+   return true;
+}
+
 void
 zink_gfx_shader_free(struct zink_screen *screen, struct zink_shader *shader)
 {
    assert(shader->info.stage != MESA_SHADER_COMPUTE);
    util_queue_fence_wait(&shader->precompile.fence);
-   set_foreach(shader->programs, entry) {
-      struct zink_gfx_program *prog = (void*)entry->key;
-      gl_shader_stage stage = shader->info.stage;
-      assert(stage < ZINK_GFX_SHADER_COUNT);
-      unsigned stages_present = prog->stages_present;
-      if (prog->shaders[MESA_SHADER_TESS_CTRL] &&
-            prog->shaders[MESA_SHADER_TESS_CTRL]->non_fs.is_generated)
-         stages_present &= ~BITFIELD_BIT(MESA_SHADER_TESS_CTRL);
-      unsigned idx = zink_program_cache_stages(stages_present);
-      if (!prog->base.removed && prog->stages_present == prog->stages_remaining &&
-          (stage == MESA_SHADER_FRAGMENT || !shader->non_fs.is_generated)) {
-         struct hash_table *ht = &prog->ctx->program_cache[idx];
-         simple_mtx_lock(&prog->ctx->program_lock[idx]);
-         struct hash_entry *he = _mesa_hash_table_search(ht, prog->shaders);
-         assert(he && he->data == prog);
-         _mesa_hash_table_remove(ht, he);
-         prog->base.removed = true;
-         simple_mtx_unlock(&prog->ctx->program_lock[idx]);
-         util_queue_fence_wait(&prog->base.cache_fence);
-
-         for (unsigned r = 0; r < ARRAY_SIZE(prog->pipelines); r++) {
-            for (int i = 0; i < ARRAY_SIZE(prog->pipelines[0]); ++i) {
-               hash_table_foreach(&prog->pipelines[r][i], entry) {
-                  struct zink_gfx_pipeline_cache_entry *pc_entry = entry->data;
-
-                  util_queue_fence_wait(&pc_entry->fence);
-               }
-            }
-         }
 
+   /* if the shader is still precompiling, the program set must be pruned under lock */
+   while (gfx_shader_prune(screen, shader));
+
+   while (util_dynarray_contains(&shader->pipeline_libs, struct zink_gfx_lib_cache*)) {
+      struct zink_gfx_lib_cache *libs = util_dynarray_pop(&shader->pipeline_libs, struct zink_gfx_lib_cache*);
+      if (!libs->removed) {
+         libs->removed = true;
+         unsigned idx = zink_program_cache_stages(libs->stages_present);
+         simple_mtx_lock(&screen->pipeline_libs_lock[idx]);
+         _mesa_set_remove_key(&screen->pipeline_libs[idx], libs);
+         simple_mtx_unlock(&screen->pipeline_libs_lock[idx]);
       }
-      while (util_dynarray_contains(&shader->pipeline_libs, struct zink_gfx_lib_cache*)) {
-         struct zink_gfx_lib_cache *libs = util_dynarray_pop(&shader->pipeline_libs, struct zink_gfx_lib_cache*);
-         //this condition is equivalent to verifying that, for each bit stages_present_i in stages_present,
-         //stages_present_i implies libs->stages_present_i
-         if ((stages_present & ~(libs->stages_present & stages_present)) != 0)
-            continue;
-         if (!libs->removed) {
-            libs->removed = true;
-            simple_mtx_lock(&screen->pipeline_libs_lock[idx]);
-            _mesa_set_remove_key(&screen->pipeline_libs[idx], libs);
-            simple_mtx_unlock(&screen->pipeline_libs_lock[idx]);
-         }
-         zink_gfx_lib_cache_unref(screen, libs);
-      }
-      if (stage == MESA_SHADER_FRAGMENT || !shader->non_fs.is_generated) {
-         prog->shaders[stage] = NULL;
-         prog->stages_remaining &= ~BITFIELD_BIT(stage);
-      }
-      /* only remove generated tcs during parent tes destruction */
-      if (stage == MESA_SHADER_TESS_EVAL && shader->non_fs.generated_tcs)
-         prog->shaders[MESA_SHADER_TESS_CTRL] = NULL;
-      if (stage != MESA_SHADER_FRAGMENT &&
-          prog->shaders[MESA_SHADER_GEOMETRY] &&
-          prog->shaders[MESA_SHADER_GEOMETRY]->non_fs.parent ==
-          shader) {
-         prog->shaders[MESA_SHADER_GEOMETRY] = NULL;
-      }
-      zink_gfx_program_reference(screen, &prog, NULL);
+      zink_gfx_lib_cache_unref(screen, libs);
    }
    if (shader->info.stage == MESA_SHADER_TESS_EVAL &&
        shader->non_fs.generated_tcs) {
diff --git a/src/gallium/drivers/zink/zink_context.c b/src/gallium/drivers/zink/zink_context.c
index 5916839c37b..8978ea2f2a1 100644
--- a/src/gallium/drivers/zink/zink_context.c
+++ b/src/gallium/drivers/zink/zink_context.c
@@ -137,7 +137,7 @@ zink_context_destroy(struct pipe_context *pctx)
       simple_mtx_lock((&ctx->program_lock[i]));
       hash_table_foreach(&ctx->program_cache[i], entry) {
          struct zink_program *pg = entry->data;
-         util_queue_fence_wait(&pg->cache_fence);
+         zink_program_finish(ctx, pg);
          pg->removed = true;
       }
       simple_mtx_unlock((&ctx->program_lock[i]));
@@ -187,21 +187,31 @@ zink_context_destroy(struct pipe_context *pctx)
          screen->free_batch_states = ctx->batch_states;
          screen->last_free_batch_state = screen->free_batch_states;
       }
-      while (screen->last_free_batch_state->next)
-         screen->last_free_batch_state = screen->last_free_batch_state->next;
    }
+   while (screen->last_free_batch_state && screen->last_free_batch_state->next)
+      screen->last_free_batch_state = screen->last_free_batch_state->next;
    if (ctx->free_batch_states) {
       if (screen->free_batch_states)
          screen->last_free_batch_state->next = ctx->free_batch_states;
-      else
+      else {
          screen->free_batch_states = ctx->free_batch_states;
-      screen->last_free_batch_state = ctx->last_free_batch_state;
+         screen->last_free_batch_state = ctx->last_free_batch_state;
+      }
    }
-   simple_mtx_unlock(&screen->free_batch_states_lock);
+   while (screen->last_free_batch_state && screen->last_free_batch_state->next)
+      screen->last_free_batch_state = screen->last_free_batch_state->next;
    if (ctx->batch.state) {
       zink_clear_batch_state(ctx, ctx->batch.state);
-      zink_batch_state_destroy(screen, ctx->batch.state);
+      if (screen->free_batch_states)
+         screen->last_free_batch_state->next = ctx->batch.state;
+      else {
+         screen->free_batch_states = ctx->batch.state;
+         screen->last_free_batch_state = screen->free_batch_states;
+      }
    }
+   while (screen->last_free_batch_state && screen->last_free_batch_state->next)
+      screen->last_free_batch_state = screen->last_free_batch_state->next;
+   simple_mtx_unlock(&screen->free_batch_states_lock);
 
    for (unsigned i = 0; i < 2; i++) {
       util_idalloc_fini(&ctx->di.bindless[i].tex_slots);
@@ -1937,9 +1947,6 @@ zink_set_shader_images(struct pipe_context *pctx,
                   /* ref already added by create */
                   a->buffer_view = bv;
                }
-               if (zink_resource_access_is_write(access))
-                  res->obj->unordered_write = false;
-               res->obj->unordered_read = false;
             } else {
                /* image rebind: get updated surface and unref old one */
                struct zink_surface *surface = create_image_surface(ctx, b, is_compute);
@@ -1959,6 +1966,9 @@ zink_set_shader_images(struct pipe_context *pctx,
                                          res->gfx_barrier);
             zink_batch_resource_usage_set(&ctx->batch, res,
                                           zink_resource_access_is_write(access), true);
+            if (zink_resource_access_is_write(access))
+               res->obj->unordered_write = false;
+            res->obj->unordered_read = false;
          } else {
             finalize_image_bind(ctx, res, is_compute);
             zink_batch_resource_usage_set(&ctx->batch, res,
@@ -2837,6 +2847,29 @@ begin_rendering(struct zink_context *ctx)
                ctx->dynamic_fb.attachments[PIPE_MAX_COLOR_BUFS+1].loadOp = VK_ATTACHMENT_LOAD_OP_CLEAR;
             }
          }
+      }
+      if (changed_size || changed_layout)
+         ctx->rp_changed = true;
+      ctx->rp_loadop_changed = false;
+      ctx->rp_layout_changed = false;
+   }
+   /* always assemble clear_buffers mask:
+    * if a scissored clear must be triggered during glFlush,
+    * the renderpass metadata may be unchanged (e.g., LOAD from previous rp),
+    * but the buffer mask must still be returned
+    */
+   if (ctx->clears_enabled) {
+      for (int i = 0; i < ctx->fb_state.nr_cbufs; i++) {
+         /* these are no-ops */
+         if (!ctx->fb_state.cbufs[i] || !zink_fb_clear_enabled(ctx, i))
+            continue;
+         /* these need actual clear calls inside the rp */
+         if (zink_fb_clear_needs_explicit(&ctx->fb_clears[i]))
+            clear_buffers |= (PIPE_CLEAR_COLOR0 << i);
+      }
+      if (ctx->fb_state.zsbuf && zink_fb_clear_enabled(ctx, PIPE_MAX_COLOR_BUFS)) {
+         struct zink_framebuffer_clear *fb_clear = &ctx->fb_clears[PIPE_MAX_COLOR_BUFS];
+         struct zink_framebuffer_clear_data *clear = zink_fb_clear_element(fb_clear, 0);
          if (zink_fb_clear_needs_explicit(fb_clear)) {
             for (int j = !zink_fb_clear_element_needs_explicit(clear);
                  (clear_buffers & PIPE_CLEAR_DEPTHSTENCIL) != PIPE_CLEAR_DEPTHSTENCIL && j < zink_fb_clear_count(fb_clear);
@@ -2844,10 +2877,6 @@ begin_rendering(struct zink_context *ctx)
                clear_buffers |= zink_fb_clear_element(fb_clear, j)->zs.bits;
          }
       }
-      if (changed_size || changed_layout)
-         ctx->rp_changed = true;
-      ctx->rp_loadop_changed = false;
-      ctx->rp_layout_changed = false;
    }
 
    if (!ctx->rp_changed && ctx->batch.in_rp)
@@ -2878,10 +2907,11 @@ begin_rendering(struct zink_context *ctx)
    if (has_swapchain) {
       ASSERTED struct zink_resource *res = zink_resource(ctx->fb_state.cbufs[0]->texture);
       zink_render_fixup_swapchain(ctx);
-      assert(ctx->dynamic_fb.info.renderArea.extent.width <= res->base.b.width0);
-      assert(ctx->dynamic_fb.info.renderArea.extent.height <= res->base.b.height0);
-      assert(ctx->fb_state.width <= res->base.b.width0);
-      assert(ctx->fb_state.height <= res->base.b.height0);
+      /* clamp for late swapchain resize */
+      if (res->base.b.width0 < ctx->dynamic_fb.info.renderArea.extent.width)
+         ctx->dynamic_fb.info.renderArea.extent.width = res->base.b.width0;
+      if (res->base.b.height0 < ctx->dynamic_fb.info.renderArea.extent.height)
+         ctx->dynamic_fb.info.renderArea.extent.height = res->base.b.height0;
    }
    if (ctx->fb_state.zsbuf && zsbuf_used) {
       struct zink_surface *surf = zink_csurface(ctx->fb_state.zsbuf);
@@ -3803,7 +3833,6 @@ zink_flush(struct pipe_context *pctx,
    struct zink_batch *batch = &ctx->batch;
    struct zink_fence *fence = NULL;
    struct zink_screen *screen = zink_screen(ctx->base.screen);
-   unsigned submit_count = 0;
    VkSemaphore export_sem = VK_NULL_HANDLE;
 
    /* triggering clears will force has_work */
@@ -3864,8 +3893,7 @@ zink_flush(struct pipe_context *pctx,
       }
    }
 
-   /* TODO: if swapchains gain timeline semaphore semantics, `flags` can be eliminated and no-op fence can return timeline id */
-   if (!batch->has_work && flags) {
+   if (!batch->has_work) {
        if (pfence) {
           /* reuse last fence */
           fence = ctx->last_fence;
@@ -3882,7 +3910,6 @@ zink_flush(struct pipe_context *pctx,
          tc_driver_internal_flush_notify(ctx->tc);
    } else {
       fence = &batch->state->fence;
-      submit_count = batch->state->usage.submit_count;
       if (deferred && !(flags & PIPE_FLUSH_FENCE_FD) && pfence)
          deferred_fence = true;
       else
@@ -3906,7 +3933,7 @@ zink_flush(struct pipe_context *pctx,
       mfence->fence = fence;
       mfence->sem = export_sem;
       if (fence) {
-         mfence->submit_count = submit_count;
+         mfence->submit_count = zink_batch_state(fence)->usage.submit_count;
          util_dynarray_append(&fence->mfences, struct zink_tc_fence *, mfence);
       }
       if (export_sem) {
@@ -4120,7 +4147,7 @@ zink_flush_resource(struct pipe_context *pctx,
    struct zink_context *ctx = zink_context(pctx);
    struct zink_resource *res = zink_resource(pres);
    if (res->obj->dt) {
-      if (zink_kopper_acquired(res->obj->dt, res->obj->dt_idx)) {
+      if (zink_kopper_acquired(res->obj->dt, res->obj->dt_idx) && (!ctx->clears_enabled || !res->fb_bind_count)) {
          zink_batch_no_rp_safe(ctx);
          zink_kopper_readback_update(ctx, res);
          zink_screen(ctx->base.screen)->image_barrier(ctx, res, VK_IMAGE_LAYOUT_PRESENT_SRC_KHR, 0, VK_PIPELINE_STAGE_BOTTOM_OF_PIPE_BIT);
@@ -4819,8 +4846,11 @@ zink_resource_commit(struct pipe_context *pctx, struct pipe_resource *pres, unsi
    VkSemaphore sem = VK_NULL_HANDLE;
    bool ret = zink_bo_commit(ctx, res, level, box, commit, &sem);
    if (ret) {
-      if (sem)
+      if (sem) {
          zink_batch_add_wait_semaphore(&ctx->batch, sem);
+         zink_batch_reference_resource_rw(&ctx->batch, res, true);
+         ctx->batch.has_work = true;
+      }
    } else {
       check_device_lost(ctx);
    }
diff --git a/src/gallium/drivers/zink/zink_descriptors.c b/src/gallium/drivers/zink/zink_descriptors.c
index 81b197c2ef0..2ddd65eaa76 100644
--- a/src/gallium/drivers/zink/zink_descriptors.c
+++ b/src/gallium/drivers/zink/zink_descriptors.c
@@ -825,11 +825,14 @@ create_pool(struct zink_screen *screen, unsigned num_type_sizes, const VkDescrip
    dpci.poolSizeCount = num_type_sizes;
    dpci.flags = flags;
    dpci.maxSets = MAX_LAZY_DESCRIPTORS;
-   VkResult result = VKSCR(CreateDescriptorPool)(screen->dev, &dpci, 0, &pool);
-   if (result != VK_SUCCESS) {
-      mesa_loge("ZINK: vkCreateDescriptorPool failed (%s)", vk_Result_to_str(result));
-      return VK_NULL_HANDLE;
-   }
+   VkResult result;
+   VRAM_ALLOC_LOOP(result,
+      VKSCR(CreateDescriptorPool)(screen->dev, &dpci, 0, &pool),
+      if (result != VK_SUCCESS) {
+         mesa_loge("ZINK: vkCreateDescriptorPool failed (%s)", vk_Result_to_str(result));
+         return VK_NULL_HANDLE;
+      }
+   );
    return pool;
 }
 
@@ -1588,7 +1591,7 @@ zink_batch_descriptor_init(struct zink_screen *screen, struct zink_batch_state *
    }
 
    if (zink_descriptor_mode == ZINK_DESCRIPTOR_MODE_DB && !(bs->ctx->flags & ZINK_CONTEXT_COPY_ONLY)) {
-      unsigned bind = ZINK_BIND_RESOURCE_DESCRIPTOR | ZINK_BIND_SAMPLER_DESCRIPTOR;
+      unsigned bind = ZINK_BIND_DESCRIPTOR;
       struct pipe_resource *pres = pipe_buffer_create(&screen->base, bind, 0, bs->ctx->dd.db.max_db_size * screen->base_descriptor_size);
       if (!pres)
          return false;
@@ -1730,7 +1733,7 @@ zink_descriptors_init_bindless(struct zink_context *ctx)
    ctx->dd.bindless_init = true;
 
    if (zink_descriptor_mode == ZINK_DESCRIPTOR_MODE_DB) {
-      unsigned bind = ZINK_BIND_RESOURCE_DESCRIPTOR | ZINK_BIND_SAMPLER_DESCRIPTOR;
+      unsigned bind = ZINK_BIND_DESCRIPTOR;
       VkDeviceSize size;
       VKSCR(GetDescriptorSetLayoutSizeEXT)(screen->dev, screen->bindless_layout, &size);
       struct pipe_resource *pres = pipe_buffer_create(&screen->base, bind, 0, size);
diff --git a/src/gallium/drivers/zink/zink_device_info.py b/src/gallium/drivers/zink/zink_device_info.py
index c7d2aa4be4a..fea43c448ae 100644
--- a/src/gallium/drivers/zink/zink_device_info.py
+++ b/src/gallium/drivers/zink/zink_device_info.py
@@ -145,10 +145,6 @@ EXTENSIONS = [
     Extension("VK_EXT_sample_locations",
               alias="sample_locations",
               properties=True),
-    Extension("VK_EXT_conservative_rasterization",
-              alias="cons_raster",
-              properties=True,
-              conditions=["$props.fullyCoveredFragmentShaderInputVariable"]),
     Extension("VK_KHR_shader_draw_parameters"),
     Extension("VK_KHR_sampler_mirror_clamp_to_edge"),
     Extension("VK_EXT_descriptor_buffer", alias="db", features=True, properties=True),
@@ -658,6 +654,36 @@ zink_get_physical_device_info(struct zink_screen *screen)
 
    info->num_extensions = num_extensions;
 
+   info->feats.pNext = NULL;
+
+%for version in versions:
+%if version.device_version < (1,2,0):
+      if (VK_MAKE_VERSION(1,2,0) <= screen->vk_version) {
+         /* VkPhysicalDeviceVulkan11Features was added in 1.2, not 1.1 as one would think */
+%else:
+      if (${version.version()} <= screen->vk_version) {
+%endif
+         info->feats${version.struct()}.pNext = info->feats.pNext;
+         info->feats.pNext = &info->feats${version.struct()};
+      }
+%endfor
+
+%for ext in extensions:
+%if ext.has_features:
+<%helpers:guard ext="${ext}">
+%if ext.features_promoted:
+      if (info->have_${ext.name_with_vendor()} && !info->have_vulkan${ext.core_since.struct()}) {
+%else:
+      if (info->have_${ext.name_with_vendor()}) {
+%endif
+         info->${ext.field("feats")}.sType = ${ext.stype("FEATURES")};
+         info->${ext.field("feats")}.pNext = info->feats.pNext;
+         info->feats.pNext = &info->${ext.field("feats")};
+      }
+</%helpers:guard>
+%endif
+%endfor
+
    return true;
 
 fail:
diff --git a/src/gallium/drivers/zink/zink_draw.cpp b/src/gallium/drivers/zink/zink_draw.cpp
index 1526f3fc077..0da405ea7b7 100644
--- a/src/gallium/drivers/zink/zink_draw.cpp
+++ b/src/gallium/drivers/zink/zink_draw.cpp
@@ -437,6 +437,7 @@ update_gfx_pipeline(struct zink_context *ctx, struct zink_batch_state *bs, enum
          VKCTX(CmdSetDepthBiasEnable)(bs->cmdbuf, VK_TRUE);
          VKCTX(CmdSetTessellationDomainOriginEXT)(bs->cmdbuf, VK_TESSELLATION_DOMAIN_ORIGIN_LOWER_LEFT);
          VKCTX(CmdSetSampleLocationsEnableEXT)(bs->cmdbuf, ctx->gfx_pipeline_state.sample_locations_enabled);
+         VKCTX(CmdSetRasterizationStreamEXT)(bs->cmdbuf, 0);
       }
       ctx->shobj_draw = true;
    }
diff --git a/src/gallium/drivers/zink/zink_fence.c b/src/gallium/drivers/zink/zink_fence.c
index 33ea9d7d90e..86bc56cf119 100644
--- a/src/gallium/drivers/zink/zink_fence.c
+++ b/src/gallium/drivers/zink/zink_fence.c
@@ -185,7 +185,12 @@ zink_fence_finish(struct zink_screen *screen, struct pipe_context *pctx, struct
    if (submit_diff > 1)
       return true;
 
-   if (fence->submitted && zink_screen_check_last_finished(screen, fence->batch_id))
+   /* - if fence is submitted, batch_id is nonzero and can be checked
+    * - if fence is not submitted here, it must be reset; batch_id will be 0 and submitted is false
+    * in either case, the fence has finished
+    */
+   if ((fence->submitted && zink_screen_check_last_finished(screen, fence->batch_id)) ||
+       (!fence->submitted && submit_diff))
       return true;
 
    return fence_wait(screen, fence, timeout_ns);
diff --git a/src/gallium/drivers/zink/zink_format.c b/src/gallium/drivers/zink/zink_format.c
index 96a046a6fef..cf36909d6ca 100644
--- a/src/gallium/drivers/zink/zink_format.c
+++ b/src/gallium/drivers/zink/zink_format.c
@@ -152,6 +152,8 @@ zink_format_get_emulated_alpha(enum pipe_format format)
    if (util_format_is_luminance(format))
       return util_format_luminance_to_red(format);
    if (util_format_is_luminance_alpha(format)) {
+      if (util_format_is_srgb(format))
+         return format;
       if (format == PIPE_FORMAT_LATC2_UNORM)
          return PIPE_FORMAT_RGTC2_UNORM;
       if (format == PIPE_FORMAT_LATC2_SNORM)
diff --git a/src/gallium/drivers/zink/zink_kopper.c b/src/gallium/drivers/zink/zink_kopper.c
index bda4c206877..b700befbc29 100644
--- a/src/gallium/drivers/zink/zink_kopper.c
+++ b/src/gallium/drivers/zink/zink_kopper.c
@@ -561,6 +561,8 @@ kopper_acquire(struct zink_screen *screen, struct zink_resource *res, uint64_t t
    if (cdt->swapchain->images[res->obj->dt_idx].readback)
       zink_resource(cdt->swapchain->images[res->obj->dt_idx].readback)->valid = false;
    res->obj->image = cdt->swapchain->images[res->obj->dt_idx].image;
+   if (!cdt->age_locked)
+      zink_kopper_update_last_written(res);
    cdt->swapchain->images[res->obj->dt_idx].acquired = false;
    if (!cdt->swapchain->images[res->obj->dt_idx].init) {
       /* swapchain images are initially in the UNDEFINED layout */
@@ -792,7 +794,7 @@ zink_kopper_present_queue(struct zink_screen *screen, struct zink_resource *res)
    cpi->res = res;
    cpi->swapchain = cdt->swapchain;
    cpi->indefinite_acquire = res->obj->indefinite_acquire;
-   res->obj->last_dt_idx = cpi->image = res->obj->dt_idx;
+   cpi->image = res->obj->dt_idx;
    cpi->info.sType = VK_STRUCTURE_TYPE_PRESENT_INFO_KHR;
    cpi->info.pNext = NULL;
    cpi->info.waitSemaphoreCount = 1;
@@ -812,11 +814,13 @@ zink_kopper_present_queue(struct zink_screen *screen, struct zink_resource *res)
     *  * Any other color buffers' ages are incremented by 1 if
     *    their age was previously greater than 0.
     */
-   for (int i = 0; i < cdt->swapchain->num_images; i++) {
-       if (i == res->obj->dt_idx)
-           cdt->swapchain->images[i].age = 1;
-       else if (cdt->swapchain->images[i].age > 0)
-           cdt->swapchain->images[i].age += 1;
+   if (!cdt->age_locked) {
+      for (int i = 0; i < cdt->swapchain->num_images; i++) {
+            if (i == res->obj->dt_idx)
+               cdt->swapchain->images[i].age = 1;
+            else if (cdt->swapchain->images[i].age > 0)
+               cdt->swapchain->images[i].age += 1;
+      }
    }
    if (util_queue_is_initialized(&screen->flush_queue)) {
       p_atomic_inc(&cpi->swapchain->async_presents);
@@ -832,6 +836,12 @@ zink_kopper_present_queue(struct zink_screen *screen, struct zink_resource *res)
    res->obj->dt_idx = UINT32_MAX;
 }
 
+void
+zink_kopper_update_last_written(struct zink_resource *res)
+{
+   res->obj->last_dt_idx = res->obj->dt_idx;
+}
+
 static void
 kopper_ensure_readback(struct zink_screen *screen, struct zink_resource *res)
 {
@@ -873,14 +883,30 @@ zink_kopper_acquire_readback(struct zink_context *ctx, struct zink_resource *res
    if (++cdt->readback_counter >= ZINK_READBACK_THRESHOLD)
       kopper_ensure_readback(screen, res);
    while (res->obj->dt_idx != last_dt_idx) {
-      if (res->obj->dt_idx != UINT32_MAX && !zink_kopper_present_readback(ctx, res))
-         break;
+      cdt->age_locked = true;
+      if (res->obj->dt_idx != UINT32_MAX) {
+         if (!zink_kopper_present_readback(ctx, res))
+            break;
+      } else if (util_queue_is_initialized(&screen->flush_queue)) {
+         /* AcquireNextImageKHR and QueuePresentKHR both access the swapchain, and
+          * if res->obj->dt_idx == UINT32_MAX then zink_kopper_present_readback is
+          * not called and we don't wait for the cdt->swapchain->present_fence.
+          * Still, a kopper_present might have been called in another thread, like
+          * e.g. with spec@!opengl 1.1@read-front, so we have to wait until the
+          * last call to QueuePresentKHR is finished to avoid an
+          *    UNASSIGNED-Threading-MultipleThreads-Write
+          * validation error that indicats a race condition when accessing the swapchain.
+          */
+         util_queue_fence_wait(&cdt->swapchain->present_fence);
+      }
+      cdt->age_locked = true;
       do {
          ret = kopper_acquire(screen, res, 0);
       } while (!is_swapchain_kill(ret) && (ret == VK_NOT_READY || ret == VK_TIMEOUT));
       if (is_swapchain_kill(ret)) {
          kill_swapchain(ctx, res);
          *readback = NULL;
+         cdt->age_locked = false;
          return false;
       }
    }
@@ -936,6 +962,10 @@ zink_kopper_present_readback(struct zink_context *ctx, struct zink_resource *res
    simple_mtx_lock(&screen->semaphores_lock);
    util_dynarray_append(&screen->semaphores, VkSemaphore, acquire);
    simple_mtx_unlock(&screen->semaphores_lock);
+
+   struct kopper_displaytarget *cdt = res->obj->dt;
+   cdt->age_locked = false;
+
    return zink_screen_handle_vkresult(screen, error);
 }
 
@@ -1042,8 +1072,13 @@ zink_kopper_set_swap_interval(struct pipe_screen *pscreen, struct pipe_resource
 
    zink_kopper_set_present_mode_for_interval(cdt, interval);
 
-   if (old_present_mode != cdt->present_mode)
-      update_swapchain(screen, cdt, cdt->caps.currentExtent.width, cdt->caps.currentExtent.height);
+   if (old_present_mode == cdt->present_mode)
+      return;
+   VkResult ret = update_swapchain(screen, cdt, cdt->caps.currentExtent.width, cdt->caps.currentExtent.height);
+   if (ret == VK_SUCCESS)
+      return;
+   cdt->present_mode = old_present_mode;
+   mesa_loge("zink: failed to set swap interval!");
 }
 
 int
diff --git a/src/gallium/drivers/zink/zink_kopper.h b/src/gallium/drivers/zink/zink_kopper.h
index f930f282e2e..2ebf9f3b212 100644
--- a/src/gallium/drivers/zink/zink_kopper.h
+++ b/src/gallium/drivers/zink/zink_kopper.h
@@ -95,6 +95,8 @@ struct kopper_displaytarget
    bool is_kill;
    VkPresentModeKHR present_mode;
    unsigned readback_counter;
+
+   bool age_locked; //disables buffer age during readback
 };
 
 struct zink_context;
@@ -119,6 +121,9 @@ zink_kopper_acquired(const struct kopper_displaytarget *cdt, uint32_t idx)
    return idx != UINT32_MAX && cdt->swapchain->images[idx].acquired;
 }
 
+void
+zink_kopper_update_last_written(struct zink_resource *res);
+
 struct kopper_displaytarget *
 zink_kopper_displaytarget_create(struct zink_screen *screen, unsigned tex_usage,
                                  enum pipe_format format, unsigned width,
diff --git a/src/gallium/drivers/zink/zink_pipeline.c b/src/gallium/drivers/zink/zink_pipeline.c
index dc7de3443d3..80d2f5479ce 100644
--- a/src/gallium/drivers/zink/zink_pipeline.c
+++ b/src/gallium/drivers/zink/zink_pipeline.c
@@ -273,7 +273,7 @@ zink_create_gfx_pipeline(struct zink_screen *screen,
    if (screen->info.have_EXT_color_write_enable)
       dynamicStateEnables[state_count++] = VK_DYNAMIC_STATE_COLOR_WRITE_ENABLE_EXT;
 
-   assert(state->rast_prim != MESA_PRIM_COUNT);
+   assert(state->rast_prim != MESA_PRIM_COUNT || zink_debug & ZINK_DEBUG_SHADERDB);
 
    VkPipelineRasterizationLineStateCreateInfoEXT rast_line_state;
    if (screen->info.have_EXT_line_rasterization &&
@@ -332,6 +332,8 @@ zink_create_gfx_pipeline(struct zink_screen *screen,
 
    VkGraphicsPipelineCreateInfo pci = {0};
    pci.sType = VK_STRUCTURE_TYPE_GRAPHICS_PIPELINE_CREATE_INFO;
+   if (zink_debug & ZINK_DEBUG_SHADERDB)
+      pci.flags |= VK_PIPELINE_CREATE_CAPTURE_STATISTICS_BIT_KHR;
    if (!optimize)
       pci.flags |= VK_PIPELINE_CREATE_DISABLE_OPTIMIZATION_BIT;
    if (screen->info.have_EXT_attachment_feedback_loop_dynamic_state) {
diff --git a/src/gallium/drivers/zink/zink_program.c b/src/gallium/drivers/zink/zink_program.c
index 0832772c038..cb6cdbfb753 100644
--- a/src/gallium/drivers/zink/zink_program.c
+++ b/src/gallium/drivers/zink/zink_program.c
@@ -669,14 +669,15 @@ update_gfx_shader_module_optimal(struct zink_context *ctx, struct zink_gfx_progr
 static void
 update_gfx_program_optimal(struct zink_context *ctx, struct zink_gfx_program *prog)
 {
-   const union zink_shader_key_optimal *optimal_key = (union zink_shader_key_optimal*)&prog->last_variant_hash;
-   if (ctx->gfx_pipeline_state.shader_keys_optimal.key.vs_bits != optimal_key->vs_bits) {
+   const union zink_shader_key_optimal *key = (union zink_shader_key_optimal*)&ctx->gfx_pipeline_state.optimal_key;
+   const union zink_shader_key_optimal *last_prog_key = (union zink_shader_key_optimal*)&prog->last_variant_hash;
+   if (key->vs_bits != last_prog_key->vs_bits) {
       assert(!prog->is_separable);
       bool changed = update_gfx_shader_module_optimal(ctx, prog, ctx->last_vertex_stage->info.stage);
       ctx->gfx_pipeline_state.modules_changed |= changed;
    }
-   const bool shadow_needs_shader_swizzle = optimal_key->fs.shadow_needs_shader_swizzle && (ctx->dirty_gfx_stages & BITFIELD_BIT(MESA_SHADER_FRAGMENT));
-   if (ctx->gfx_pipeline_state.shader_keys_optimal.key.fs_bits != optimal_key->fs_bits ||
+   const bool shadow_needs_shader_swizzle = last_prog_key->fs.shadow_needs_shader_swizzle && (ctx->dirty_gfx_stages & BITFIELD_BIT(MESA_SHADER_FRAGMENT));
+   if (key->fs_bits != last_prog_key->fs_bits ||
        /* always recheck shadow swizzles since they aren't directly part of the key */
        unlikely(shadow_needs_shader_swizzle)) {
       assert(!prog->is_separable);
@@ -688,7 +689,7 @@ update_gfx_program_optimal(struct zink_context *ctx, struct zink_gfx_program *pr
       }
    }
    if (prog->shaders[MESA_SHADER_TESS_CTRL] && prog->shaders[MESA_SHADER_TESS_CTRL]->non_fs.is_generated &&
-       ctx->gfx_pipeline_state.shader_keys_optimal.key.tcs_bits != optimal_key->tcs_bits) {
+       key->tcs_bits != last_prog_key->tcs_bits) {
       assert(!prog->is_separable);
       bool changed = update_gfx_shader_module_optimal(ctx, prog, MESA_SHADER_TESS_CTRL);
       ctx->gfx_pipeline_state.modules_changed |= changed;
@@ -724,12 +725,16 @@ zink_gfx_program_update_optimal(struct zink_context *ctx)
          ctx->gfx_pipeline_state.final_hash ^= ctx->curr_program->last_variant_hash;
       if (entry) {
          prog = (struct zink_gfx_program*)entry->data;
-         if (prog->is_separable && !(zink_debug & ZINK_DEBUG_NOOPT)) {
+         if (prog->is_separable) {
             /* shader variants can't be handled by separable programs: sync and compile */
-            if (!ZINK_SHADER_KEY_OPTIMAL_IS_DEFAULT(ctx->gfx_pipeline_state.optimal_key))
+            if (!ZINK_SHADER_KEY_OPTIMAL_IS_DEFAULT(ctx->gfx_pipeline_state.optimal_key) ||
+                (prog->base.uses_shobj ? !zink_can_use_shader_objects(ctx) : !zink_can_use_pipeline_libs(ctx)))
                util_queue_fence_wait(&prog->base.cache_fence);
             /* If the optimized linked pipeline is done compiling, swap it into place. */
-            if (util_queue_fence_is_signalled(&prog->base.cache_fence)) {
+            if (util_queue_fence_is_signalled(&prog->base.cache_fence) &&
+                /* but only if needed for ZINK_DEBUG=noopt */
+                (!(zink_debug & ZINK_DEBUG_NOOPT) || !ZINK_SHADER_KEY_OPTIMAL_IS_DEFAULT(ctx->gfx_pipeline_state.optimal_key) ||
+                 (prog->base.uses_shobj ? !zink_can_use_shader_objects(ctx) : !zink_can_use_pipeline_libs(ctx)))) {
                prog = replace_separable_prog(screen, entry, prog);
             }
          }
@@ -754,19 +759,19 @@ zink_gfx_program_update_optimal(struct zink_context *ctx)
       /* remove old hash */
       ctx->gfx_pipeline_state.optimal_key = zink_sanitize_optimal_key(ctx->gfx_stages, ctx->gfx_pipeline_state.shader_keys_optimal.key.val);
       ctx->gfx_pipeline_state.final_hash ^= ctx->curr_program->last_variant_hash;
-      if (ctx->curr_program->is_separable && !(zink_debug & ZINK_DEBUG_NOOPT)) {
+      if (ctx->curr_program->is_separable && !ZINK_SHADER_KEY_OPTIMAL_IS_DEFAULT(ctx->gfx_pipeline_state.optimal_key) &&
+          (ctx->curr_program->base.uses_shobj ? !zink_can_use_shader_objects(ctx) : !zink_can_use_pipeline_libs(ctx))) {
          struct zink_gfx_program *prog = ctx->curr_program;
-         if (!ZINK_SHADER_KEY_OPTIMAL_IS_DEFAULT(ctx->gfx_pipeline_state.optimal_key)) {
-            util_queue_fence_wait(&prog->base.cache_fence);
-            /* shader variants can't be handled by separable programs: sync and compile */
-            perf_debug(ctx, "zink[gfx_compile]: non-default shader variant required with separate shader object program\n");
-            struct hash_table *ht = &ctx->program_cache[zink_program_cache_stages(ctx->shader_stages)];
-            const uint32_t hash = ctx->gfx_hash;
-            simple_mtx_lock(&ctx->program_lock[zink_program_cache_stages(ctx->shader_stages)]);
-            struct hash_entry *entry = _mesa_hash_table_search_pre_hashed(ht, hash, ctx->gfx_stages);
-            ctx->curr_program = replace_separable_prog(screen, entry, prog);
-            simple_mtx_unlock(&ctx->program_lock[zink_program_cache_stages(ctx->shader_stages)]);
-         }
+
+         util_queue_fence_wait(&prog->base.cache_fence);
+         /* shader variants can't be handled by separable programs: sync and compile */
+         perf_debug(ctx, "zink[gfx_compile]: non-default shader variant required with separate shader object program\n");
+         struct hash_table *ht = &ctx->program_cache[zink_program_cache_stages(ctx->shader_stages)];
+         const uint32_t hash = ctx->gfx_hash;
+         simple_mtx_lock(&ctx->program_lock[zink_program_cache_stages(ctx->shader_stages)]);
+         struct hash_entry *entry = _mesa_hash_table_search_pre_hashed(ht, hash, ctx->gfx_stages);
+         ctx->curr_program = replace_separable_prog(screen, entry, prog);
+         simple_mtx_unlock(&ctx->program_lock[zink_program_cache_stages(ctx->shader_stages)]);
       }
       update_gfx_program_optimal(ctx, ctx->curr_program);
       /* apply new hash */
@@ -825,6 +830,23 @@ zink_gfx_program_compile_queue(struct zink_context *ctx, struct zink_gfx_pipelin
    }
 }
 
+void
+zink_program_finish(struct zink_context *ctx, struct zink_program *pg)
+{
+   util_queue_fence_wait(&pg->cache_fence);
+   if (pg->is_compute)
+      return;
+   struct zink_gfx_program *prog = (struct zink_gfx_program*)pg;
+   for (int r = 0; r < ARRAY_SIZE(prog->pipelines); ++r) {
+      for (int i = 0; i < ARRAY_SIZE(prog->pipelines[0]); ++i) {
+         hash_table_foreach(&prog->pipelines[r][i], entry) {
+            struct zink_gfx_pipeline_cache_entry *pc_entry = entry->data;
+            util_queue_fence_wait(&pc_entry->fence);
+         }
+      }
+   }
+}
+
 static void
 update_cs_shader_module(struct zink_context *ctx, struct zink_compute_program *comp)
 {
@@ -1006,6 +1028,8 @@ create_lib_cache(struct zink_gfx_program *prog, bool generated_tcs)
 {
    struct zink_gfx_lib_cache *libs = CALLOC_STRUCT(zink_gfx_lib_cache);
    libs->stages_present = prog->stages_present;
+   if (generated_tcs)
+      libs->stages_present &= ~BITFIELD_BIT(MESA_SHADER_TESS_CTRL);
    simple_mtx_init(&libs->lock, mtx_plain);
    if (generated_tcs)
       _mesa_set_init(&libs->libs, NULL, hash_pipeline_lib_generated_tcs, equals_pipeline_lib_generated_tcs);
@@ -2196,6 +2220,7 @@ zink_link_gfx_shader(struct pipe_context *pctx, void **shaders)
                                                      ctx->gfx_pipeline_state.element_state->binding_map,
                                                      shaders[MESA_SHADER_TESS_EVAL] ? VK_PRIMITIVE_TOPOLOGY_PATCH_LIST : VK_PRIMITIVE_TOPOLOGY_TRIANGLE_LIST, true, NULL);
       print_pipeline_stats(screen, pipeline);
+      VKSCR(DestroyPipeline)(screen->dev, pipeline, NULL);
    } else {
       if (zink_screen(pctx->screen)->info.have_EXT_shader_object)
          prog->base.uses_shobj = !BITSET_TEST(zshaders[MESA_SHADER_FRAGMENT]->info.system_values_read, SYSTEM_VALUE_SAMPLE_MASK_IN);
diff --git a/src/gallium/drivers/zink/zink_program.h b/src/gallium/drivers/zink/zink_program.h
index 432dcb0a9e4..6f372923135 100644
--- a/src/gallium/drivers/zink/zink_program.h
+++ b/src/gallium/drivers/zink/zink_program.h
@@ -133,6 +133,8 @@ uint32_t hash_gfx_input_dynamic(const void *key);
 
 void
 zink_gfx_program_compile_queue(struct zink_context *ctx, struct zink_gfx_pipeline_cache_entry *pc_entry);
+void
+zink_program_finish(struct zink_context *ctx, struct zink_program *pg);
 
 static inline unsigned
 get_primtype_idx(enum mesa_prim mode)
@@ -418,6 +420,21 @@ zink_can_use_pipeline_libs(const struct zink_context *ctx)
           !ctx->is_generated_gs_bound;
 }
 
+/* stricter requirements */
+ALWAYS_INLINE static bool
+zink_can_use_shader_objects(const struct zink_context *ctx)
+{
+   return
+          /* TODO: if there's ever a dynamic render extension with input attachments */
+          !ctx->gfx_pipeline_state.render_pass &&
+          ZINK_SHADER_KEY_OPTIMAL_IS_DEFAULT(ctx->gfx_pipeline_state.optimal_key) &&
+          /* TODO: is sample shading even possible to handle with GPL? */
+          !ctx->gfx_stages[MESA_SHADER_FRAGMENT]->info.fs.uses_sample_shading &&
+          !ctx->gfx_pipeline_state.force_persample_interp &&
+          !ctx->gfx_pipeline_state.min_samples &&
+          !ctx->is_generated_gs_bound;
+}
+
 bool
 zink_set_rasterizer_discard(struct zink_context *ctx, bool disable);
 void
diff --git a/src/gallium/drivers/zink/zink_program_state.hpp b/src/gallium/drivers/zink/zink_program_state.hpp
index 10e2fb94897..2cabc678660 100644
--- a/src/gallium/drivers/zink/zink_program_state.hpp
+++ b/src/gallium/drivers/zink/zink_program_state.hpp
@@ -168,7 +168,7 @@ zink_get_gfx_pipeline(struct zink_context *ctx,
           !prog->inline_variants && likely(prog->last_pipeline[rp_idx][idx]) &&
           /* this data is too big to compare in the fast-path */
           likely(!prog->shaders[MESA_SHADER_FRAGMENT]->fs.legacy_shadow_mask)) {
-         state->pipeline = prog->last_pipeline[rp_idx][idx];
+         state->pipeline = prog->last_pipeline[rp_idx][idx]->pipeline;
          return state->pipeline;
       }
    }
@@ -245,7 +245,7 @@ zink_get_gfx_pipeline(struct zink_context *ctx,
    /* update states for fastpath */
    if (DYNAMIC_STATE >= ZINK_DYNAMIC_VERTEX_INPUT) {
       prog->last_finalized_hash[rp_idx][idx] = state->final_hash;
-      prog->last_pipeline[rp_idx][idx] = cache_entry->pipeline;
+      prog->last_pipeline[rp_idx][idx] = cache_entry;
    }
    return state->pipeline;
 }
diff --git a/src/gallium/drivers/zink/zink_query.c b/src/gallium/drivers/zink/zink_query.c
index c10fb4fd8f4..e00bcafa047 100644
--- a/src/gallium/drivers/zink/zink_query.c
+++ b/src/gallium/drivers/zink/zink_query.c
@@ -971,7 +971,7 @@ zink_begin_query(struct pipe_context *pctx,
    util_dynarray_clear(&query->starts);
    query->start_offset = 0;
 
-   if (batch->in_rp) {
+   if (batch->in_rp || (query->type == PIPE_QUERY_TIME_ELAPSED)) {
       begin_query(ctx, batch, query);
    } else {
       /* never directly start queries out of renderpass, always defer */
diff --git a/src/gallium/drivers/zink/zink_resource.c b/src/gallium/drivers/zink/zink_resource.c
index ae78836d15a..84a657d5331 100644
--- a/src/gallium/drivers/zink/zink_resource.c
+++ b/src/gallium/drivers/zink/zink_resource.c
@@ -282,10 +282,8 @@ create_bci(struct zink_screen *screen, const struct pipe_resource *templ, unsign
    if (bind & ZINK_BIND_DESCRIPTOR) {
       /* gallium sizes are all uint32_t, while the total size of this buffer may exceed that limit */
       bci.usage = 0;
-      if (bind & ZINK_BIND_SAMPLER_DESCRIPTOR)
-         bci.usage |= VK_BUFFER_USAGE_SAMPLER_DESCRIPTOR_BUFFER_BIT_EXT;
-      if (bind & ZINK_BIND_RESOURCE_DESCRIPTOR)
-         bci.usage |= VK_BUFFER_USAGE_RESOURCE_DESCRIPTOR_BUFFER_BIT_EXT;
+      bci.usage |= VK_BUFFER_USAGE_SAMPLER_DESCRIPTOR_BUFFER_BIT_EXT |
+                   VK_BUFFER_USAGE_RESOURCE_DESCRIPTOR_BUFFER_BIT_EXT;
    } else {
       bci.usage = VK_BUFFER_USAGE_TRANSFER_SRC_BIT |
                   VK_BUFFER_USAGE_TRANSFER_DST_BIT |
@@ -671,7 +669,7 @@ retry:
    }
    if (want_cube) {
       ici->flags |= VK_IMAGE_CREATE_CUBE_COMPATIBLE_BIT;
-      if (get_image_usage(screen, ici, templ, bind, modifiers_count, modifiers, &mod) != ici->usage)
+      if ((get_image_usage(screen, ici, templ, bind, modifiers_count, modifiers, &mod) & ici->usage) != ici->usage)
          ici->flags &= ~VK_IMAGE_CREATE_CUBE_COMPATIBLE_BIT;
    }
 
@@ -729,7 +727,8 @@ init_ici(struct zink_screen *screen, VkImageCreateInfo *ici, const struct pipe_r
 
    case PIPE_TEXTURE_3D:
       ici->imageType = VK_IMAGE_TYPE_3D;
-      ici->flags |= VK_IMAGE_CREATE_2D_ARRAY_COMPATIBLE_BIT;
+      if (!(templ->flags & PIPE_RESOURCE_FLAG_SPARSE))
+         ici->flags |= VK_IMAGE_CREATE_2D_ARRAY_COMPATIBLE_BIT;
       if (screen->info.have_EXT_image_2d_view_of_3d)
          ici->flags |= VK_IMAGE_CREATE_2D_VIEW_COMPATIBLE_BIT_EXT;
       break;
@@ -1180,6 +1179,10 @@ resource_object_create(struct zink_screen *screen, const struct pipe_resource *t
    mai.pNext = NULL;
    mai.allocationSize = reqs.size;
    enum zink_heap heap = zink_heap_from_domain_flags(flags, aflags);
+   if (templ->flags & PIPE_RESOURCE_FLAG_MAP_COHERENT) {
+      if (!(vk_domain_from_heap(heap) & VK_MEMORY_PROPERTY_HOST_COHERENT_BIT))
+         heap = zink_heap_from_domain_flags(flags & ~VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT, aflags);
+   }
 
    VkMemoryDedicatedAllocateInfo ded_alloc_info = {
       .sType = VK_STRUCTURE_TYPE_MEMORY_DEDICATED_ALLOCATE_INFO,
@@ -1267,7 +1270,7 @@ resource_object_create(struct zink_screen *screen, const struct pipe_resource *t
       alignment = MAX2(alignment, screen->info.props.limits.minMemoryMapAlignment);
    obj->alignment = alignment;
 
-   if (zink_mem_type_idx_from_bits(screen, heap, reqs.memoryTypeBits) == UINT32_MAX) {
+   if (zink_mem_type_idx_from_types(screen, heap, reqs.memoryTypeBits) == UINT32_MAX) {
       /* not valid based on reqs; demote to more compatible type */
       switch (heap) {
       case ZINK_HEAP_DEVICE_LOCAL_VISIBLE:
@@ -1279,7 +1282,7 @@ resource_object_create(struct zink_screen *screen, const struct pipe_resource *t
       default:
          break;
       }
-      assert(zink_mem_type_idx_from_bits(screen, heap, reqs.memoryTypeBits) != UINT32_MAX);
+      assert(zink_mem_type_idx_from_types(screen, heap, reqs.memoryTypeBits) != UINT32_MAX);
    }
 
 retry:
@@ -1611,6 +1614,11 @@ add_resource_bind(struct zink_context *ctx, struct zink_resource *res, unsigned
       box.depth = util_num_layers(&res->base.b, i);
       ctx->base.resource_copy_region(&ctx->base, &res->base.b, i, 0, 0, 0, &staging.base.b, i, &box);
    }
+   if (old_obj->exportable) {
+      simple_mtx_lock(&ctx->batch.state->exportable_lock);
+      _mesa_set_remove_key(&ctx->batch.state->dmabuf_exports, &staging);
+      simple_mtx_unlock(&ctx->batch.state->exportable_lock);
+   }
    zink_resource_object_reference(screen, &old_obj, NULL);
    return true;
 }
@@ -2201,31 +2209,31 @@ zink_buffer_map(struct pipe_context *pctx,
       if (!zink_resource_usage_check_completion(screen, res, ZINK_RESOURCE_ACCESS_WRITE))
          goto success;
       usage |= PIPE_MAP_UNSYNCHRONIZED;
-   } else if (!(usage & PIPE_MAP_UNSYNCHRONIZED) &&
-              (((usage & PIPE_MAP_READ) && !(usage & PIPE_MAP_PERSISTENT) &&
+   } else if (((usage & PIPE_MAP_READ) && !(usage & PIPE_MAP_PERSISTENT) &&
                ((screen->info.mem_props.memoryTypes[res->obj->bo->base.base.placement].propertyFlags & VK_STAGING_RAM) != VK_STAGING_RAM)) ||
-              !res->obj->host_visible)) {
-      /* the above conditional catches uncached reads and non-HV writes */
-      assert(!(usage & (TC_TRANSFER_MAP_THREADED_UNSYNC)));
+              !res->obj->host_visible) {
       /* any read, non-HV write, or unmappable that reaches this point needs staging */
       if ((usage & PIPE_MAP_READ) || !res->obj->host_visible || res->base.b.flags & PIPE_RESOURCE_FLAG_DONT_MAP_DIRECTLY) {
 overwrite:
-         trans->offset = box->x % screen->info.props.limits.minMemoryMapAlignment;
+         trans->offset = box->x % MAX2(screen->info.props.limits.minMemoryMapAlignment, 1 << MIN_SLAB_ORDER);
          trans->staging_res = pipe_buffer_create(&screen->base, PIPE_BIND_LINEAR, PIPE_USAGE_STAGING, box->width + trans->offset);
          if (!trans->staging_res)
             goto fail;
          struct zink_resource *staging_res = zink_resource(trans->staging_res);
-         if (usage & PIPE_MAP_THREAD_SAFE) {
+         if (usage & (PIPE_MAP_THREAD_SAFE | PIPE_MAP_UNSYNCHRONIZED | TC_TRANSFER_MAP_THREADED_UNSYNC)) {
+            assert(ctx != screen->copy_context);
             /* this map can't access the passed context: use the copy context */
             zink_screen_lock_context(screen);
             ctx = screen->copy_context;
          }
-         zink_copy_buffer(ctx, staging_res, res, trans->offset, box->x, box->width);
+         if (usage & PIPE_MAP_READ)
+            zink_copy_buffer(ctx, staging_res, res, trans->offset, box->x, box->width);
          res = staging_res;
          usage &= ~PIPE_MAP_UNSYNCHRONIZED;
          map_offset = trans->offset;
       }
    } else if ((usage & PIPE_MAP_UNSYNCHRONIZED) && !res->obj->host_visible) {
+      assert(!(usage & PIPE_MAP_READ));
       trans->offset = box->x % screen->info.props.limits.minMemoryMapAlignment;
       trans->staging_res = pipe_buffer_create(&screen->base, PIPE_BIND_LINEAR, PIPE_USAGE_STAGING, box->width + trans->offset);
       if (!trans->staging_res)
diff --git a/src/gallium/drivers/zink/zink_resource.h b/src/gallium/drivers/zink/zink_resource.h
index f09a4e89f52..c7185d32125 100644
--- a/src/gallium/drivers/zink/zink_resource.h
+++ b/src/gallium/drivers/zink/zink_resource.h
@@ -27,9 +27,7 @@
 #include "zink_types.h"
 
 #define ZINK_MAP_TEMPORARY (PIPE_MAP_DRV_PRV << 0)
-#define ZINK_BIND_SAMPLER_DESCRIPTOR (1u << 26)
-#define ZINK_BIND_RESOURCE_DESCRIPTOR (1u << 27)
-#define ZINK_BIND_DESCRIPTOR (ZINK_BIND_SAMPLER_DESCRIPTOR | ZINK_BIND_RESOURCE_DESCRIPTOR)
+#define ZINK_BIND_DESCRIPTOR (1u << 27)
 #define ZINK_BIND_MUTABLE (1u << 28)
 #define ZINK_BIND_DMABUF (1u << 29)
 #define ZINK_BIND_TRANSIENT (1u << 30) //transient fb attachment
diff --git a/src/gallium/drivers/zink/zink_screen.c b/src/gallium/drivers/zink/zink_screen.c
index 1f06a606b66..0362d53de5e 100644
--- a/src/gallium/drivers/zink/zink_screen.c
+++ b/src/gallium/drivers/zink/zink_screen.c
@@ -837,6 +837,9 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
       return 1;
 
    case PIPE_CAP_BINDLESS_TEXTURE:
+      if (zink_descriptor_mode == ZINK_DESCRIPTOR_MODE_DB &&
+          (screen->info.db_props.maxDescriptorBufferBindings < 2 || screen->info.db_props.maxSamplerDescriptorBufferBindings < 2))
+         return 0;
       return screen->info.have_EXT_descriptor_indexing;
 
    case PIPE_CAP_TEXTURE_BUFFER_OFFSET_ALIGNMENT:
@@ -1004,7 +1007,7 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
       return 0;
 
    case PIPE_CAP_MAX_SHADER_PATCH_VARYINGS:
-      return screen->info.props.limits.maxTessellationControlPerVertexOutputComponents / 4;
+      return screen->info.props.limits.maxTessellationControlPerPatchOutputComponents / 4;
    case PIPE_CAP_MAX_VARYINGS:
       /* need to reserve up to 60 of our varying components and 16 slots for streamout */
       return MIN2(screen->info.props.limits.maxVertexOutputComponents / 4 / 2, 16);
@@ -1463,12 +1466,6 @@ static void
 zink_destroy_screen(struct pipe_screen *pscreen)
 {
    struct zink_screen *screen = zink_screen(pscreen);
-   struct zink_batch_state *bs = screen->free_batch_states;
-   while (bs) {
-      struct zink_batch_state *bs_next = bs->next;
-      zink_batch_state_destroy(screen, bs);
-      bs = bs_next;
-   }
 
 #ifdef HAVE_RENDERDOC_APP_H
    if (screen->renderdoc_capture_all && p_atomic_dec_zero(&num_screens))
@@ -1481,6 +1478,13 @@ zink_destroy_screen(struct pipe_screen *pscreen)
    if (screen->copy_context)
       screen->copy_context->base.destroy(&screen->copy_context->base);
 
+   struct zink_batch_state *bs = screen->free_batch_states;
+   while (bs) {
+      struct zink_batch_state *bs_next = bs->next;
+      zink_batch_state_destroy(screen, bs);
+      bs = bs_next;
+   }
+
    if (VK_NULL_HANDLE != screen->debugUtilsCallbackHandle) {
       VKSCR(DestroyDebugUtilsMessengerEXT)(screen->instance, screen->debugUtilsCallbackHandle, NULL);
    }
@@ -2274,7 +2278,7 @@ zink_screen_export_dmabuf_semaphore(struct zink_screen *screen, struct zink_reso
       .fd = -1,
    };
 
-   int fd;
+   int fd = -1;
    if (res->obj->is_aux) {
       fd = os_dupfd_cloexec(res->obj->handle);
    } else {
@@ -2285,6 +2289,11 @@ zink_screen_export_dmabuf_semaphore(struct zink_screen *screen, struct zink_reso
       VKSCR(GetMemoryFdKHR)(screen->dev, &fd_info, &fd);
    }
 
+   if (unlikely(fd < 0)) {
+      mesa_loge("MESA: Unable to get a valid memory fd");
+      return VK_NULL_HANDLE;
+   }
+
    int ret = drmIoctl(fd, DMA_BUF_IOCTL_EXPORT_SYNC_FILE, &export);
    if (ret) {
       if (errno == ENOTTY || errno == EBADF || errno == ENOSYS) {
@@ -2543,9 +2552,11 @@ zink_get_sparse_texture_virtual_page_size(struct pipe_screen *pscreen,
    default:
       return 0;
    }
-   VkImageUsageFlags flags = VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_TRANSFER_DST_BIT | VK_IMAGE_USAGE_TRANSFER_SRC_BIT |
-                             VK_IMAGE_USAGE_STORAGE_BIT | VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT;
-   flags |= is_zs ? VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT : VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT;
+
+   VkImageUsageFlags use_flags = VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_TRANSFER_DST_BIT | VK_IMAGE_USAGE_TRANSFER_SRC_BIT |
+                                 VK_IMAGE_USAGE_STORAGE_BIT;
+   use_flags |= is_zs ? VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT : VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT;
+   VkImageUsageFlags flags = screen->format_props[pformat].optimalTilingFeatures & use_flags;
    VkSparseImageFormatProperties props[4]; //planar?
    unsigned prop_count = ARRAY_SIZE(props);
    VKSCR(GetPhysicalDeviceSparseImageFormatProperties)(screen->pdev, format, type,
@@ -2554,11 +2565,21 @@ zink_get_sparse_texture_virtual_page_size(struct pipe_screen *pscreen,
                                                        VK_IMAGE_TILING_OPTIMAL,
                                                        &prop_count, props);
    if (!prop_count) {
-      if (pformat == PIPE_FORMAT_R9G9B9E5_FLOAT) {
-         screen->faked_e5sparse = true;
-         goto hack_it_up;
+      /* format may not support storage; try without */
+      flags &= ~VK_IMAGE_USAGE_STORAGE_BIT;
+      prop_count = ARRAY_SIZE(props);
+      VKSCR(GetPhysicalDeviceSparseImageFormatProperties)(screen->pdev, format, type,
+                                                         multi_sample ? VK_SAMPLE_COUNT_2_BIT : VK_SAMPLE_COUNT_1_BIT,
+                                                         flags,
+                                                         VK_IMAGE_TILING_OPTIMAL,
+                                                         &prop_count, props);
+      if (!prop_count) {
+         if (pformat == PIPE_FORMAT_R9G9B9E5_FLOAT) {
+            screen->faked_e5sparse = true;
+            goto hack_it_up;
+         }
+         return 0;
       }
-      return 0;
    }
 
    if (size) {
@@ -3196,12 +3217,12 @@ zink_internal_create_screen(const struct pipe_screen_config *config, int64_t dev
       }
    }
 
-   vk_instance_dispatch_table_load(&screen->vk.instance,
-                                   screen->vk_GetInstanceProcAddr,
-                                   screen->instance);
-   vk_physical_device_dispatch_table_load(&screen->vk.physical_device,
-                                          screen->vk_GetInstanceProcAddr,
-                                          screen->instance);
+   vk_instance_uncompacted_dispatch_table_load(&screen->vk.instance,
+                                                screen->vk_GetInstanceProcAddr,
+                                                screen->instance);
+   vk_physical_device_uncompacted_dispatch_table_load(&screen->vk.physical_device,
+                                                      screen->vk_GetInstanceProcAddr,
+                                                      screen->instance);
 
    zink_verify_instance_extensions(screen);
 
@@ -3298,9 +3319,9 @@ zink_internal_create_screen(const struct pipe_screen_config *config, int64_t dev
    if (!screen->dev)
       goto fail;
 
-   vk_device_dispatch_table_load(&screen->vk.device,
-                                 screen->vk_GetDeviceProcAddr,
-                                 screen->dev);
+   vk_device_uncompacted_dispatch_table_load(&screen->vk.device,
+                                             screen->vk_GetDeviceProcAddr,
+                                             screen->dev);
 
    init_queue(screen);
 
@@ -3465,20 +3486,11 @@ zink_internal_create_screen(const struct pipe_screen_config *config, int64_t dev
          mesa_logw("zink: bug detected: inputAttachmentDescriptorSize(%u) > %u", (unsigned)screen->info.db_props.inputAttachmentDescriptorSize, ZINK_FBFETCH_DESCRIPTOR_SIZE);
          can_db = false;
       }
-      if (screen->compact_descriptors) {
-         if (screen->info.db_props.maxDescriptorBufferBindings < 3) {
-            if (zink_descriptor_mode == ZINK_DESCRIPTOR_MODE_DB) {
-               mesa_loge("Cannot use db descriptor mode with compact descriptors with maxDescriptorBufferBindings < 3");
-               goto fail;
-            }
-            can_db = false;
-         }
-      } else {
-         if (screen->info.db_props.maxDescriptorBufferBindings < 5) {
-            if (zink_descriptor_mode == ZINK_DESCRIPTOR_MODE_DB) {
-               mesa_loge("Cannot use db descriptor mode with maxDescriptorBufferBindings < 5");
-               goto fail;
-            }
+      if (screen->info.db_props.maxDescriptorBufferBindings < 2 || screen->info.db_props.maxSamplerDescriptorBufferBindings < 2) {
+         if (zink_descriptor_mode == ZINK_DESCRIPTOR_MODE_DB) {
+            /* allow for testing, but disable bindless */
+            mesa_logw("Cannot use bindless and db descriptor mode with (maxDescriptorBufferBindings||maxSamplerDescriptorBufferBindings) < 2");
+         } else {
             can_db = false;
          }
       }
diff --git a/src/gallium/drivers/zink/zink_screen.h b/src/gallium/drivers/zink/zink_screen.h
index 886700b65ad..c907bc6e85d 100644
--- a/src/gallium/drivers/zink/zink_screen.h
+++ b/src/gallium/drivers/zink/zink_screen.h
@@ -61,6 +61,7 @@ static inline bool
 zink_screen_check_last_finished(struct zink_screen *screen, uint32_t batch_id)
 {
    const uint32_t check_id = (uint32_t)batch_id;
+   assert(check_id);
    /* last_finished may have wrapped */
    if (screen->last_finished < UINT_MAX / 2) {
       /* last_finished has wrapped, batch_id has not */
diff --git a/src/gallium/drivers/zink/zink_synchronization.cpp b/src/gallium/drivers/zink/zink_synchronization.cpp
index 05a904f3231..22fc3fc66e5 100644
--- a/src/gallium/drivers/zink/zink_synchronization.cpp
+++ b/src/gallium/drivers/zink/zink_synchronization.cpp
@@ -673,6 +673,7 @@ zink_resource_buffer_barrier(struct zink_context *ctx, struct zink_resource *res
          } else {
             bmb.srcAccessMask = res->obj->access;
          }
+         bmb.dstAccessMask = flags;
          VKCTX(CmdPipelineBarrier)(
             cmdbuf,
             stages,
diff --git a/src/gallium/drivers/zink/zink_types.h b/src/gallium/drivers/zink/zink_types.h
index 41eaa21215c..ba4aea002e8 100644
--- a/src/gallium/drivers/zink/zink_types.h
+++ b/src/gallium/drivers/zink/zink_types.h
@@ -1137,7 +1137,7 @@ struct zink_gfx_program {
    uint32_t last_variant_hash;
 
    uint32_t last_finalized_hash[2][4]; //[dynamic, renderpass][primtype idx]
-   VkPipeline last_pipeline[2][4]; //[dynamic, renderpass][primtype idx]
+   struct zink_gfx_pipeline_cache_entry *last_pipeline[2][4]; //[dynamic, renderpass][primtype idx]
 
    struct zink_gfx_lib_cache *libs;
 };
@@ -1519,7 +1519,7 @@ struct zink_screen {
    bool renderdoc_capture_all;
 #endif
 
-   struct vk_dispatch_table vk;
+   struct vk_uncompacted_dispatch_table vk;
 
    void (*buffer_barrier)(struct zink_context *ctx, struct zink_resource *res, VkAccessFlags flags, VkPipelineStageFlags pipeline);
    void (*image_barrier)(struct zink_context *ctx, struct zink_resource *res, VkImageLayout new_layout, VkAccessFlags flags, VkPipelineStageFlags pipeline);
diff --git a/src/gallium/frontends/clover/llvm/invocation.cpp b/src/gallium/frontends/clover/llvm/invocation.cpp
index 6ab32befbcd..e899b205d22 100644
--- a/src/gallium/frontends/clover/llvm/invocation.cpp
+++ b/src/gallium/frontends/clover/llvm/invocation.cpp
@@ -513,6 +513,7 @@ namespace {
       LLVMRunPasses(wrap(&mod), opt_str, tm, opts);
 
       LLVMDisposeTargetMachine(tm);
+      LLVMDisposePassBuilderOptions(opts);
    }
 
    std::unique_ptr<Module>
diff --git a/src/gallium/frontends/dri/dri2.c b/src/gallium/frontends/dri/dri2.c
index 751a15d43a5..994e7e6c035 100644
--- a/src/gallium/frontends/dri/dri2.c
+++ b/src/gallium/frontends/dri/dri2.c
@@ -2385,7 +2385,7 @@ dri2_init_screen(struct dri_screen *screen)
       pscreen = pipe_loader_create_screen(screen->dev);
 
    if (!pscreen)
-       goto fail;
+       return NULL;
 
    dri_init_options(screen);
    screen->throttle = pscreen->get_param(pscreen, PIPE_CAP_THROTTLE);
@@ -2419,7 +2419,7 @@ dri2_init_screen(struct dri_screen *screen)
    return configs;
 
 fail:
-   dri_release_screen(screen);
+   pipe_loader_release(&screen->dev, 1);
 
    return NULL;
 }
diff --git a/src/gallium/frontends/dri/dri_drawable.h b/src/gallium/frontends/dri/dri_drawable.h
index 55a7ec82684..f4b951fc104 100644
--- a/src/gallium/frontends/dri/dri_drawable.h
+++ b/src/gallium/frontends/dri/dri_drawable.h
@@ -90,6 +90,7 @@ struct dri_drawable
    struct kopper_loader_info info;
    __DRIimage   *image; //texture_from_pixmap
    bool is_window;
+   bool window_valid;
    bool has_modifiers;
 
    /* hooks filled in by dri2 & drisw */
diff --git a/src/gallium/frontends/dri/drisw.c b/src/gallium/frontends/dri/drisw.c
index 7c6a75a228b..4c9b66cedfb 100644
--- a/src/gallium/frontends/dri/drisw.c
+++ b/src/gallium/frontends/dri/drisw.c
@@ -546,6 +546,8 @@ drisw_init_screen(struct dri_screen *screen)
    struct pipe_screen *pscreen = NULL;
    const struct drisw_loader_funcs *lf = &drisw_lf;
 
+   (void) mtx_init(&screen->opencl_func_mutex, mtx_plain);
+
    screen->swrast_no_present = debug_get_option_swrast_no_present();
 
    if (loader->base.version >= 4) {
@@ -565,7 +567,7 @@ drisw_init_screen(struct dri_screen *screen)
       pscreen = pipe_loader_create_screen(screen->dev);
 
    if (!pscreen)
-      goto fail;
+      return NULL;
 
    dri_init_options(screen);
    configs = dri_init_screen(screen, pscreen);
@@ -593,7 +595,7 @@ drisw_init_screen(struct dri_screen *screen)
 
    return configs;
 fail:
-   dri_release_screen(screen);
+   pipe_loader_release(&screen->dev, 1);
    return NULL;
 }
 
diff --git a/src/gallium/frontends/dri/kopper.c b/src/gallium/frontends/dri/kopper.c
index 70b9980b6f5..d247f2223c7 100644
--- a/src/gallium/frontends/dri/kopper.c
+++ b/src/gallium/frontends/dri/kopper.c
@@ -115,6 +115,8 @@ kopper_init_screen(struct dri_screen *screen)
    const __DRIconfig **configs;
    struct pipe_screen *pscreen = NULL;
 
+   (void) mtx_init(&screen->opencl_func_mutex, mtx_plain);
+
    if (!screen->kopper_loader) {
       fprintf(stderr, "mesa: Kopper interface not found!\n"
                       "      Ensure the versions of %s built with this version of Zink are\n"
@@ -134,7 +136,7 @@ kopper_init_screen(struct dri_screen *screen)
       pscreen = pipe_loader_create_screen(screen->dev);
 
    if (!pscreen)
-      goto fail;
+      return NULL;
 
    dri_init_options(screen);
    screen->unwrapped_screen = trace_screen_unwrap(pscreen);
@@ -167,7 +169,7 @@ kopper_init_screen(struct dri_screen *screen)
 
    return configs;
 fail:
-   dri_release_screen(screen);
+   pipe_loader_release(&screen->dev, 1);
    return NULL;
 }
 
@@ -606,6 +608,7 @@ XXX do this once swapinterval is hooked up
             assert(data);
             drawable->textures[statts[i]] =
                screen->base.screen->resource_create_drawable(screen->base.screen, &templ, data);
+            drawable->window_valid = !!drawable->textures[statts[i]];
          }
 #ifdef VK_USE_PLATFORM_XCB_KHR
          else if (is_pixmap && statts[i] == ST_ATTACHMENT_FRONT_LEFT && !screen->is_sw) {
@@ -928,6 +931,9 @@ kopperSetSwapInterval(__DRIdrawable *dPriv, int interval)
                                 drawable->textures[ST_ATTACHMENT_BACK_LEFT] :
                                 drawable->textures[ST_ATTACHMENT_FRONT_LEFT];
 
+   /* can't set swap interval on non-windows */
+   if (!drawable->window_valid)
+      return;
    /* the conditional is because we can be called before buffer allocation.  If
     * we're before allocation, then the initial_swap_interval will be used when
     * the swapchain is eventually created.
@@ -946,6 +952,10 @@ kopperQueryBufferAge(__DRIdrawable *dPriv)
                                 drawable->textures[ST_ATTACHMENT_BACK_LEFT] :
                                 drawable->textures[ST_ATTACHMENT_FRONT_LEFT];
 
+   /* can't get buffer age from non-window swapchain */
+   if (!drawable->window_valid)
+      return 0;
+
    /* Wait for glthread to finish because we can't use pipe_context from
     * multiple threads.
     */
diff --git a/src/gallium/frontends/lavapipe/lvp_descriptor_set.c b/src/gallium/frontends/lavapipe/lvp_descriptor_set.c
index fa198f3e575..7e176a42324 100644
--- a/src/gallium/frontends/lavapipe/lvp_descriptor_set.c
+++ b/src/gallium/frontends/lavapipe/lvp_descriptor_set.c
@@ -1070,20 +1070,28 @@ VKAPI_ATTR void VKAPI_CALL lvp_GetDescriptorEXT(
       if (info && info->imageView) {
          LVP_FROM_HANDLE(lvp_image_view, iview, info->imageView);
 
-         lp_jit_texture_from_pipe(&desc->texture, iview->planes[0].sv);
-         desc->functions = iview->planes[0].texture_handle->functions;
-
-         if (info->sampler) {
-            LVP_FROM_HANDLE(lvp_sampler, sampler, info->sampler);
-            desc->sampler = sampler->desc.sampler;
-            desc->texture.sampler_index = sampler->desc.texture.sampler_index;
-         } else {
-            lp_jit_sampler_from_pipe(&desc->sampler, &sampler);
-            desc->texture.sampler_index = 0;
+         unsigned plane_count = iview->plane_count;
+
+         for (unsigned p = 0; p < plane_count; p++) {
+            lp_jit_texture_from_pipe(&desc[p].texture, iview->planes[p].sv);
+            desc[p].functions = iview->planes[p].texture_handle->functions;
+
+            if (info->sampler) {
+               LVP_FROM_HANDLE(lvp_sampler, sampler, info->sampler);
+               desc[p].sampler = sampler->desc.sampler;
+                 desc[p].texture.sampler_index = sampler->desc.texture.sampler_index;
+            } else {
+               lp_jit_sampler_from_pipe(&desc->sampler, &sampler);
+               desc[p].texture.sampler_index = 0;
+            }
          }
       } else {
-         desc->functions = device->null_texture_handle->functions;
-         desc->texture.sampler_index = 0;
+         unsigned plane_count = size / sizeof(struct lp_descriptor);
+
+         for (unsigned p = 0; p < plane_count; p++) {
+            desc[p].functions = device->null_texture_handle->functions;
+            desc[p].texture.sampler_index = 0;
+         }
       }
 
       break;
@@ -1092,11 +1100,20 @@ VKAPI_ATTR void VKAPI_CALL lvp_GetDescriptorEXT(
    case VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE: {
       if (pCreateInfo->data.pSampledImage && pCreateInfo->data.pSampledImage->imageView) {
          LVP_FROM_HANDLE(lvp_image_view, iview, pCreateInfo->data.pSampledImage->imageView);
-         lp_jit_texture_from_pipe(&desc->texture, iview->planes[0].sv);
-         desc->functions = iview->planes[0].texture_handle->functions;
+
+         unsigned plane_count = iview->plane_count;
+
+         for (unsigned p = 0; p < plane_count; p++) {
+            lp_jit_texture_from_pipe(&desc[p].texture, iview->planes[p].sv);
+            desc[p].functions = iview->planes[p].texture_handle->functions;
+         }
       } else {
-         desc->functions = device->null_texture_handle->functions;
-         desc->texture.sampler_index = 0;
+         unsigned plane_count = size / sizeof(struct lp_descriptor);
+
+         for (unsigned p = 0; p < plane_count; p++) {
+            desc[p].functions = device->null_texture_handle->functions;
+            desc[p].texture.sampler_index = 0;
+         }
       }
       break;
    }
@@ -1106,10 +1123,18 @@ VKAPI_ATTR void VKAPI_CALL lvp_GetDescriptorEXT(
    case VK_DESCRIPTOR_TYPE_INPUT_ATTACHMENT: {
       if (pCreateInfo->data.pStorageImage && pCreateInfo->data.pStorageImage->imageView) {
          LVP_FROM_HANDLE(lvp_image_view, iview, pCreateInfo->data.pStorageImage->imageView);
-         lp_jit_image_from_pipe(&desc->image, &iview->planes[0].iv);
-         desc->functions = iview->planes[0].image_handle->functions;
+
+         unsigned plane_count = iview->plane_count;
+
+         for (unsigned p = 0; p < plane_count; p++) {
+            lp_jit_image_from_pipe(&desc[p].image, &iview->planes[p].iv);
+            desc[p].functions = iview->planes[p].image_handle->functions;
+         }
       } else {
-         desc->functions = device->null_image_handle->functions;
+         unsigned plane_count = size / sizeof(struct lp_descriptor);
+
+         for (unsigned p = 0; p < plane_count; p++)
+            desc[p].functions = device->null_image_handle->functions;
       }
       break;
    }
diff --git a/src/gallium/frontends/lavapipe/lvp_execute.c b/src/gallium/frontends/lavapipe/lvp_execute.c
index a0c978f858f..85dd46d04b8 100644
--- a/src/gallium/frontends/lavapipe/lvp_execute.c
+++ b/src/gallium/frontends/lavapipe/lvp_execute.c
@@ -88,6 +88,7 @@ struct rendering_state {
    bool blend_dirty;
    bool rs_dirty;
    bool dsa_dirty;
+   bool dsa_no_stencil;
    bool stencil_ref_dirty;
    bool clip_state_dirty;
    bool blend_color_dirty;
@@ -470,8 +471,16 @@ static void emit_state(struct rendering_state *state)
    }
 
    if (state->dsa_dirty) {
+      bool s0_enabled = state->dsa_state.stencil[0].enabled;
+      bool s1_enabled = state->dsa_state.stencil[1].enabled;
+      if (state->dsa_no_stencil) {
+         state->dsa_state.stencil[0].enabled = false;
+         state->dsa_state.stencil[1].enabled = false;
+      }
       cso_set_depth_stencil_alpha(state->cso, &state->dsa_state);
       state->dsa_dirty = false;
+      state->dsa_state.stencil[0].enabled = s0_enabled;
+      state->dsa_state.stencil[1].enabled = s1_enabled;
    }
 
    if (state->sample_mask_dirty) {
@@ -1784,6 +1793,8 @@ handle_begin_rendering(struct vk_cmd_queue_entry *cmd,
 
    render_att_init(&state->depth_att, info->pDepthAttachment, state->poison_mem, false);
    render_att_init(&state->stencil_att, info->pStencilAttachment, state->poison_mem, true);
+   state->dsa_no_stencil = !state->stencil_att.imgv;
+   state->dsa_dirty = true;
    if (state->depth_att.imgv || state->stencil_att.imgv) {
       assert(state->depth_att.imgv == NULL ||
              state->stencil_att.imgv == NULL ||
@@ -2647,7 +2658,7 @@ static void handle_index_buffer2(struct vk_cmd_queue_entry *cmd,
       state->index_buffer = lvp_buffer_from_handle(ib->buffer)->bo;
    } else {
       state->index_size = 4;
-      state->index_buffer_size = sizeof(uint32_t);
+      state->index_buffer_size = UINT32_MAX;
       state->index_offset = 0;
       state->index_buffer = state->device->zero_buffer;
    }
diff --git a/src/gallium/frontends/lavapipe/lvp_query.c b/src/gallium/frontends/lavapipe/lvp_query.c
index bc0a3680c52..6511d4699d9 100644
--- a/src/gallium/frontends/lavapipe/lvp_query.c
+++ b/src/gallium/frontends/lavapipe/lvp_query.c
@@ -168,7 +168,7 @@ VKAPI_ATTR VkResult VKAPI_CALL lvp_GetQueryPoolResults(
                *dest32++ = (uint32_t)
                   MIN2(result.so_statistics.primitives_storage_needed, UINT32_MAX);
             } else {
-               *dest32++ = (uint32_t) MIN2(result.u64, UINT32_MAX);
+               *dest32++ = (uint32_t) (result.u64 & UINT32_MAX);
             }
          } else {
             if (pool->type == VK_QUERY_TYPE_TRANSFORM_FEEDBACK_STREAM_EXT) {
diff --git a/src/gallium/frontends/nine/device9.c b/src/gallium/frontends/nine/device9.c
index 16c9a8bc234..9255f8d4f56 100644
--- a/src/gallium/frontends/nine/device9.c
+++ b/src/gallium/frontends/nine/device9.c
@@ -1062,6 +1062,7 @@ NineDevice9_Reset( struct NineDevice9 *This,
     /* XXX: better use GetBackBuffer here ? */
 
     This->device_needs_reset = (hr != D3D_OK);
+    This->in_scene = FALSE; /* Not sure if should be done also for ResetEx */
     return hr;
 }
 
diff --git a/src/gallium/frontends/nine/iunknown.c b/src/gallium/frontends/nine/iunknown.c
index 18f477787fd..92ec9686034 100644
--- a/src/gallium/frontends/nine/iunknown.c
+++ b/src/gallium/frontends/nine/iunknown.c
@@ -48,6 +48,7 @@ NineUnknown_ctor( struct NineUnknown *This,
         This->forward = false;
         This->bind = 0;
     }
+    This->has_bind_or_refs = This->bind + This->refs;
 
     This->container = pParams->container;
     This->device = pParams->device;
@@ -119,6 +120,7 @@ NineUnknown_AddRef( struct NineUnknown *This )
         r = p_atomic_inc_return(&This->refs);
 
     if (r == 1) {
+        p_atomic_inc(&This->has_bind_or_refs);
         if (This->device)
             NineUnknown_AddRef(NineUnknown(This->device));
     }
@@ -142,9 +144,11 @@ NineUnknown_Release( struct NineUnknown *This )
 
     if (r == 0) {
         struct NineDevice9 *device = This->device;
+        UINT b_or_ref = p_atomic_dec_return(&This->has_bind_or_refs);
         /* Containers (here with !forward) take care of item destruction */
 
-        if (!This->container && This->bind == 0) {
+        if (!This->container && b_or_ref == 0) {
+            assert(p_atomic_read(&This->bind) == 0);
             This->dtor(This);
         }
         if (device) {
@@ -166,8 +170,10 @@ NineUnknown_ReleaseWithDtorLock( struct NineUnknown *This )
 
     if (r == 0) {
         struct NineDevice9 *device = This->device;
+        UINT b_or_ref = p_atomic_dec_return(&This->has_bind_or_refs);
         /* Containers (here with !forward) take care of item destruction */
-        if (!This->container && This->bind == 0) {
+        if (!This->container && b_or_ref == 0) {
+            assert(p_atomic_read(&This->bind) == 0);
             NineLockGlobalMutex();
             This->dtor(This);
             NineUnlockGlobalMutex();
diff --git a/src/gallium/frontends/nine/iunknown.h b/src/gallium/frontends/nine/iunknown.h
index cdf27fa9551..822f8040fc8 100644
--- a/src/gallium/frontends/nine/iunknown.h
+++ b/src/gallium/frontends/nine/iunknown.h
@@ -47,6 +47,7 @@ struct NineUnknown
 
     int32_t refs; /* external reference count */
     int32_t bind; /* internal bind count */
+    int32_t has_bind_or_refs; /* 0 if no ref, 1 if bind or ref, 2 if both */
     bool forward; /* whether to forward references to the container */
 
     /* container: for surfaces and volumes only.
@@ -130,7 +131,7 @@ NineUnknown_FreePrivateData( struct NineUnknown *This,
 static inline void
 NineUnknown_Destroy( struct NineUnknown *This )
 {
-    assert(!(This->refs | This->bind));
+    assert(!(This->refs | This->bind) && !This->has_bind_or_refs);
     This->dtor(This);
 }
 
@@ -140,6 +141,8 @@ NineUnknown_Bind( struct NineUnknown *This )
     UINT b = p_atomic_inc_return(&This->bind);
     assert(b);
 
+    if (b == 1)
+        p_atomic_inc(&This->has_bind_or_refs);
     if (b == 1 && This->forward)
         NineUnknown_Bind(This->container);
 
@@ -150,10 +153,13 @@ static inline UINT
 NineUnknown_Unbind( struct NineUnknown *This )
 {
     UINT b = p_atomic_dec_return(&This->bind);
+    UINT b_or_ref = 1;
 
+    if (b == 0)
+        b_or_ref = p_atomic_dec_return(&This->has_bind_or_refs);
     if (b == 0 && This->forward)
         NineUnknown_Unbind(This->container);
-    else if (b == 0 && This->refs == 0 && !This->container)
+    else if (b_or_ref == 0 && !This->container)
         This->dtor(This);
 
     return b;
@@ -173,7 +179,7 @@ NineUnknown_Detach( struct NineUnknown *This )
     assert(This->container && !This->forward);
 
     This->container = NULL;
-    if (!(This->refs | This->bind))
+    if (!(This->has_bind_or_refs))
         This->dtor(This);
 }
 
diff --git a/src/gallium/frontends/nine/nine_ff.c b/src/gallium/frontends/nine/nine_ff.c
index a5182fbd0a8..e32f7c70342 100644
--- a/src/gallium/frontends/nine/nine_ff.c
+++ b/src/gallium/frontends/nine/nine_ff.c
@@ -1953,7 +1953,7 @@ nine_ff_load_lights(struct NineDevice9 *device)
         dst[19].z = dst[25].z * mtl->Ambient.b + mtl->Emissive.b;
     }
 
-    if (!(context->changed.group & NINE_STATE_FF_LIGHTING))
+    if (!(context->changed.group & NINE_STATE_FF_LIGHTING) && !IS_D3DTS_DIRTY(context, VIEW))
         return;
 
     for (l = 0; l < context->ff.num_lights_active; ++l) {
diff --git a/src/gallium/frontends/nine/nine_ff.h b/src/gallium/frontends/nine/nine_ff.h
index 6143896b021..cd4fe94f676 100644
--- a/src/gallium/frontends/nine/nine_ff.h
+++ b/src/gallium/frontends/nine/nine_ff.h
@@ -83,20 +83,21 @@ nine_ff_get_projected_key(struct nine_context *context, unsigned num_stages)
     for (s = 0; s < num_stages; ++s) {
         unsigned gen = (context->ff.tex_stage[s][D3DTSS_TEXCOORDINDEX] >> 16) + 1;
         unsigned dim = context->ff.tex_stage[s][D3DTSS_TEXTURETRANSFORMFLAGS] & 0x7;
+        unsigned idx = context->ff.tex_stage[s][D3DTSS_TEXCOORDINDEX] & 7;
         unsigned proj = !!(context->ff.tex_stage[s][D3DTSS_TEXTURETRANSFORMFLAGS] & D3DTTFF_PROJECTED);
 
-        if (!context->vs) {
+        if (!context->programmable_vs) {
             if (dim > 4)
-                dim = input_texture_coord[s];
+                dim = input_texture_coord[idx];
 
             if (!dim && gen == NINED3DTSS_TCI_PASSTHRU)
-                dim = input_texture_coord[s];
+                dim = input_texture_coord[idx];
             else if (!dim)
                 dim = 4;
 
             if (dim == 1) /* NV behaviour */
                 proj = 0;
-            if (dim > input_texture_coord[s] && gen == NINED3DTSS_TCI_PASSTHRU)
+            if (dim > input_texture_coord[idx] && gen == NINED3DTSS_TCI_PASSTHRU)
                 proj = 0;
         } else {
             dim = 4;
diff --git a/src/gallium/frontends/rusticl/core/event.rs b/src/gallium/frontends/rusticl/core/event.rs
index 6f45bb59140..29b43fe30c7 100644
--- a/src/gallium/frontends/rusticl/core/event.rs
+++ b/src/gallium/frontends/rusticl/core/event.rs
@@ -10,6 +10,7 @@ use mesa_rust_util::static_assert;
 use rusticl_opencl_gen::*;
 
 use std::collections::HashSet;
+use std::mem;
 use std::slice;
 use std::sync::Arc;
 use std::sync::Condvar;
@@ -109,7 +110,7 @@ impl Event {
         self.state().status
     }
 
-    fn set_status(&self, lock: &mut MutexGuard<EventMutState>, new: cl_int) {
+    fn set_status(&self, mut lock: MutexGuard<EventMutState>, new: cl_int) {
         lock.status = new;
 
         // signal on completion or an error
@@ -117,16 +118,22 @@ impl Event {
             self.cv.notify_all();
         }
 
-        if [CL_COMPLETE, CL_RUNNING, CL_SUBMITTED].contains(&(new as u32)) {
-            if let Some(cbs) = lock.cbs.get_mut(new as usize) {
-                cbs.drain(..).for_each(|cb| cb.call(self, new));
+        // on error we need to call the CL_COMPLETE callbacks
+        let cb_idx = if new < 0 { CL_COMPLETE } else { new as u32 };
+
+        if [CL_COMPLETE, CL_RUNNING, CL_SUBMITTED].contains(&cb_idx) {
+            if let Some(cbs) = lock.cbs.get_mut(cb_idx as usize) {
+                let cbs = mem::take(cbs);
+                // applications might want to access the event in the callback, so drop the lock
+                // before calling into the callbacks.
+                drop(lock);
+                cbs.into_iter().for_each(|cb| cb.call(self, new));
             }
         }
     }
 
     pub fn set_user_status(&self, status: cl_int) {
-        let mut lock = self.state();
-        self.set_status(&mut lock, status);
+        self.set_status(self.state(), status);
     }
 
     pub fn is_error(&self) -> bool {
@@ -172,10 +179,8 @@ impl Event {
     }
 
     pub(super) fn signal(&self) {
-        let mut lock = self.state();
-
-        self.set_status(&mut lock, CL_RUNNING as cl_int);
-        self.set_status(&mut lock, CL_COMPLETE as cl_int);
+        self.set_status(self.state(), CL_RUNNING as cl_int);
+        self.set_status(self.state(), CL_COMPLETE as cl_int);
     }
 
     pub fn wait(&self) -> cl_int {
@@ -231,7 +236,7 @@ impl Event {
                 lock.time_start = query_start.unwrap().read_blocked();
                 lock.time_end = query_end.unwrap().read_blocked();
             }
-            self.set_status(&mut lock, new);
+            self.set_status(lock, new);
         }
     }
 
@@ -269,6 +274,27 @@ impl Event {
     }
 }
 
+impl Drop for Event {
+    // implement drop in order to prevent stack overflows of long dependency chains.
+    //
+    // This abuses the fact that `Arc::into_inner` only succeeds when there is one strong reference
+    // so we turn a recursive drop chain into a drop list for events having no other references.
+    fn drop(&mut self) {
+        if self.deps.is_empty() {
+            return;
+        }
+
+        let mut deps_list = vec![mem::take(&mut self.deps)];
+        while let Some(deps) = deps_list.pop() {
+            for dep in deps {
+                if let Some(mut dep) = Arc::into_inner(dep) {
+                    deps_list.push(mem::take(&mut dep.deps));
+                }
+            }
+        }
+    }
+}
+
 // TODO worker thread per device
 // Condvar to wait on new events to work on
 // notify condvar when flushing queue events to worker
diff --git a/src/gallium/frontends/rusticl/core/kernel.rs b/src/gallium/frontends/rusticl/core/kernel.rs
index 05296c0cd0e..63e9f7e864c 100644
--- a/src/gallium/frontends/rusticl/core/kernel.rs
+++ b/src/gallium/frontends/rusticl/core/kernel.rs
@@ -458,22 +458,8 @@ fn lower_and_optimize_nir(
     let mut args = KernelArg::from_spirv_nir(args, nir);
     let mut internal_args = Vec::new();
 
-    let dv_opts = nir_remove_dead_variables_options {
-        can_remove_var: Some(can_remove_var),
-        can_remove_var_data: ptr::null_mut(),
-    };
-    nir_pass!(
-        nir,
-        nir_remove_dead_variables,
-        nir_variable_mode::nir_var_uniform
-            | nir_variable_mode::nir_var_image
-            | nir_variable_mode::nir_var_mem_constant
-            | nir_variable_mode::nir_var_mem_shared
-            | nir_variable_mode::nir_var_function_temp,
-        &dv_opts,
-    );
-
-    // asign locations for inline samplers
+    // asign locations for inline samplers.
+    // IMPORTANT: this needs to happen before nir_remove_dead_variables.
     let mut last_loc = -1;
     for v in nir
         .variables_with_mode(nir_variable_mode::nir_var_uniform | nir_variable_mode::nir_var_image)
@@ -501,6 +487,21 @@ fn lower_and_optimize_nir(
         }
     }
 
+    let dv_opts = nir_remove_dead_variables_options {
+        can_remove_var: Some(can_remove_var),
+        can_remove_var_data: ptr::null_mut(),
+    };
+    nir_pass!(
+        nir,
+        nir_remove_dead_variables,
+        nir_variable_mode::nir_var_uniform
+            | nir_variable_mode::nir_var_image
+            | nir_variable_mode::nir_var_mem_constant
+            | nir_variable_mode::nir_var_mem_shared
+            | nir_variable_mode::nir_var_function_temp,
+        &dv_opts,
+    );
+
     nir_pass!(nir, nir_lower_readonly_images_to_tex, true);
     nir_pass!(
         nir,
diff --git a/src/gallium/frontends/rusticl/core/platform.rs b/src/gallium/frontends/rusticl/core/platform.rs
index 6e948e358c5..74a0f15b842 100644
--- a/src/gallium/frontends/rusticl/core/platform.rs
+++ b/src/gallium/frontends/rusticl/core/platform.rs
@@ -7,6 +7,8 @@ use mesa_rust_gen::*;
 use rusticl_opencl_gen::*;
 
 use std::env;
+use std::ptr::addr_of;
+use std::ptr::addr_of_mut;
 use std::sync::Arc;
 use std::sync::Once;
 
@@ -71,7 +73,8 @@ static mut PLATFORM_FEATURES: PlatformFeatures = PlatformFeatures {
 };
 
 fn load_env() {
-    let debug = unsafe { &mut PLATFORM_DBG };
+    // SAFETY: no other references exist at this point
+    let debug = unsafe { &mut *addr_of_mut!(PLATFORM_DBG) };
     if let Ok(debug_flags) = env::var("RUSTICL_DEBUG") {
         for flag in debug_flags.split(',') {
             match flag {
@@ -85,7 +88,8 @@ fn load_env() {
         }
     }
 
-    let features = unsafe { &mut PLATFORM_FEATURES };
+    // SAFETY: no other references exist at this point
+    let features = unsafe { &mut *addr_of_mut!(PLATFORM_FEATURES) };
     if let Ok(feature_flags) = env::var("RUSTICL_FEATURES") {
         for flag in feature_flags.split(',') {
             match flag {
@@ -106,17 +110,17 @@ impl Platform {
     pub fn get() -> &'static Self {
         debug_assert!(PLATFORM_ONCE.is_completed());
         // SAFETY: no mut references exist at this point
-        unsafe { &PLATFORM }
+        unsafe { &*addr_of!(PLATFORM) }
     }
 
     pub fn dbg() -> &'static PlatformDebug {
         debug_assert!(PLATFORM_ENV_ONCE.is_completed());
-        unsafe { &PLATFORM_DBG }
+        unsafe { &*addr_of!(PLATFORM_DBG) }
     }
 
     pub fn features() -> &'static PlatformFeatures {
         debug_assert!(PLATFORM_ENV_ONCE.is_completed());
-        unsafe { &PLATFORM_FEATURES }
+        unsafe { &*addr_of!(PLATFORM_FEATURES) }
     }
 
     fn init(&mut self) {
diff --git a/src/gallium/frontends/rusticl/core/program.rs b/src/gallium/frontends/rusticl/core/program.rs
index eeb9c113538..93465c71c72 100644
--- a/src/gallium/frontends/rusticl/core/program.rs
+++ b/src/gallium/frontends/rusticl/core/program.rs
@@ -20,6 +20,7 @@ use std::collections::HashSet;
 use std::ffi::CString;
 use std::mem::size_of;
 use std::ptr;
+use std::ptr::addr_of;
 use std::slice;
 use std::sync::Arc;
 use std::sync::Mutex;
@@ -55,7 +56,7 @@ fn get_disk_cache() -> &'static Option<DiskCache> {
         DISK_CACHE_ONCE.call_once(|| {
             DISK_CACHE = DiskCache::new("rusticl", &func_ptrs, 0);
         });
-        &DISK_CACHE
+        &*addr_of!(DISK_CACHE)
     }
 }
 
@@ -313,9 +314,11 @@ fn prepare_options(options: &str, dev: &Device) -> Vec<CString> {
     res.push(&options[old..]);
 
     res.iter()
-        .map(|&a| match a {
-            "-cl-denorms-are-zero" => "-fdenormal-fp-math=positive-zero",
-            _ => a,
+        .filter_map(|&a| match a {
+            "-cl-denorms-are-zero" => Some("-fdenormal-fp-math=positive-zero"),
+            // We can ignore it as long as we don't support ifp
+            "-cl-no-subgroup-ifp" => None,
+            _ => Some(a),
         })
         .map(CString::new)
         .map(Result::unwrap)
@@ -505,7 +508,12 @@ impl Program {
         for (i, d) in self.devs.iter().enumerate() {
             let mut ptr = ptrs[i];
             let info = lock.dev_build(d);
-            let spirv = info.spirv.as_ref().unwrap().to_bin();
+
+            // no spirv means nothing to write
+            let Some(spirv) = info.spirv.as_ref() else {
+                continue;
+            };
+            let spirv = spirv.to_bin();
 
             unsafe {
                 // 1. binary format version
diff --git a/src/gallium/frontends/rusticl/core/queue.rs b/src/gallium/frontends/rusticl/core/queue.rs
index 85cb6f39552..b1c37c1767a 100644
--- a/src/gallium/frontends/rusticl/core/queue.rs
+++ b/src/gallium/frontends/rusticl/core/queue.rs
@@ -6,8 +6,6 @@ use crate::core::platform::*;
 use crate::impl_cl_type_trait;
 
 use mesa_rust::pipe::context::PipeContext;
-use mesa_rust::pipe::resource::PipeResource;
-use mesa_rust::pipe::screen::ResourceType;
 use mesa_rust_util::properties::*;
 use rusticl_opencl_gen::*;
 
@@ -25,7 +23,7 @@ use std::thread::JoinHandle;
 /// Used for tracking bound GPU state to lower CPU overhead and centralize state tracking
 pub struct QueueContext {
     ctx: PipeContext,
-    cb0: Option<PipeResource>,
+    use_stream: bool,
 }
 
 impl QueueContext {
@@ -34,30 +32,18 @@ impl QueueContext {
             .screen()
             .create_context()
             .ok_or(CL_OUT_OF_HOST_MEMORY)?;
-        let size = device.param_max_size() as u32;
-        let cb0 = if device.prefers_real_buffer_in_cb0() {
-            device
-                .screen()
-                .resource_create_buffer(size, ResourceType::Cb0, 0)
-        } else {
-            None
-        };
-
-        if let Some(cb0) = &cb0 {
-            ctx.bind_constant_buffer(0, cb0);
-        }
 
-        Ok(Self { ctx: ctx, cb0: cb0 })
+        Ok(Self {
+            ctx: ctx,
+            use_stream: device.prefers_real_buffer_in_cb0(),
+        })
     }
 
     pub fn update_cb0(&self, data: &[u8]) {
         // only update if we actually bind data
         if !data.is_empty() {
-            // if we have a real buffer, update that, otherwise just set the data directly
-            if let Some(cb) = &self.cb0 {
-                debug_assert!(data.len() <= cb.width() as usize);
-                self.ctx
-                    .buffer_subdata(cb, 0, data.as_ptr().cast(), data.len() as u32);
+            if self.use_stream {
+                self.ctx.set_constant_buffer_stream(0, data);
             } else {
                 self.ctx.set_constant_buffer(0, data);
             }
diff --git a/src/gallium/frontends/rusticl/mesa/pipe/context.rs b/src/gallium/frontends/rusticl/mesa/pipe/context.rs
index 0d0faeda33a..b2ff2cfeb2f 100644
--- a/src/gallium/frontends/rusticl/mesa/pipe/context.rs
+++ b/src/gallium/frontends/rusticl/mesa/pipe/context.rs
@@ -8,6 +8,7 @@ use mesa_rust_gen::pipe_fd_type::*;
 use mesa_rust_gen::*;
 use mesa_rust_util::has_required_feature;
 
+use std::mem::size_of;
 use std::os::raw::*;
 use std::ptr;
 use std::ptr::*;
@@ -417,6 +418,37 @@ impl PipeContext {
         }
     }
 
+    pub fn set_constant_buffer_stream(&self, idx: u32, data: &[u8]) {
+        let mut cb = pipe_constant_buffer {
+            buffer: ptr::null_mut(),
+            buffer_offset: 0,
+            buffer_size: data.len() as u32,
+            user_buffer: ptr::null_mut(),
+        };
+
+        unsafe {
+            let stream = self.pipe.as_ref().stream_uploader;
+            u_upload_data(
+                stream,
+                0,
+                data.len() as u32,
+                size_of::<[u64; 16]>() as u32,
+                data.as_ptr().cast(),
+                &mut cb.buffer_offset,
+                &mut cb.buffer,
+            );
+            u_upload_unmap(stream);
+
+            self.pipe.as_ref().set_constant_buffer.unwrap()(
+                self.pipe.as_ptr(),
+                pipe_shader_type::PIPE_SHADER_COMPUTE,
+                idx,
+                false,
+                &cb,
+            );
+        }
+    }
+
     pub fn launch_grid(
         &self,
         work_dim: u32,
@@ -626,6 +658,7 @@ impl PipeContext {
 
 impl Drop for PipeContext {
     fn drop(&mut self) {
+        self.flush().wait();
         unsafe {
             self.pipe.as_ref().destroy.unwrap()(self.pipe.as_ptr());
         }
diff --git a/src/gallium/frontends/rusticl/rusticl_mesa_bindings.h b/src/gallium/frontends/rusticl/rusticl_mesa_bindings.h
index 291088e2843..5d0c23d7641 100644
--- a/src/gallium/frontends/rusticl/rusticl_mesa_bindings.h
+++ b/src/gallium/frontends/rusticl/rusticl_mesa_bindings.h
@@ -19,6 +19,7 @@
 #include "util/disk_cache.h"
 #include "util/os_time.h"
 #include "util/sha1/sha1.h"
+#include "util/u_upload_mgr.h"
 #include "util/u_printf.h"
 #include "util/u_sampler.h"
 #include "util/u_surface.h"
diff --git a/src/gallium/frontends/va/buffer.c b/src/gallium/frontends/va/buffer.c
index bbe0c75f23e..c11cead3802 100644
--- a/src/gallium/frontends/va/buffer.c
+++ b/src/gallium/frontends/va/buffer.c
@@ -316,7 +316,7 @@ vlVaDestroyBuffer(VADriverContextP ctx, VABufferID buf_id)
 
    if (buf->type == VAEncCodedBufferType) {
       VACodedBufferSegment* node = buf->data;
-      while(!node) {
+      while (node) {
          VACodedBufferSegment* next = (VACodedBufferSegment*) node->next;
          FREE(node);
          node = next;
diff --git a/src/gallium/frontends/va/picture.c b/src/gallium/frontends/va/picture.c
index 9172980bc77..1a3cf42ff83 100644
--- a/src/gallium/frontends/va/picture.c
+++ b/src/gallium/frontends/va/picture.c
@@ -299,7 +299,7 @@ handleIQMatrixBuffer(vlVaContext *context, vlVaBuffer *buf)
 }
 
 static void
-handleSliceParameterBuffer(vlVaContext *context, vlVaBuffer *buf, unsigned num_slices)
+handleSliceParameterBuffer(vlVaContext *context, vlVaBuffer *buf, unsigned num_slices, unsigned slice_offset)
 {
    switch (u_reduce_video_profile(context->templat.profile)) {
    case PIPE_VIDEO_FORMAT_MPEG12:
@@ -331,7 +331,7 @@ handleSliceParameterBuffer(vlVaContext *context, vlVaBuffer *buf, unsigned num_s
       break;
 
    case PIPE_VIDEO_FORMAT_AV1:
-      vlVaHandleSliceParameterBufferAV1(context, buf, num_slices);
+      vlVaHandleSliceParameterBufferAV1(context, buf, num_slices, slice_offset);
       break;
 
    default:
@@ -968,6 +968,7 @@ vlVaRenderPicture(VADriverContextP ctx, VAContextID context_id, VABufferID *buff
 
    unsigned i;
    unsigned slice_idx = 0;
+   unsigned slice_offset = 0;
    vlVaBuffer *seq_param_buf = NULL;
 
    if (!ctx)
@@ -1023,13 +1024,17 @@ vlVaRenderPicture(VADriverContextP ctx, VAContextID context_id, VABufferID *buff
 
             slice_idx is the zero based number of total slices received
                before this call to handleSliceParameterBuffer
+
+            slice_offset is the slice offset in bitstream buffer
          */
-         handleSliceParameterBuffer(context, buf, slice_idx);
+         handleSliceParameterBuffer(context, buf, slice_idx, slice_offset);
          slice_idx += buf->num_elements;
       } break;
 
       case VASliceDataBufferType:
          vaStatus = handleVASliceDataBufferType(context, buf);
+         if (slice_idx)
+            slice_offset += buf->size;
          break;
 
       case VAProcPipelineParameterBufferType:
diff --git a/src/gallium/frontends/va/picture_av1.c b/src/gallium/frontends/va/picture_av1.c
index e6f2652e362..c014b0b1168 100644
--- a/src/gallium/frontends/va/picture_av1.c
+++ b/src/gallium/frontends/va/picture_av1.c
@@ -396,7 +396,7 @@ void vlVaHandlePictureParameterBufferAV1(vlVaDriver *drv, vlVaContext *context,
   context->desc.av1.slice_parameter.slice_count = 0;
 }
 
-void vlVaHandleSliceParameterBufferAV1(vlVaContext *context, vlVaBuffer *buf, unsigned num_slices)
+void vlVaHandleSliceParameterBufferAV1(vlVaContext *context, vlVaBuffer *buf, unsigned num_slices, unsigned slice_offset)
 {
    for (uint32_t buffer_idx = 0; buffer_idx < buf->num_elements; buffer_idx++) {
       uint32_t slice_index =
@@ -407,7 +407,7 @@ void vlVaHandleSliceParameterBufferAV1(vlVaContext *context, vlVaBuffer *buf, un
 
       VASliceParameterBufferAV1 *av1 = &(((VASliceParameterBufferAV1*)buf->data)[buffer_idx]);
       context->desc.av1.slice_parameter.slice_data_size[slice_index] = av1->slice_data_size;
-      context->desc.av1.slice_parameter.slice_data_offset[slice_index] = av1->slice_data_offset;
+      context->desc.av1.slice_parameter.slice_data_offset[slice_index] = slice_offset + av1->slice_data_offset;
       context->desc.av1.slice_parameter.slice_data_row[slice_index] = av1->tile_row;
       context->desc.av1.slice_parameter.slice_data_col[slice_index] = av1->tile_column;
       context->desc.av1.slice_parameter.slice_data_anchor_frame_idx[slice_index] = av1->anchor_frame_idx;
diff --git a/src/gallium/frontends/va/picture_h264.c b/src/gallium/frontends/va/picture_h264.c
index 62d94b51db5..f5e99fd795c 100644
--- a/src/gallium/frontends/va/picture_h264.c
+++ b/src/gallium/frontends/va/picture_h264.c
@@ -186,6 +186,7 @@ void vlVaHandleSliceParameterBufferH264(vlVaContext *context, vlVaBuffer *buf)
    assert(context->desc.h264.slice_count < max_pipe_h264_slices);
 
    context->desc.h264.slice_parameter.slice_info_present = true;
+   context->desc.h264.slice_parameter.slice_type[context->desc.h264.slice_count] = h264->slice_type;
    context->desc.h264.slice_parameter.slice_data_size[context->desc.h264.slice_count] = h264->slice_data_size;
    context->desc.h264.slice_parameter.slice_data_offset[context->desc.h264.slice_count] = h264->slice_data_offset;
 
diff --git a/src/gallium/frontends/va/picture_vp9.c b/src/gallium/frontends/va/picture_vp9.c
index ff3da929f0b..f08b019fac1 100644
--- a/src/gallium/frontends/va/picture_vp9.c
+++ b/src/gallium/frontends/va/picture_vp9.c
@@ -56,8 +56,10 @@ void vlVaHandlePictureParameterBufferVP9(vlVaDriver *drv, vlVaContext *context,
    context->desc.vp9.picture_parameter.pic_fields.refresh_frame_context = vp9->pic_fields.bits.refresh_frame_context;
    context->desc.vp9.picture_parameter.pic_fields.frame_context_idx = vp9->pic_fields.bits.frame_context_idx;
    context->desc.vp9.picture_parameter.pic_fields.segmentation_enabled = vp9->pic_fields.bits.segmentation_enabled;
-   context->desc.vp9.picture_parameter.pic_fields.segmentation_temporal_update = vp9->pic_fields.bits.segmentation_temporal_update;
-   context->desc.vp9.picture_parameter.pic_fields.segmentation_update_map = vp9->pic_fields.bits.segmentation_update_map;
+   context->desc.vp9.picture_parameter.pic_fields.segmentation_temporal_update =
+      vp9->pic_fields.bits.segmentation_enabled && vp9->pic_fields.bits.segmentation_temporal_update;
+   context->desc.vp9.picture_parameter.pic_fields.segmentation_update_map =
+      vp9->pic_fields.bits.segmentation_enabled && vp9->pic_fields.bits.segmentation_update_map;
    context->desc.vp9.picture_parameter.pic_fields.last_ref_frame = vp9->pic_fields.bits.last_ref_frame;
    context->desc.vp9.picture_parameter.pic_fields.last_ref_frame_sign_bias = vp9->pic_fields.bits.last_ref_frame_sign_bias;
    context->desc.vp9.picture_parameter.pic_fields.golden_ref_frame = vp9->pic_fields.bits.golden_ref_frame;
diff --git a/src/gallium/frontends/va/surface.c b/src/gallium/frontends/va/surface.c
index 2f153abde64..e79f4f78846 100644
--- a/src/gallium/frontends/va/surface.c
+++ b/src/gallium/frontends/va/surface.c
@@ -713,6 +713,16 @@ vlVaQuerySurfaceAttributes(VADriverContextP ctx, VAConfigID config_id,
                                   config->profile, config->entrypoint,
                                   PIPE_VIDEO_CAP_MAX_HEIGHT);
       i++;
+#if VA_CHECK_VERSION(1, 21, 0)
+      attribs[i].type = VASurfaceAttribAlignmentSize;
+      attribs[i].value.type = VAGenericValueTypeInteger;
+      attribs[i].flags = VA_SURFACE_ATTRIB_GETTABLE;
+      attribs[i].value.value.i =
+         pscreen->get_video_param(pscreen,
+                                  config->profile, config->entrypoint,
+                                  PIPE_VIDEO_CAP_ENC_SURFACE_ALIGNMENT);
+      i++;
+#endif
    } else {
       attribs[i].type = VASurfaceAttribMaxWidth;
       attribs[i].value.type = VAGenericValueTypeInteger;
diff --git a/src/gallium/frontends/va/va_private.h b/src/gallium/frontends/va/va_private.h
index bd4c1e4a698..47a3fd8c666 100644
--- a/src/gallium/frontends/va/va_private.h
+++ b/src/gallium/frontends/va/va_private.h
@@ -537,7 +537,7 @@ void vlVaHandlePictureParameterBufferVP9(vlVaDriver *drv, vlVaContext *context,
 void vlVaHandleSliceParameterBufferVP9(vlVaContext *context, vlVaBuffer *buf);
 void vlVaDecoderVP9BitstreamHeader(vlVaContext *context, vlVaBuffer *buf);
 void vlVaHandlePictureParameterBufferAV1(vlVaDriver *drv, vlVaContext *context, vlVaBuffer *buf);
-void vlVaHandleSliceParameterBufferAV1(vlVaContext *context, vlVaBuffer *buf, unsigned num_slices);
+void vlVaHandleSliceParameterBufferAV1(vlVaContext *context, vlVaBuffer *buf, unsigned num_slices, unsigned slice_offset);
 void getEncParamPresetH264(vlVaContext *context);
 void getEncParamPresetH265(vlVaContext *context);
 void getEncParamPresetAV1(vlVaContext *context);
diff --git a/src/gallium/frontends/vdpau/query.c b/src/gallium/frontends/vdpau/query.c
index 158256b961b..6c48e9e8c12 100644
--- a/src/gallium/frontends/vdpau/query.c
+++ b/src/gallium/frontends/vdpau/query.c
@@ -108,6 +108,8 @@ vlVdpVideoSurfaceQueryGetPutBitsYCbCrCapabilities(VdpDevice device, VdpChromaTyp
 {
    vlVdpDevice *dev;
    struct pipe_screen *pscreen;
+   VdpYCbCrFormat ycbcrFormat;
+   bool supported;
 
    if (!is_supported)
       return VDP_STATUS_INVALID_POINTER;
@@ -122,47 +124,50 @@ vlVdpVideoSurfaceQueryGetPutBitsYCbCrCapabilities(VdpDevice device, VdpChromaTyp
 
    mtx_lock(&dev->mutex);
 
+   ycbcrFormat = bits_ycbcr_format;
    switch(bits_ycbcr_format) {
    case VDP_YCBCR_FORMAT_NV12:
-      *is_supported = surface_chroma_type == VDP_CHROMA_TYPE_420;
+      supported = surface_chroma_type == VDP_CHROMA_TYPE_420;
       break;
 
    case VDP_YCBCR_FORMAT_YV12:
-      *is_supported = surface_chroma_type == VDP_CHROMA_TYPE_420;
+      supported = surface_chroma_type == VDP_CHROMA_TYPE_420;
 
       /* We can convert YV12 to NV12 on the fly! */
-      if (*is_supported &&
-          pscreen->is_video_format_supported(pscreen,
-                                             PIPE_FORMAT_NV12,
-                                             PIPE_VIDEO_PROFILE_UNKNOWN,
-                                             PIPE_VIDEO_ENTRYPOINT_BITSTREAM)) {
-         mtx_unlock(&dev->mutex);
-         return VDP_STATUS_OK;
-      }
+      ycbcrFormat = VDP_YCBCR_FORMAT_NV12;
       break;
 
    case VDP_YCBCR_FORMAT_UYVY:
    case VDP_YCBCR_FORMAT_YUYV:
-      *is_supported = surface_chroma_type == VDP_CHROMA_TYPE_422;
+      supported = surface_chroma_type == VDP_CHROMA_TYPE_422;
       break;
 
    case VDP_YCBCR_FORMAT_Y8U8V8A8:
    case VDP_YCBCR_FORMAT_V8U8Y8A8:
-      *is_supported = surface_chroma_type == VDP_CHROMA_TYPE_444;
+      supported = surface_chroma_type == VDP_CHROMA_TYPE_444;
+      break;
+
+   case VDP_YCBCR_FORMAT_P010:
+   case VDP_YCBCR_FORMAT_P016:
+      /* Do any other profiles imply support for this chroma type? */
+      supported = (surface_chroma_type == VDP_CHROMA_TYPE_420_16)
+                  && vl_codec_supported(pscreen, PIPE_VIDEO_PROFILE_HEVC_MAIN_10, false);
       break;
 
    default:
-      *is_supported = false;
+      supported = false;
       break;
    }
 
-   if (*is_supported &&
+   if (supported &&
        !pscreen->is_video_format_supported(pscreen,
-                                           FormatYCBCRToPipe(bits_ycbcr_format),
+                                           FormatYCBCRToPipe(ycbcrFormat),
                                            PIPE_VIDEO_PROFILE_UNKNOWN,
                                            PIPE_VIDEO_ENTRYPOINT_BITSTREAM)) {
-      *is_supported = false;
+      supported = false;
    }
+   *is_supported = supported;
+
    mtx_unlock(&dev->mutex);
 
    return VDP_STATUS_OK;
diff --git a/src/gallium/frontends/wgl/stw_device.c b/src/gallium/frontends/wgl/stw_device.c
index 2f852aca0aa..5261028d73d 100644
--- a/src/gallium/frontends/wgl/stw_device.c
+++ b/src/gallium/frontends/wgl/stw_device.c
@@ -74,7 +74,7 @@ static int
 get_refresh_rate(void)
 {
 #ifndef _GAMING_XBOX
-   DEVMODE devModes;
+   DEVMODE devModes = { .dmSize = sizeof(DEVMODE) };
 
    if (EnumDisplaySettings(NULL, ENUM_CURRENT_SETTINGS, &devModes)) {
       /* clamp the value, just in case we get garbage */
@@ -258,7 +258,8 @@ stw_cleanup(void)
    st_screen_destroy(stw_dev->fscreen);
    FREE(stw_dev->fscreen);
 
-   stw_dev->screen->destroy(stw_dev->screen);
+   if (stw_dev->screen)
+      stw_dev->screen->destroy(stw_dev->screen);
 
    stw_tls_cleanup();
 
diff --git a/src/gallium/include/pipe/p_video_enums.h b/src/gallium/include/pipe/p_video_enums.h
index 8c03fe74755..08852351cda 100644
--- a/src/gallium/include/pipe/p_video_enums.h
+++ b/src/gallium/include/pipe/p_video_enums.h
@@ -161,6 +161,10 @@ enum pipe_video_cap
     * Encoding Region Of Interest feature
     */
    PIPE_VIDEO_CAP_ENC_ROI = 49,
+   /*
+    * Encoding surface width/height alignment
+    */
+   PIPE_VIDEO_CAP_ENC_SURFACE_ALIGNMENT = 50,
 };
 
 enum pipe_video_h264_enc_dbk_filter_mode_flags
diff --git a/src/gallium/include/pipe/p_video_state.h b/src/gallium/include/pipe/p_video_state.h
index 1afa57dadf4..671823b37ae 100644
--- a/src/gallium/include/pipe/p_video_state.h
+++ b/src/gallium/include/pipe/p_video_state.h
@@ -411,6 +411,7 @@ struct pipe_h264_picture_desc
    {
       bool slice_info_present;
       uint32_t slice_count;
+      uint8_t slice_type[128];
       uint32_t slice_data_size[128];
       uint32_t slice_data_offset[128];
       enum pipe_slice_buffer_placement_type slice_data_flag[128];
@@ -2008,6 +2009,21 @@ union pipe_enc_cap_roi {
    uint32_t value;
 };
 
+union pipe_enc_cap_surface_alignment {
+   struct {
+      /**
+       * log2_width_alignment
+       */
+      uint32_t log2_width_alignment                 : 4;
+      /**
+       * log2_height_alignment
+       */
+      uint32_t log2_height_alignment                : 4;
+      uint32_t reserved                             : 24;
+   } bits;
+   uint32_t value;
+};
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/src/gallium/targets/d3dadapter9/meson.build b/src/gallium/targets/d3dadapter9/meson.build
index 282859fd932..d5c75a7fb96 100644
--- a/src/gallium/targets/d3dadapter9/meson.build
+++ b/src/gallium/targets/d3dadapter9/meson.build
@@ -29,7 +29,7 @@ gallium_nine_link_depends = []
 gallium_nine_link_with = [
     libgallium, libnine_st,
     libpipe_loader_static, libws_null, libwsw, libswdri,
-    libswkmsdri,
+    libswkmsdri, libgalliumvl_stub,
 ]
 
 if with_ld_version_script
@@ -37,13 +37,6 @@ if with_ld_version_script
   gallium_nine_link_depends += files('d3dadapter9.sym')
 endif
 
-if (with_gallium_va or with_gallium_vdpau or with_gallium_omx != 'disabled' or
-    with_dri)
-  gallium_nine_link_with += libgalliumvl
-else
-  gallium_nine_link_with += libgalliumvl_stub
-endif
-
 libgallium_nine = shared_library(
   'd3dadapter9',
   files('description.c', 'getproc.c', 'drm.c'),
diff --git a/src/gallium/targets/pipe-loader/meson.build b/src/gallium/targets/pipe-loader/meson.build
index 943faec469d..48497fa9feb 100644
--- a/src/gallium/targets/pipe-loader/meson.build
+++ b/src/gallium/targets/pipe-loader/meson.build
@@ -20,19 +20,13 @@
 
 pipe_loader_link_args = [ld_args_gc_sections, ld_args_build_id]
 pipe_loader_link_deps = []
-pipe_loader_link_with = [libgallium]
+pipe_loader_link_with = [libgallium, libgalliumvl_stub]
 pipe_loader_comp_args = []
 pipe_loader_incs = [
   inc_include, inc_src, inc_util, inc_gallium, inc_gallium_drivers,
   inc_gallium_winsys, inc_gallium_aux,
 ]
 
-if (with_gallium_va or with_gallium_vdpau or with_gallium_omx != 'disabled' or
-    with_dri or with_gallium_radeonsi)
-  pipe_loader_link_with += libgalliumvl
-else
-  pipe_loader_link_with += libgalliumvl_stub
-endif
 if (with_gallium_va or with_gallium_vdpau or with_gallium_omx != 'disabled')
   pipe_loader_link_with += libgalliumvlwinsys
 endif
diff --git a/src/gallium/targets/rusticl/meson.build b/src/gallium/targets/rusticl/meson.build
index b2963fe6dfa..e1acaca7e68 100644
--- a/src/gallium/targets/rusticl/meson.build
+++ b/src/gallium/targets/rusticl/meson.build
@@ -42,6 +42,7 @@ librusticl = shared_library(
   ],
   link_whole : librusticl,
   link_with : [
+    libgalliumvl_stub,
     libpipe_loader_static,
     libswdri,
     libswkmsdri,
diff --git a/src/gallium/winsys/i915/drm/meson.build b/src/gallium/winsys/i915/drm/meson.build
index 57597972417..d68f74327ea 100644
--- a/src/gallium/winsys/i915/drm/meson.build
+++ b/src/gallium/winsys/i915/drm/meson.build
@@ -28,5 +28,5 @@ libi915drm = static_library(
     inc_include, inc_src, inc_gallium, inc_gallium_aux, inc_gallium_drivers
   ],
   link_with : [libintel_common],
-  dependencies : [dep_libdrm, dep_libdrm_intel],
+  dependencies : [dep_libdrm, dep_libdrm_intel, idep_intel_dev_wa],
 )
diff --git a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
index 2e1d9c488e2..7979cad75fa 100644
--- a/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
+++ b/src/gallium/winsys/radeon/drm/radeon_drm_bo.c
@@ -729,7 +729,7 @@ bool radeon_bo_can_reclaim_slab(void *priv, struct pb_slab_entry *entry)
 {
    struct radeon_bo *bo = container_of(entry, struct radeon_bo, u.slab.entry);
 
-   return radeon_bo_can_reclaim(NULL, &bo->base);
+   return radeon_bo_can_reclaim(priv, &bo->base);
 }
 
 static void radeon_bo_slab_destroy(void *winsys, struct pb_buffer_lean *_buf)
diff --git a/src/glx/drisw_glx.c b/src/glx/drisw_glx.c
index 3d3f75259bc..f9465c4a512 100644
--- a/src/glx/drisw_glx.c
+++ b/src/glx/drisw_glx.c
@@ -32,7 +32,9 @@
 #include <dlfcn.h>
 #include "dri_common.h"
 #include "drisw_priv.h"
+#ifdef HAVE_DRI3
 #include "dri3_priv.h"
+#endif
 #include <X11/extensions/shmproto.h>
 #include <assert.h>
 #include <vulkan/vulkan_core.h>
@@ -1001,7 +1003,9 @@ driswCreateScreenDriver(int screen, struct glx_display *priv,
       if (!psc->has_multibuffer &&
           !debug_get_bool_option("LIBGL_ALWAYS_SOFTWARE", false) &&
           !debug_get_bool_option("LIBGL_KOPPER_DRI2", false)) {
-         CriticalErrorMessageF("DRI3 not available\n");
+         /* only print error if zink was explicitly requested */
+         if (pdpyp->zink == TRY_ZINK_YES)
+            CriticalErrorMessageF("DRI3 not available\n");
          goto handle_error;
       }
    }
@@ -1049,7 +1053,8 @@ driswCreateScreenDriver(int screen, struct glx_display *priv,
    glx_screen_cleanup(&psc->base);
    free(psc);
 
-   CriticalErrorMessageF("failed to load driver: %s\n", driver);
+   if (pdpyp->zink == TRY_ZINK_YES)
+      CriticalErrorMessageF("failed to load driver: %s\n", driver);
 
    return NULL;
 }
@@ -1079,7 +1084,7 @@ driswDestroyDisplay(__GLXDRIdisplay * dpy)
  * display pointer.
  */
 _X_HIDDEN __GLXDRIdisplay *
-driswCreateDisplay(Display * dpy, bool zink)
+driswCreateDisplay(Display * dpy, enum try_zink zink)
 {
    struct drisw_display *pdpyp;
 
diff --git a/src/glx/drisw_priv.h b/src/glx/drisw_priv.h
index 53674f81a24..c7687ebcb63 100644
--- a/src/glx/drisw_priv.h
+++ b/src/glx/drisw_priv.h
@@ -33,7 +33,7 @@
 struct drisw_display
 {
    __GLXDRIdisplay base;
-   bool zink;
+   enum try_zink zink;
 };
 
 struct drisw_screen
diff --git a/src/glx/glxclient.h b/src/glx/glxclient.h
index 38980d76ddf..057d4a4b89f 100644
--- a/src/glx/glxclient.h
+++ b/src/glx/glxclient.h
@@ -131,11 +131,17 @@ struct __GLXDRIdrawableRec
    int refcount;
 };
 
+enum try_zink {
+   TRY_ZINK_NO,
+   TRY_ZINK_INFER,
+   TRY_ZINK_YES,
+};
+
 /*
 ** Function to create and DRI display data and initialize the display
 ** dependent methods.
 */
-extern __GLXDRIdisplay *driswCreateDisplay(Display * dpy, bool zink);
+extern __GLXDRIdisplay *driswCreateDisplay(Display * dpy, enum try_zink zink);
 extern __GLXDRIdisplay *dri2CreateDisplay(Display * dpy);
 extern __GLXDRIdisplay *dri3_create_display(Display * dpy);
 extern __GLXDRIdisplay *driwindowsCreateDisplay(Display * dpy);
diff --git a/src/glx/glxext.c b/src/glx/glxext.c
index 05c825a2b07..31e85113abf 100644
--- a/src/glx/glxext.c
+++ b/src/glx/glxext.c
@@ -908,13 +908,16 @@ __glXInitialize(Display * dpy)
 #endif /* HAVE_DRI3 */
       if (!debug_get_bool_option("LIBGL_DRI2_DISABLE", false))
          dpyPriv->dri2Display = dri2CreateDisplay(dpy);
+#if defined(HAVE_ZINK)
       if (!dpyPriv->dri3Display && !dpyPriv->dri2Display)
          try_zink = !debug_get_bool_option("LIBGL_KOPPER_DISABLE", false) &&
                     !getenv("GALLIUM_DRIVER");
+#endif /* HAVE_ZINK */
    }
 #endif /* GLX_USE_DRM */
    if (glx_direct)
-      dpyPriv->driswDisplay = driswCreateDisplay(dpy, zink | try_zink);
+      dpyPriv->driswDisplay = driswCreateDisplay(dpy, zink ? TRY_ZINK_YES :
+                                                             try_zink ? TRY_ZINK_INFER : TRY_ZINK_NO);
 
 #ifdef GLX_USE_WINDOWSGL
    if (glx_direct && glx_accel)
@@ -935,7 +938,7 @@ __glXInitialize(Display * dpy)
       if (try_zink) {
          free(dpyPriv->screens);
          dpyPriv->driswDisplay->destroyDisplay(dpyPriv->driswDisplay);
-         dpyPriv->driswDisplay = driswCreateDisplay(dpy, false);
+         dpyPriv->driswDisplay = driswCreateDisplay(dpy, TRY_ZINK_NO);
          fail = !AllocAndFetchScreenConfigs(dpy, dpyPriv, False);
       }
 #endif
diff --git a/src/intel/blorp/blorp.h b/src/intel/blorp/blorp.h
index a9aa72afef9..da6fb4bf76a 100644
--- a/src/intel/blorp/blorp.h
+++ b/src/intel/blorp/blorp.h
@@ -124,6 +124,17 @@ void blorp_batch_init(struct blorp_context *blorp, struct blorp_batch *batch,
                       void *driver_batch, enum blorp_batch_flags flags);
 void blorp_batch_finish(struct blorp_batch *batch);
 
+static inline isl_surf_usage_flags_t
+blorp_batch_isl_copy_usage(const struct blorp_batch *batch, bool is_dest)
+{
+   if (batch->flags & BLORP_BATCH_USE_COMPUTE)
+      return is_dest ? ISL_SURF_USAGE_STORAGE_BIT : ISL_SURF_USAGE_TEXTURE_BIT;
+   else if (batch->flags & BLORP_BATCH_USE_BLITTER)
+      return is_dest ? ISL_SURF_USAGE_BLITTER_DST_BIT : ISL_SURF_USAGE_BLITTER_SRC_BIT;
+   else
+      return is_dest ? ISL_SURF_USAGE_RENDER_TARGET_BIT : ISL_SURF_USAGE_TEXTURE_BIT;
+}
+
 struct blorp_address {
    void *buffer;
    int64_t offset;
diff --git a/src/intel/blorp/blorp_clear.c b/src/intel/blorp/blorp_clear.c
index 19d3949a4d9..c385baf5c02 100644
--- a/src/intel/blorp/blorp_clear.c
+++ b/src/intel/blorp/blorp_clear.c
@@ -605,15 +605,17 @@ blorp_clear(struct blorp_batch *batch,
    if (batch->blorp->isl_dev->info->ver < 6)
       use_simd16_replicated_data = false;
 
-   /* From the BSpec: 47719 Replicate Data:
+   /* From the BSpec: 47719 (TGL/DG2/MTL) Replicate Data:
     *
     * "Replicate Data Render Target Write message should not be used
     *  on all projects TGL+."
     *
+    * Xe2 spec (57350) does not mention this restriction.
+    *
     *  See 14017879046, 14017880152 for additional information.
     */
    if (batch->blorp->isl_dev->info->ver >= 12 &&
-       format == ISL_FORMAT_R10G10B10_FLOAT_A2_UNORM)
+       batch->blorp->isl_dev->info->ver < 20)
       use_simd16_replicated_data = false;
 
    if (compute)
diff --git a/src/intel/blorp/blorp_genX_exec.h b/src/intel/blorp/blorp_genX_exec.h
index 09c184abcd0..ed72739caf3 100644
--- a/src/intel/blorp/blorp_genX_exec.h
+++ b/src/intel/blorp/blorp_genX_exec.h
@@ -308,6 +308,8 @@ blorp_emit_vertex_data(struct blorp_batch *batch,
    };
 
    void *data = blorp_alloc_vertex_buffer(batch, sizeof(vertices), addr);
+   if (data == NULL)
+      return;
    memcpy(data, vertices, sizeof(vertices));
    *size = sizeof(vertices);
    blorp_flush_range(batch, data, *size);
@@ -329,6 +331,8 @@ blorp_emit_input_varying_data(struct blorp_batch *batch,
 
    const uint32_t *const inputs_src = (const uint32_t *)&params->wm_inputs;
    void *data = blorp_alloc_vertex_buffer(batch, *size, addr);
+   if (data == NULL)
+      return;
    uint32_t *inputs = data;
 
    /* Copy in the VS inputs */
@@ -424,8 +428,10 @@ blorp_emit_vertex_buffers(struct blorp_batch *batch,
    const uint32_t num_vbs = ARRAY_SIZE(vb);
 
    struct blorp_address addrs[2] = {};
-   uint32_t sizes[2];
+   uint32_t sizes[2] = {};
    blorp_emit_vertex_data(batch, params, &addrs[0], &sizes[0]);
+   if (sizes[0] == 0)
+      return;
    blorp_fill_vertex_buffer_state(vb, 0, addrs[0], sizes[0],
                                   3 * sizeof(float));
 
@@ -1147,6 +1153,8 @@ blorp_emit_blend_state(struct blorp_batch *batch,
    int size = GENX(BLEND_STATE_length) * 4;
    size += GENX(BLEND_STATE_ENTRY_length) * 4 * params->num_draw_buffers;
    uint32_t *state = blorp_alloc_dynamic_state(batch, size, 64, &offset);
+   if (state == NULL)
+      return 0;
    uint32_t *pos = state;
 
    GENX(BLEND_STATE_pack)(NULL, pos, &blend);
@@ -2103,6 +2111,11 @@ blorp_get_compute_push_const(struct blorp_batch *batch,
                                 &push_const_offset) :
       blorp_alloc_dynamic_state(batch, push_const_size, 64,
                                 &push_const_offset);
+   if (push_const == NULL) {
+      *state_offset = 0;
+      *state_size = 0;
+      return;
+   }
    memset(push_const, 0x0, push_const_size);
 
    void *dst = push_const;
@@ -2167,6 +2180,7 @@ blorp_exec_compute(struct blorp_batch *batch, const struct blorp_params *params)
    assert(cs_prog_data->push.per_thread.regs == 0);
    blorp_emit(batch, GENX(COMPUTE_WALKER), cw) {
       cw.SIMDSize                       = dispatch.simd_size / 16;
+      cw.MessageSIMD                    = dispatch.simd_size / 16,
       cw.LocalXMaximum                  = cs_prog_data->local_size[0] - 1;
       cw.LocalYMaximum                  = cs_prog_data->local_size[1] - 1;
       cw.LocalZMaximum                  = cs_prog_data->local_size[2] - 1;
@@ -2280,6 +2294,8 @@ blorp_exec_compute(struct blorp_batch *batch, const struct blorp_params *params)
    uint32_t idd_offset;
    uint32_t size = GENX(INTERFACE_DESCRIPTOR_DATA_length) * sizeof(uint32_t);
    void *state = blorp_alloc_dynamic_state(batch, size, 64, &idd_offset);
+   if (state == NULL)
+      return;
    GENX(INTERFACE_DESCRIPTOR_DATA_pack)(NULL, state, &idd);
 
    blorp_emit(batch, GENX(MEDIA_INTERFACE_DESCRIPTOR_LOAD), mid) {
@@ -2538,10 +2554,6 @@ blorp_xy_block_copy_blt(struct blorp_batch *batch,
          blt.SourceClearValueEnable = !!params->src.clear_color_addr.buffer;
          blt.SourceClearAddress = params->src.clear_color_addr;
       }
-
-      /* XeHP needs special MOCS values for the blitter */
-      blt.DestinationMOCS = isl_dev->mocs.blitter_dst;
-      blt.SourceMOCS = isl_dev->mocs.blitter_src;
 #endif
    }
 #endif
@@ -2624,8 +2636,7 @@ blorp_xy_fast_color_blit(struct blorp_batch *batch,
          blt.DestinationClearAddress = params->dst.clear_color_addr;
       }
 
-      /* XeHP needs special MOCS values for the blitter */
-      blt.DestinationMOCS = isl_dev->mocs.blitter_dst;
+      blt.DestinationMOCS = params->dst.addr.mocs;
 #endif
    }
 #endif
diff --git a/src/intel/ci/gitlab-ci-inc.yml b/src/intel/ci/gitlab-ci-inc.yml
index 16a9e19408a..af6a6cdcd38 100644
--- a/src/intel/ci/gitlab-ci-inc.yml
+++ b/src/intel/ci/gitlab-ci-inc.yml
@@ -206,14 +206,23 @@
     GPU_VERSION: iris-glk
     RUNNER_TAG: mesa-ci-x86-64-lava-hp-x360-12b-ca0010nr-n4020-octopus
 
-# Status: https://lava.collabora.dev/scheduler/device_type/asus-cx9400-volteer
-.lava-asus-cx9400-volteer:x86_64:
+# Status: https://lava.collabora.dev/scheduler/device_type/acer-cp514-2h-1130g7-volteer
+.lava-acer-cp514-2h-1130g7-volteer:x86_64:
   variables:
     BOOT_METHOD: depthcharge
-    DEVICE_TYPE: asus-cx9400-volteer
+    DEVICE_TYPE: acer-cp514-2h-1130g7-volteer
     FDO_CI_CONCURRENT: 9
     GPU_VERSION: anv-tgl
-    RUNNER_TAG: mesa-ci-x86-64-lava-asus-cx9400-volteer
+    RUNNER_TAG: mesa-ci-x86-64-lava-acer-cp514-2h-1130g7-volteer
+
+# Status: https://lava.collabora.dev/scheduler/device_type/acer-cp514-2h-1160g7-volteer
+.lava-acer-cp514-2h-1160g7-volteer:x86_64:
+  variables:
+    BOOT_METHOD: depthcharge
+    DEVICE_TYPE: acer-cp514-2h-1160g7-volteer
+    FDO_CI_CONCURRENT: 9
+    GPU_VERSION: anv-tgl
+    RUNNER_TAG: mesa-ci-x86-64-lava-acer-cp514-2h-1160g7-volteer
 
 # Status: https://lava.collabora.dev/scheduler/device_type/asus-C433TA-AJ0005-rammus
 .lava-asus-C433TA-AJ0005-rammus:x86_64:
@@ -278,11 +287,6 @@
     - .intel-rules
 
 ## ANV only
-.anv-tgl-test:
-  extends:
-    - .anv-test
-    - .lava-asus-cx9400-volteer:x86_64
-
 .anv-angle-test:
   extends:
     - .anv-test
@@ -387,7 +391,7 @@
 ## Intel (anv + iris)
 .intel-tgl-test:
   extends:
-    - .anv-tgl-test
+    - .anv-test
     - .intel-test
   variables:
     GPU_VERSION: intel-tgl
diff --git a/src/intel/ci/gitlab-ci.yml b/src/intel/ci/gitlab-ci.yml
index 57e40992ff6..59372415f3d 100644
--- a/src/intel/ci/gitlab-ci.yml
+++ b/src/intel/ci/gitlab-ci.yml
@@ -24,12 +24,13 @@ anv-jsl:
 
 anv-tgl:
   extends:
-    - .anv-tgl-test
+    - .lava-acer-cp514-2h-1160g7-volteer:x86_64
+    - .anv-test
   variables:
     DEQP_SUITE: anv-tgl
     DEQP_FRACTION: 5 # temporarily drop network load
     VK_DRIVER: intel
-  parallel: 6 # 15 - 3 zink - 3 post-merge perf + headroom
+  parallel: 5  # 5 - 5 # -1 for zink, but it's disabled now
 
 anv-tgl-full:
   extends:
@@ -80,7 +81,8 @@ anv-jsl-angle-full:
 
 anv-tgl-angle:
   extends:
-    - .anv-tgl-test
+    - .lava-acer-cp514-2h-1130g7-volteer:x86_64
+    - .anv-test
     - .anv-angle-test
   variables:
     DEQP_SUITE: anv-tgl-angle
@@ -242,6 +244,7 @@ iris-kbl-traces-performance:
 
 intel-tgl-skqp:
   extends:
+    - .lava-acer-cp514-2h-1130g7-volteer:x86_64
     - .intel-tgl-test
   variables:
     DEQP_SUITE: intel-tgl-skqp
diff --git a/src/intel/compiler/brw_compiler.h b/src/intel/compiler/brw_compiler.h
index 55deba7e2f8..b4a3d927ef1 100644
--- a/src/intel/compiler/brw_compiler.h
+++ b/src/intel/compiler/brw_compiler.h
@@ -1290,8 +1290,34 @@ wm_prog_data_barycentric_modes(const struct brw_wm_prog_data *prog_data,
        *    "MSDISPMODE_PERSAMPLE is required in order to select Perspective
        *     Sample or Non-perspective Sample barycentric coordinates."
        */
-      modes &= ~(BITFIELD_BIT(BRW_BARYCENTRIC_PERSPECTIVE_SAMPLE) |
-                 BITFIELD_BIT(BRW_BARYCENTRIC_NONPERSPECTIVE_SAMPLE));
+      uint32_t sample_bits = (BITFIELD_BIT(BRW_BARYCENTRIC_PERSPECTIVE_SAMPLE) |
+                              BITFIELD_BIT(BRW_BARYCENTRIC_NONPERSPECTIVE_SAMPLE));
+      uint32_t requested_sample = modes & sample_bits;
+      modes &= ~sample_bits;
+      /*
+       * If the shader requested some sample modes and we have to disable
+       * them, make sure we add back the pixel variant back to not mess up the
+       * thread payload.
+       *
+       * Why does this works out? Because of the ordering in the thread payload :
+       *
+       *   R7:10  Perspective Centroid Barycentric
+       *   R11:14 Perspective Sample Barycentric
+       *   R15:18 Linear Pixel Location Barycentric
+       *
+       * In the backend when persample dispatch is dynamic, we always select
+       * the sample barycentric and turn off the pixel location (even if
+       * requested through intrinsics). That way when we dynamically select
+       * pixel or sample dispatch, the barycentric always match, since the
+       * pixel location barycentric register offset will align with the sample
+       * barycentric.
+       */
+      if (requested_sample) {
+         if (requested_sample & BITFIELD_BIT(BRW_BARYCENTRIC_PERSPECTIVE_SAMPLE))
+            modes |= BITFIELD_BIT(BRW_BARYCENTRIC_PERSPECTIVE_PIXEL);
+         if (requested_sample & BITFIELD_BIT(BRW_BARYCENTRIC_NONPERSPECTIVE_SAMPLE))
+            modes |= BITFIELD_BIT(BRW_BARYCENTRIC_NONPERSPECTIVE_PIXEL);
+      }
    }
 
    return modes;
diff --git a/src/intel/compiler/brw_disasm.c b/src/intel/compiler/brw_disasm.c
index 3ef20f24675..c06f35d0827 100644
--- a/src/intel/compiler/brw_disasm.c
+++ b/src/intel/compiler/brw_disasm.c
@@ -1560,7 +1560,10 @@ src0_dpas_3src(FILE *file, const struct intel_device_info *devinfo,
 
    if (subreg_nr)
       format(file, ".%d", subreg_nr);
-   src_align1_region(file, 1, 1, 0);
+   src_align1_region(file,
+                     BRW_VERTICAL_STRIDE_1,
+                     BRW_WIDTH_1,
+                     BRW_ALIGN1_3SRC_SRC_HORIZONTAL_STRIDE_0);
 
    string(file, brw_reg_type_to_letters(type));
 
@@ -1581,7 +1584,10 @@ src1_dpas_3src(FILE *file, const struct intel_device_info *devinfo,
 
    if (subreg_nr)
       format(file, ".%d", subreg_nr);
-   src_align1_region(file, 1, 1, 0);
+   src_align1_region(file,
+                     BRW_VERTICAL_STRIDE_1,
+                     BRW_WIDTH_1,
+                     BRW_ALIGN1_3SRC_SRC_HORIZONTAL_STRIDE_0);
 
    string(file, brw_reg_type_to_letters(type));
 
@@ -1602,7 +1608,10 @@ src2_dpas_3src(FILE *file, const struct intel_device_info *devinfo,
 
    if (subreg_nr)
       format(file, ".%d", subreg_nr);
-   src_align1_region(file, 1, 1, 0);
+   src_align1_region(file,
+                     BRW_VERTICAL_STRIDE_1,
+                     BRW_WIDTH_1,
+                     BRW_ALIGN1_3SRC_SRC_HORIZONTAL_STRIDE_0);
 
    string(file, brw_reg_type_to_letters(type));
 
diff --git a/src/intel/compiler/brw_eu_defines.h b/src/intel/compiler/brw_eu_defines.h
index 820ac0f4ef5..1a1ca11202d 100644
--- a/src/intel/compiler/brw_eu_defines.h
+++ b/src/intel/compiler/brw_eu_defines.h
@@ -779,6 +779,7 @@ enum opcode {
    SHADER_OPCODE_BTD_SPAWN_LOGICAL,
    SHADER_OPCODE_BTD_RETIRE_LOGICAL,
 
+   SHADER_OPCODE_READ_MASK_REG,
    SHADER_OPCODE_READ_SR_REG,
 
    RT_OPCODE_TRACE_RAY_LOGICAL,
diff --git a/src/intel/compiler/brw_eu_emit.c b/src/intel/compiler/brw_eu_emit.c
index 9d312a6400f..409066da630 100644
--- a/src/intel/compiler/brw_eu_emit.c
+++ b/src/intel/compiler/brw_eu_emit.c
@@ -121,7 +121,7 @@ brw_set_dest(struct brw_codegen *p, brw_inst *inst, struct brw_reg dest)
               dest.vstride == dest.width + 1));
       assert(!dest.negate && !dest.abs);
       brw_inst_set_dst_reg_file(devinfo, inst, dest.file);
-      brw_inst_set_dst_da_reg_nr(devinfo, inst, dest.nr);
+      brw_inst_set_dst_da_reg_nr(devinfo, inst, phys_nr(devinfo, dest));
 
    } else if (brw_inst_opcode(p->isa, inst) == BRW_OPCODE_SENDS ||
               brw_inst_opcode(p->isa, inst) == BRW_OPCODE_SENDSC) {
@@ -141,10 +141,10 @@ brw_set_dest(struct brw_codegen *p, brw_inst *inst, struct brw_reg dest)
       brw_inst_set_dst_address_mode(devinfo, inst, dest.address_mode);
 
       if (dest.address_mode == BRW_ADDRESS_DIRECT) {
-         brw_inst_set_dst_da_reg_nr(devinfo, inst, dest.nr);
+         brw_inst_set_dst_da_reg_nr(devinfo, inst, phys_nr(devinfo, dest));
 
          if (brw_inst_access_mode(devinfo, inst) == BRW_ALIGN_1) {
-            brw_inst_set_dst_da1_subreg_nr(devinfo, inst, dest.subnr);
+            brw_inst_set_dst_da1_subreg_nr(devinfo, inst, phys_subnr(devinfo, dest));
             if (dest.hstride == BRW_HORIZONTAL_STRIDE_0)
                dest.hstride = BRW_HORIZONTAL_STRIDE_1;
             brw_inst_set_dst_hstride(devinfo, inst, dest.hstride);
@@ -162,7 +162,7 @@ brw_set_dest(struct brw_codegen *p, brw_inst *inst, struct brw_reg dest)
             brw_inst_set_dst_hstride(devinfo, inst, 1);
          }
       } else {
-         brw_inst_set_dst_ia_subreg_nr(devinfo, inst, dest.subnr);
+         brw_inst_set_dst_ia_subreg_nr(devinfo, inst, phys_subnr(devinfo, dest));
 
          /* These are different sizes in align1 vs align16:
           */
@@ -242,7 +242,7 @@ brw_set_src0(struct brw_codegen *p, brw_inst *inst, struct brw_reg reg)
               reg.vstride == reg.width + 1));
       assert(!reg.negate && !reg.abs);
       brw_inst_set_send_src0_reg_file(devinfo, inst, reg.file);
-      brw_inst_set_src0_da_reg_nr(devinfo, inst, reg.nr);
+      brw_inst_set_src0_da_reg_nr(devinfo, inst, phys_nr(devinfo, reg));
 
    } else if (brw_inst_opcode(p->isa, inst) == BRW_OPCODE_SENDS ||
               brw_inst_opcode(p->isa, inst) == BRW_OPCODE_SENDSC) {
@@ -279,14 +279,14 @@ brw_set_src0(struct brw_codegen *p, brw_inst *inst, struct brw_reg reg)
          }
       } else {
          if (reg.address_mode == BRW_ADDRESS_DIRECT) {
-            brw_inst_set_src0_da_reg_nr(devinfo, inst, reg.nr);
+            brw_inst_set_src0_da_reg_nr(devinfo, inst, phys_nr(devinfo, reg));
             if (brw_inst_access_mode(devinfo, inst) == BRW_ALIGN_1) {
-                brw_inst_set_src0_da1_subreg_nr(devinfo, inst, reg.subnr);
+               brw_inst_set_src0_da1_subreg_nr(devinfo, inst, phys_subnr(devinfo, reg));
             } else {
                brw_inst_set_src0_da16_subreg_nr(devinfo, inst, reg.subnr / 16);
             }
          } else {
-            brw_inst_set_src0_ia_subreg_nr(devinfo, inst, reg.subnr);
+            brw_inst_set_src0_ia_subreg_nr(devinfo, inst, phys_subnr(devinfo, reg));
 
             if (brw_inst_access_mode(devinfo, inst) == BRW_ALIGN_1) {
                brw_inst_set_src0_ia1_addr_imm(devinfo, inst, reg.indirect_offset);
@@ -362,7 +362,7 @@ brw_set_src1(struct brw_codegen *p, brw_inst *inst, struct brw_reg reg)
              (reg.hstride == BRW_HORIZONTAL_STRIDE_1 &&
               reg.vstride == reg.width + 1));
       assert(!reg.negate && !reg.abs);
-      brw_inst_set_send_src1_reg_nr(devinfo, inst, reg.nr);
+      brw_inst_set_send_src1_reg_nr(devinfo, inst, phys_nr(devinfo, reg));
       brw_inst_set_send_src1_reg_file(devinfo, inst, reg.file);
    } else {
       /* From the IVB PRM Vol. 4, Pt. 3, Section 3.3.3.5:
@@ -395,9 +395,9 @@ brw_set_src1(struct brw_codegen *p, brw_inst *inst, struct brw_reg reg)
          assert (reg.address_mode == BRW_ADDRESS_DIRECT);
          /* assert (reg.file == BRW_GENERAL_REGISTER_FILE); */
 
-         brw_inst_set_src1_da_reg_nr(devinfo, inst, reg.nr);
+         brw_inst_set_src1_da_reg_nr(devinfo, inst, phys_nr(devinfo, reg));
          if (brw_inst_access_mode(devinfo, inst) == BRW_ALIGN_1) {
-            brw_inst_set_src1_da1_subreg_nr(devinfo, inst, reg.subnr);
+            brw_inst_set_src1_da1_subreg_nr(devinfo, inst, phys_subnr(devinfo, reg));
          } else {
             brw_inst_set_src1_da16_subreg_nr(devinfo, inst, reg.subnr / 16);
          }
@@ -832,7 +832,7 @@ brw_alu3(struct brw_codegen *p, unsigned opcode, struct brw_reg dest,
 
       if (devinfo->ver >= 12) {
          brw_inst_set_3src_a1_dst_reg_file(devinfo, inst, dest.file);
-         brw_inst_set_3src_dst_reg_nr(devinfo, inst, dest.nr);
+         brw_inst_set_3src_dst_reg_nr(devinfo, inst, phys_nr(devinfo, dest));
       } else {
          if (dest.file == BRW_ARCHITECTURE_REGISTER_FILE) {
             brw_inst_set_3src_a1_dst_reg_file(devinfo, inst,
@@ -844,7 +844,7 @@ brw_alu3(struct brw_codegen *p, unsigned opcode, struct brw_reg dest,
             brw_inst_set_3src_dst_reg_nr(devinfo, inst, dest.nr);
          }
       }
-      brw_inst_set_3src_a1_dst_subreg_nr(devinfo, inst, dest.subnr / 8);
+      brw_inst_set_3src_a1_dst_subreg_nr(devinfo, inst, phys_subnr(devinfo, dest) / 8);
 
       brw_inst_set_3src_a1_dst_hstride(devinfo, inst, BRW_ALIGN1_3SRC_DST_HORIZONTAL_STRIDE_1);
 
@@ -868,11 +868,11 @@ brw_alu3(struct brw_codegen *p, unsigned opcode, struct brw_reg dest,
             devinfo, inst, to_3src_align1_vstride(devinfo, src0.vstride));
          brw_inst_set_3src_a1_src0_hstride(devinfo, inst,
                                            to_3src_align1_hstride(src0.hstride));
-         brw_inst_set_3src_a1_src0_subreg_nr(devinfo, inst, src0.subnr);
+         brw_inst_set_3src_a1_src0_subreg_nr(devinfo, inst, phys_subnr(devinfo, src0));
          if (src0.type == BRW_REGISTER_TYPE_NF) {
             brw_inst_set_3src_src0_reg_nr(devinfo, inst, BRW_ARF_ACCUMULATOR);
          } else {
-            brw_inst_set_3src_src0_reg_nr(devinfo, inst, src0.nr);
+            brw_inst_set_3src_src0_reg_nr(devinfo, inst, phys_nr(devinfo, src0));
          }
          brw_inst_set_3src_src0_abs(devinfo, inst, src0.abs);
          brw_inst_set_3src_src0_negate(devinfo, inst, src0.negate);
@@ -882,11 +882,11 @@ brw_alu3(struct brw_codegen *p, unsigned opcode, struct brw_reg dest,
       brw_inst_set_3src_a1_src1_hstride(devinfo, inst,
                                         to_3src_align1_hstride(src1.hstride));
 
-      brw_inst_set_3src_a1_src1_subreg_nr(devinfo, inst, src1.subnr);
+      brw_inst_set_3src_a1_src1_subreg_nr(devinfo, inst, phys_subnr(devinfo, src1));
       if (src1.file == BRW_ARCHITECTURE_REGISTER_FILE) {
          brw_inst_set_3src_src1_reg_nr(devinfo, inst, BRW_ARF_ACCUMULATOR);
       } else {
-         brw_inst_set_3src_src1_reg_nr(devinfo, inst, src1.nr);
+         brw_inst_set_3src_src1_reg_nr(devinfo, inst, phys_nr(devinfo, src1));
       }
       brw_inst_set_3src_src1_abs(devinfo, inst, src1.abs);
       brw_inst_set_3src_src1_negate(devinfo, inst, src1.negate);
@@ -897,8 +897,8 @@ brw_alu3(struct brw_codegen *p, unsigned opcode, struct brw_reg dest,
          brw_inst_set_3src_a1_src2_hstride(devinfo, inst,
                                            to_3src_align1_hstride(src2.hstride));
          /* no vstride on src2 */
-         brw_inst_set_3src_a1_src2_subreg_nr(devinfo, inst, src2.subnr);
-         brw_inst_set_3src_src2_reg_nr(devinfo, inst, src2.nr);
+         brw_inst_set_3src_a1_src2_subreg_nr(devinfo, inst, phys_subnr(devinfo, src2));
+         brw_inst_set_3src_src2_reg_nr(devinfo, inst, phys_nr(devinfo, src2));
          brw_inst_set_3src_src2_abs(devinfo, inst, src2.abs);
          brw_inst_set_3src_src2_negate(devinfo, inst, src2.negate);
       }
@@ -2923,7 +2923,7 @@ brw_send_indirect_split_message(struct brw_codegen *p,
       assert(ex_desc.nr == BRW_ARF_ADDRESS);
       assert((ex_desc.subnr & 0x3) == 0);
       brw_inst_set_send_sel_reg32_ex_desc(devinfo, send, 1);
-      brw_inst_set_send_ex_desc_ia_subreg_nr(devinfo, send, ex_desc.subnr >> 2);
+      brw_inst_set_send_ex_desc_ia_subreg_nr(devinfo, send, phys_subnr(devinfo, ex_desc) >> 2);
    }
 
    if (ex_bso) {
diff --git a/src/intel/compiler/brw_fs.cpp b/src/intel/compiler/brw_fs.cpp
index cdf4db7a67f..75339fb07fd 100644
--- a/src/intel/compiler/brw_fs.cpp
+++ b/src/intel/compiler/brw_fs.cpp
@@ -1195,7 +1195,8 @@ fs_visitor::import_uniforms(fs_visitor *v)
 }
 
 enum brw_barycentric_mode
-brw_barycentric_mode(nir_intrinsic_instr *intr)
+brw_barycentric_mode(const struct brw_wm_prog_key *key,
+                     nir_intrinsic_instr *intr)
 {
    const glsl_interp_mode mode =
       (enum glsl_interp_mode) nir_intrinsic_interp_mode(intr);
@@ -1207,7 +1208,13 @@ brw_barycentric_mode(nir_intrinsic_instr *intr)
    switch (intr->intrinsic) {
    case nir_intrinsic_load_barycentric_pixel:
    case nir_intrinsic_load_barycentric_at_offset:
-      bary = BRW_BARYCENTRIC_PERSPECTIVE_PIXEL;
+      /* When per sample interpolation is dynamic, assume sample
+       * interpolation. We'll dynamically remap things so that the FS thread
+       * payload is not affected.
+       */
+      bary = key->persample_interp == BRW_SOMETIMES ?
+             BRW_BARYCENTRIC_PERSPECTIVE_SAMPLE :
+             BRW_BARYCENTRIC_PERSPECTIVE_PIXEL;
       break;
    case nir_intrinsic_load_barycentric_centroid:
       bary = BRW_BARYCENTRIC_PERSPECTIVE_CENTROID;
@@ -3049,11 +3056,10 @@ fs_visitor::opt_split_sends()
 
    foreach_block_and_inst(block, fs_inst, send, cfg) {
       if (send->opcode != SHADER_OPCODE_SEND ||
-          send->mlen <= reg_unit(devinfo) || send->ex_mlen > 0)
+          send->mlen <= reg_unit(devinfo) || send->ex_mlen > 0 ||
+          send->src[2].file != VGRF)
          continue;
 
-      assert(send->src[2].file == VGRF);
-
       /* Currently don't split sends that reuse a previously used payload. */
       fs_inst *lp = (fs_inst *) send->prev;
 
@@ -4473,7 +4479,8 @@ fs_visitor::lower_sub_sat()
           */
          if (inst->exec_size == 8 && inst->src[0].type != BRW_REGISTER_TYPE_Q &&
              inst->src[0].type != BRW_REGISTER_TYPE_UQ) {
-            fs_reg acc(ARF, BRW_ARF_ACCUMULATOR, inst->src[1].type);
+            fs_reg acc = retype(brw_acc_reg(inst->exec_size),
+                                inst->src[1].type);
 
             ibld.MOV(acc, inst->src[1]);
             fs_inst *add = ibld.ADD(inst->dst, acc, inst->src[0]);
@@ -5473,7 +5480,8 @@ fs_visitor::lower_simd_width()
           */
          const unsigned max_width = MAX2(inst->exec_size, lower_width);
 
-         const fs_builder bld = fs_builder(this).at_end();
+         const fs_builder bld =
+            fs_builder(this, MAX2(max_width, this->dispatch_width)).at_end();
          const fs_builder ibld = bld.at(block, inst)
                                     .exec_all(inst->force_writemask_all)
                                     .group(max_width, inst->group / max_width);
@@ -5744,7 +5752,6 @@ fs_visitor::lower_find_live_channel()
        * instruction has execution masking disabled, so it's kind of
        * useless there.
        */
-      fs_reg exec_mask(retype(brw_mask_reg(0), BRW_REGISTER_TYPE_UD));
 
       const fs_builder ibld(this, block, inst);
       if (!inst->is_partial_write())
@@ -5752,6 +5759,10 @@ fs_visitor::lower_find_live_channel()
 
       const fs_builder ubld = fs_builder(this, block, inst).exec_all().group(1, 0);
 
+      fs_reg exec_mask = ubld.vgrf(BRW_REGISTER_TYPE_UD);
+      ubld.UNDEF(exec_mask);
+      ubld.emit(SHADER_OPCODE_READ_MASK_REG, exec_mask, brw_imm_ud(0));
+
       /* ce0 doesn't consider the thread dispatch mask (DMask or VMask),
        * so combine the execution and dispatch masks to obtain the true mask.
        *
@@ -7100,6 +7111,9 @@ fs_visitor::run_fs(bool allow_spilling, bool do_rep_send)
    payload_ = new fs_thread_payload(*this, source_depth_to_render_target,
                                     runtime_check_aads_emit);
 
+   if (nir->info.ray_queries > 0)
+      limit_dispatch_width(16, "SIMD32 not supported with ray queries.\n");
+
    if (do_rep_send) {
       assert(dispatch_width == 16);
       emit_repclear_shader();
@@ -7336,6 +7350,7 @@ is_used_in_not_interp_frag_coord(nir_def *def)
  */
 static unsigned
 brw_compute_barycentric_interp_modes(const struct intel_device_info *devinfo,
+                                     const struct brw_wm_prog_key *key,
                                      const nir_shader *shader)
 {
    unsigned barycentric_interp_modes = 0;
@@ -7364,7 +7379,7 @@ brw_compute_barycentric_interp_modes(const struct intel_device_info *devinfo,
 
             nir_intrinsic_op bary_op = intrin->intrinsic;
             enum brw_barycentric_mode bary =
-               brw_barycentric_mode(intrin);
+               brw_barycentric_mode(key, intrin);
 
             barycentric_interp_modes |= 1 << bary;
 
@@ -7573,7 +7588,7 @@ brw_nir_populate_wm_prog_data(nir_shader *shader,
    prog_data->inner_coverage = shader->info.fs.inner_coverage;
 
    prog_data->barycentric_interp_modes =
-      brw_compute_barycentric_interp_modes(devinfo, shader);
+      brw_compute_barycentric_interp_modes(devinfo, key, shader);
 
    /* From the BDW PRM documentation for 3DSTATE_WM:
     *
@@ -7767,9 +7782,6 @@ brw_compile_fs(const struct brw_compiler *compiler,
                                " pixel shading.\n");
    }
 
-   if (nir->info.ray_queries > 0 && v8)
-      v8->limit_dispatch_width(16, "SIMD32 with ray queries.\n");
-
    if (!has_spilled &&
        (!v8 || v8->max_dispatch_width >= 16) &&
        (INTEL_SIMD(FS, 16) || params->use_rep_send)) {
diff --git a/src/intel/compiler/brw_fs.h b/src/intel/compiler/brw_fs.h
index a57274250b9..8ca8b18e5a5 100644
--- a/src/intel/compiler/brw_fs.h
+++ b/src/intel/compiler/brw_fs.h
@@ -611,7 +611,8 @@ fs_reg setup_imm_b(const brw::fs_builder &bld,
 fs_reg setup_imm_ub(const brw::fs_builder &bld,
                    uint8_t v);
 
-enum brw_barycentric_mode brw_barycentric_mode(nir_intrinsic_instr *intr);
+enum brw_barycentric_mode brw_barycentric_mode(const struct brw_wm_prog_key *key,
+                                               nir_intrinsic_instr *intr);
 
 uint32_t brw_fb_write_msg_control(const fs_inst *inst,
                                   const struct brw_wm_prog_data *prog_data);
diff --git a/src/intel/compiler/brw_fs_cse.cpp b/src/intel/compiler/brw_fs_cse.cpp
index 8fa1d281b06..a84564b3f6c 100644
--- a/src/intel/compiler/brw_fs_cse.cpp
+++ b/src/intel/compiler/brw_fs_cse.cpp
@@ -89,6 +89,7 @@ is_expression(const fs_visitor *v, const fs_inst *const inst)
    case FS_OPCODE_TXB_LOGICAL:
    case SHADER_OPCODE_TXF_CMS_LOGICAL:
    case SHADER_OPCODE_TXF_CMS_W_LOGICAL:
+   case SHADER_OPCODE_TXF_CMS_W_GFX12_LOGICAL:
    case SHADER_OPCODE_TXF_UMS_LOGICAL:
    case SHADER_OPCODE_TXF_MCS_LOGICAL:
    case SHADER_OPCODE_LOD_LOGICAL:
diff --git a/src/intel/compiler/brw_fs_generator.cpp b/src/intel/compiler/brw_fs_generator.cpp
index c6a315a4486..9f193728fbf 100644
--- a/src/intel/compiler/brw_fs_generator.cpp
+++ b/src/intel/compiler/brw_fs_generator.cpp
@@ -485,7 +485,7 @@ fs_generator::generate_mov_indirect(fs_inst *inst,
 
       reg.nr = imm_byte_offset / REG_SIZE;
       reg.subnr = imm_byte_offset % REG_SIZE;
-      if (type_sz(reg.type) > 4 && !devinfo->has_64bit_float) {
+      if (type_sz(reg.type) > 4 && !devinfo->has_64bit_int) {
          brw_MOV(p, subscript(dst, BRW_REGISTER_TYPE_D, 0),
                     subscript(reg, BRW_REGISTER_TYPE_D, 0));
          brw_set_default_swsb(p, tgl_swsb_null());
@@ -567,7 +567,7 @@ fs_generator::generate_mov_indirect(fs_inst *inst,
       if (type_sz(reg.type) > 4 &&
           ((devinfo->verx10 == 70) ||
            devinfo->platform == INTEL_PLATFORM_CHV || intel_device_info_is_9lp(devinfo) ||
-           !devinfo->has_64bit_float || devinfo->verx10 >= 125)) {
+           !devinfo->has_64bit_int)) {
          /* IVB has an issue (which we found empirically) where it reads two
           * address register components per channel for indirectly addressed
           * 64-bit sources.
@@ -2324,6 +2324,26 @@ fs_generator::generate_code(const cfg_t *cfg, int dispatch_width,
          brw_float_controls_mode(p, src[0].d, src[1].d);
          break;
 
+      case SHADER_OPCODE_READ_MASK_REG:
+         if (devinfo->ver >= 12) {
+            /* There is a SWSB restriction that requires that any time sr0 is
+             * accessed both the instruction doing the access and the next one
+             * have SWSB set to RegDist(1).
+             */
+            if (brw_get_default_swsb(p).mode != TGL_SBID_NULL)
+               brw_SYNC(p, TGL_SYNC_NOP);
+            assert(src[0].file == BRW_IMMEDIATE_VALUE);
+            brw_set_default_swsb(p, tgl_swsb_regdist(1));
+            brw_MOV(p, dst, retype(brw_mask_reg(src[0].ud),
+                                   BRW_REGISTER_TYPE_UD));
+            brw_set_default_swsb(p, tgl_swsb_regdist(1));
+            brw_AND(p, dst, dst, brw_imm_ud(0xffffffff));
+         } else {
+            brw_MOV(p, dst, retype(brw_mask_reg(src[0].ud),
+                                   BRW_REGISTER_TYPE_UD));
+         }
+         break;
+
       case SHADER_OPCODE_READ_SR_REG:
          if (devinfo->ver >= 12) {
             /* There is a SWSB restriction that requires that any time sr0 is
diff --git a/src/intel/compiler/brw_fs_lower_regioning.cpp b/src/intel/compiler/brw_fs_lower_regioning.cpp
index 3bff7770cd0..873e255f0a3 100644
--- a/src/intel/compiler/brw_fs_lower_regioning.cpp
+++ b/src/intel/compiler/brw_fs_lower_regioning.cpp
@@ -190,18 +190,6 @@ namespace {
          else
             return brw_int_type(type_sz(t), false);
 
-      case SHADER_OPCODE_BROADCAST:
-      case SHADER_OPCODE_MOV_INDIRECT:
-         if (((devinfo->verx10 == 70 ||
-               devinfo->platform == INTEL_PLATFORM_CHV ||
-               intel_device_info_is_9lp(devinfo) ||
-               devinfo->verx10 >= 125) && type_sz(inst->src[0].type) > 4) ||
-             (devinfo->verx10 >= 125 &&
-              brw_reg_type_is_floating_point(inst->src[0].type)))
-            return brw_int_type(type_sz(t), false);
-         else
-            return t;
-
       default:
          return t;
       }
@@ -572,6 +560,12 @@ namespace {
          ibld.at(block, inst->next).MOV(subscript(inst->dst, raw_type, j),
                                         subscript(tmp, raw_type, j));
 
+      /* If the destination was an accumulator, after lowering it will be a
+       * GRF. Clear writes_accumulator for the instruction.
+       */
+      if (inst->dst.is_accumulator())
+         inst->writes_accumulator = false;
+
       /* Point the original instruction at the temporary, making sure to keep
        * any destination modifiers in the instruction.
        */
diff --git a/src/intel/compiler/brw_fs_nir.cpp b/src/intel/compiler/brw_fs_nir.cpp
index ccdd0fe7db8..0e5f6bba4b8 100644
--- a/src/intel/compiler/brw_fs_nir.cpp
+++ b/src/intel/compiler/brw_fs_nir.cpp
@@ -500,6 +500,9 @@ optimize_extract_to_float(nir_to_brw_state &ntb, nir_alu_instr *instr,
    const intel_device_info *devinfo = ntb.devinfo;
    const fs_builder &bld = ntb.bld;
 
+   /* No fast path for f16 (yet) or f64. */
+   assert(instr->op == nir_op_i2f32 || instr->op == nir_op_u2f32);
+
    if (!instr->src[0].src.ssa->parent_instr)
       return false;
 
@@ -509,16 +512,46 @@ optimize_extract_to_float(nir_to_brw_state &ntb, nir_alu_instr *instr,
    nir_alu_instr *src0 =
       nir_instr_as_alu(instr->src[0].src.ssa->parent_instr);
 
-   if (src0->op != nir_op_extract_u8 && src0->op != nir_op_extract_u16 &&
-       src0->op != nir_op_extract_i8 && src0->op != nir_op_extract_i16)
+   unsigned bytes;
+   bool is_signed;
+
+   switch (src0->op) {
+   case nir_op_extract_u8:
+   case nir_op_extract_u16:
+      bytes = src0->op == nir_op_extract_u8 ? 1 : 2;
+
+      /* i2f(extract_u8(a, b)) and u2f(extract_u8(a, b)) produce the same
+       * result. Ditto for extract_u16.
+       */
+      is_signed = false;
+      break;
+
+   case nir_op_extract_i8:
+   case nir_op_extract_i16:
+      bytes = src0->op == nir_op_extract_i8 ? 1 : 2;
+
+      /* The fast path can't handle u2f(extract_i8(a, b)) because the implicit
+       * sign extension of the extract_i8 is lost. For example,
+       * u2f(extract_i8(0x0000ff00, 1)) should produce 4294967295.0, but a
+       * fast path could either give 255.0 (by implementing the fast path as
+       * u2f(extract_u8(x))) or -1.0 (by implementing the fast path as
+       * i2f(extract_i8(x))). At one point in time, we incorrectly implemented
+       * the former.
+       */
+      if (instr->op != nir_op_i2f32)
+         return false;
+
+      is_signed = true;
+      break;
+
+   default:
       return false;
+   }
 
    unsigned element = nir_src_as_uint(src0->src[1].src);
 
    /* Element type to extract.*/
-   const brw_reg_type type = brw_int_type(
-      src0->op == nir_op_extract_u16 || src0->op == nir_op_extract_i16 ? 2 : 1,
-      src0->op == nir_op_extract_i16 || src0->op == nir_op_extract_i8);
+   const brw_reg_type type = brw_int_type(bytes, is_signed);
 
    fs_reg op0 = get_nir_src(ntb, src0->src[0].src);
    op0.type = brw_type_for_nir_type(devinfo,
@@ -4289,7 +4322,8 @@ fs_nir_emit_fs_intrinsic(nir_to_brw_state &ntb,
    case nir_intrinsic_load_barycentric_centroid:
    case nir_intrinsic_load_barycentric_sample: {
       /* Use the delta_xy values computed from the payload */
-      enum brw_barycentric_mode bary = brw_barycentric_mode(instr);
+      enum brw_barycentric_mode bary = brw_barycentric_mode(
+         reinterpret_cast<const brw_wm_prog_key *>(s.key), instr);
       const fs_reg srcs[] = { offset(s.delta_xy[bary], bld, 0),
                               offset(s.delta_xy[bary], bld, 1) };
       bld.LOAD_PAYLOAD(dest, srcs, ARRAY_SIZE(srcs), 0);
@@ -4384,7 +4418,8 @@ fs_nir_emit_fs_intrinsic(nir_to_brw_state &ntb,
          dst_xy = retype(get_nir_src(ntb, instr->src[0]), BRW_REGISTER_TYPE_F);
       } else {
          /* Use the delta_xy values computed from the payload */
-         enum brw_barycentric_mode bary = brw_barycentric_mode(bary_intrinsic);
+         enum brw_barycentric_mode bary = brw_barycentric_mode(
+            reinterpret_cast<const brw_wm_prog_key *>(s.key), bary_intrinsic);
          dst_xy = s.delta_xy[bary];
       }
 
@@ -4608,7 +4643,7 @@ fs_nir_emit_cs_intrinsic(nir_to_brw_state &ntb,
          brw_type_for_nir_type(devinfo, nir_intrinsic_src_type(instr));
 
       dest = retype(dest, dest_type);
-      fs_reg src2 = retype(get_nir_src(ntb, instr->src[2]), dest_type);
+      fs_reg src0 = retype(get_nir_src(ntb, instr->src[0]), dest_type);
       const fs_reg dest_hf = dest;
 
       fs_builder bld8 = bld.exec_all().group(8, 0);
@@ -4624,24 +4659,24 @@ fs_nir_emit_cs_intrinsic(nir_to_brw_state &ntb,
           !s.compiler->lower_dpas) {
          dest = bld8.vgrf(BRW_REGISTER_TYPE_F, rcount);
 
-         if (src2.file != ARF) {
-            const fs_reg src2_hf = src2;
+         if (src0.file != ARF) {
+            const fs_reg src0_hf = src0;
 
-            src2 = bld8.vgrf(BRW_REGISTER_TYPE_F, rcount);
+            src0 = bld8.vgrf(BRW_REGISTER_TYPE_F, rcount);
 
             for (unsigned i = 0; i < 4; i++) {
-               bld16.MOV(byte_offset(src2, REG_SIZE * i * 2),
-                         byte_offset(src2_hf, REG_SIZE * i));
+               bld16.MOV(byte_offset(src0, REG_SIZE * i * 2),
+                         byte_offset(src0_hf, REG_SIZE * i));
             }
          } else {
-            src2 = retype(src2, BRW_REGISTER_TYPE_F);
+            src0 = retype(src0, BRW_REGISTER_TYPE_F);
          }
       }
 
       bld8.DPAS(dest,
-                src2,
+                src0,
+                retype(get_nir_src(ntb, instr->src[2]), src_type),
                 retype(get_nir_src(ntb, instr->src[1]), src_type),
-                retype(get_nir_src(ntb, instr->src[0]), src_type),
                 sdepth,
                 rcount)
          ->saturate = nir_intrinsic_saturate(instr);
diff --git a/src/intel/compiler/brw_fs_validate.cpp b/src/intel/compiler/brw_fs_validate.cpp
index 499bc8181c3..7ef2be70146 100644
--- a/src/intel/compiler/brw_fs_validate.cpp
+++ b/src/intel/compiler/brw_fs_validate.cpp
@@ -191,8 +191,8 @@ fs_visitor::validate()
        */
       if (intel_needs_workaround(devinfo, 14014617373) &&
           inst->dst.is_accumulator() &&
-          inst->dst.offset == 0) {
-         fsv_assert_eq(inst->dst.stride, 1);
+          phys_subnr(devinfo, inst->dst.as_brw_reg()) == 0) {
+         fsv_assert_eq(inst->dst.hstride, 1);
       }
    }
 }
diff --git a/src/intel/compiler/brw_ir_performance.cpp b/src/intel/compiler/brw_ir_performance.cpp
index 9ab7ef563b0..f14a3cc9716 100644
--- a/src/intel/compiler/brw_ir_performance.cpp
+++ b/src/intel/compiler/brw_ir_performance.cpp
@@ -363,6 +363,7 @@ namespace {
       case TCS_OPCODE_SRC0_010_IS_ZERO:
       case TCS_OPCODE_GET_PRIMITIVE_ID:
       case TES_OPCODE_GET_PRIMITIVE_ID:
+      case SHADER_OPCODE_READ_MASK_REG:
       case SHADER_OPCODE_READ_SR_REG:
          if (devinfo->ver >= 11) {
             return calculate_desc(info, EU_UNIT_FPU, 0, 2, 0, 0, 2,
diff --git a/src/intel/compiler/brw_lower_logical_sends.cpp b/src/intel/compiler/brw_lower_logical_sends.cpp
index 1a784010bf8..472b15387f6 100644
--- a/src/intel/compiler/brw_lower_logical_sends.cpp
+++ b/src/intel/compiler/brw_lower_logical_sends.cpp
@@ -1009,10 +1009,14 @@ lower_sampler_logical_send_gfx7(const fs_builder &bld, fs_inst *inst, opcode op,
       /* Build the actual header */
       const fs_builder ubld = bld.exec_all().group(8 * reg_unit(devinfo), 0);
       const fs_builder ubld1 = ubld.group(1, 0);
-      ubld.MOV(header, retype(brw_vec8_grf(0, 0), BRW_REGISTER_TYPE_UD));
+      if (devinfo->ver >= 11)
+         ubld.MOV(header, brw_imm_ud(0));
+      else
+         ubld.MOV(header, retype(brw_vec8_grf(0, 0), BRW_REGISTER_TYPE_UD));
       if (inst->offset) {
          ubld1.MOV(component(header, 2), brw_imm_ud(inst->offset));
-      } else if (bld.shader->stage != MESA_SHADER_VERTEX &&
+      } else if (devinfo->ver < 11 &&
+                 bld.shader->stage != MESA_SHADER_VERTEX &&
                  bld.shader->stage != MESA_SHADER_FRAGMENT) {
          /* The vertex and fragment stages have g0.2 set to 0, so
           * header0.2 is 0 when g0 is copied. Other stages may not, so we
diff --git a/src/intel/compiler/brw_nir.h b/src/intel/compiler/brw_nir.h
index 119de1c6086..7d8870732ad 100644
--- a/src/intel/compiler/brw_nir.h
+++ b/src/intel/compiler/brw_nir.h
@@ -122,8 +122,7 @@ brw_nir_ubo_surface_index_is_pushable(nir_src src)
 
    if (intrin && intrin->intrinsic == nir_intrinsic_resource_intel) {
       return (nir_intrinsic_resource_access_intel(intrin) &
-              nir_resource_intel_pushable) &&
-             nir_src_is_const(intrin->src[1]);
+              nir_resource_intel_pushable);
    }
 
    return nir_src_is_const(src);
@@ -146,6 +145,14 @@ brw_nir_ubo_surface_index_get_push_block(nir_src src)
    return nir_intrinsic_resource_block_intel(intrin);
 }
 
+/* This helper return the binding table index of a surface access (any
+ * buffer/image/etc...). It works off the source of one of the intrinsics
+ * (load_ubo, load_ssbo, store_ssbo, load_image, store_image, etc...).
+ *
+ * If the source is constant, then this is the binding table index. If we're
+ * going through a resource_intel intel intrinsic, then we need to check
+ * src[1] of that intrinsic.
+ */
 static inline unsigned
 brw_nir_ubo_surface_index_get_bti(nir_src src)
 {
@@ -155,8 +162,19 @@ brw_nir_ubo_surface_index_get_bti(nir_src src)
    assert(src.ssa->parent_instr->type == nir_instr_type_intrinsic);
 
    nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(src.ssa->parent_instr);
-   assert(intrin->intrinsic == nir_intrinsic_resource_intel);
-   assert(nir_src_is_const(intrin->src[1]));
+   if (!intrin || intrin->intrinsic != nir_intrinsic_resource_intel)
+      return UINT32_MAX;
+
+   /* In practice we could even drop this intrinsic because the bindless
+    * access always operate from a base offset coming from a push constant, so
+    * they can never be constant.
+    */
+   if (nir_intrinsic_resource_access_intel(intrin) &
+       nir_resource_intel_bindless)
+      return UINT32_MAX;
+
+   if (!nir_src_is_const(intrin->src[1]))
+      return UINT32_MAX;
 
    return nir_src_as_uint(intrin->src[1]);
 }
diff --git a/src/intel/compiler/brw_nir_lower_alpha_to_coverage.c b/src/intel/compiler/brw_nir_lower_alpha_to_coverage.c
index eb13698b536..99c8855d43c 100644
--- a/src/intel/compiler/brw_nir_lower_alpha_to_coverage.c
+++ b/src/intel/compiler/brw_nir_lower_alpha_to_coverage.c
@@ -58,8 +58,7 @@
 static nir_def *
 build_dither_mask(nir_builder *b, nir_def *color)
 {
-   assert(color->num_components == 4);
-   nir_def *alpha = nir_channel(b, color, 3);
+   nir_def *alpha = nir_channel(b, color, color->num_components - 1);
 
    nir_def *m =
       nir_f2i32(b, nir_fmul_imm(b, nir_fsat(b, alpha), 16.0));
@@ -130,6 +129,11 @@ brw_nir_lower_alpha_to_coverage(nir_shader *shader,
 
          if (location == FRAG_RESULT_COLOR ||
              location == FRAG_RESULT_DATA0) {
+            uint32_t mask = nir_intrinsic_write_mask(intrin) <<
+                            nir_intrinsic_component(intrin);
+            /* need the w component */
+            if (!(mask & BITFIELD_BIT(3)))
+               continue;
             assert(color0_write == NULL);
             color0_write = intrin;
          }
@@ -137,22 +141,19 @@ brw_nir_lower_alpha_to_coverage(nir_shader *shader,
    }
 
    /* It's possible that shader_info may be out-of-date and the writes to
-    * either gl_SampleMask or the first color value may have been removed.
+    * either gl_SampleMask, or the first color value may have been removed,
+    * or that the w component is not written.
     * This can happen if, for instance a nir_undef is written to the
     * color value.  In that case, just bail and don't do anything rather
     * than crashing.
-    */
-   if (color0_write == NULL || sample_mask_write == NULL)
-      goto skip;
-
-   /* It's possible that the color value isn't actually a vec4.  In this case,
+    * It's also possible that the color value isn't actually a vec4.  In this case,
     * assuming an alpha of 1.0 and letting the sample mask pass through
     * unaltered seems like the kindest thing to do to apps.
     */
-   nir_def *color0 = color0_write->src[0].ssa;
-   if (color0->num_components < 4)
+   if (color0_write == NULL || sample_mask_write == NULL)
       goto skip;
 
+   nir_def *color0 = color0_write->src[0].ssa;
    nir_def *sample_mask = sample_mask_write->src[0].ssa;
 
    if (sample_mask_write_first) {
diff --git a/src/intel/compiler/brw_nir_lower_cooperative_matrix.c b/src/intel/compiler/brw_nir_lower_cooperative_matrix.c
index 8ed937baaed..8f1ff3ed0e3 100644
--- a/src/intel/compiler/brw_nir_lower_cooperative_matrix.c
+++ b/src/intel/compiler/brw_nir_lower_cooperative_matrix.c
@@ -251,10 +251,22 @@ lower_cmat_load_store(nir_builder *b, nir_intrinsic_instr *intrin,
    const unsigned packing_factor = get_packing_factor(*desc, slice->type);
 
    nir_deref_instr *pointer = nir_src_as_deref(intrin->src[ptr_src]);
+   const unsigned ptr_comp_width = glsl_get_bit_size(pointer->type);
+   const unsigned ptr_num_comps = glsl_get_vector_elements(pointer->type);
+
+   /* The stride is given in number of elements of the pointed type, which
+    * doesn't necessarily match the matrix element type, so we need to adjust
+    * it considering it may be a vector and have a different bit-width.
+    */
+   nir_def *stride = nir_udiv_imm(b,
+                                  nir_imul_imm(b,
+                                               intrin->src[2].ssa,
+                                               ptr_comp_width * ptr_num_comps),
+                                  glsl_base_type_get_bit_size(desc->element_type));
 
    if ((nir_intrinsic_matrix_layout(intrin) == GLSL_MATRIX_LAYOUT_ROW_MAJOR) ==
        (desc->use != GLSL_CMAT_USE_B)) {
-      nir_def *stride = nir_udiv_imm(b, intrin->src[2].ssa, packing_factor);
+      stride = nir_udiv_imm(b, stride, packing_factor);
 
       const struct glsl_type *element_type =
          glsl_scalar_type(glsl_get_base_type(slice->type));
@@ -304,8 +316,6 @@ lower_cmat_load_store(nir_builder *b, nir_intrinsic_instr *intrin,
          }
       }
    } else {
-      nir_def *stride = intrin->src[2].ssa;
-
       const struct glsl_type *element_type = glsl_scalar_type(desc->element_type);
       const unsigned element_bits = glsl_base_type_get_bit_size(desc->element_type);
       const unsigned element_stride = element_bits / 8;
@@ -636,14 +646,44 @@ lower_cmat_instr(nir_builder *b, nir_instr *instr, void *_state)
       const unsigned packing_factor = get_packing_factor(dst_desc, dst_slice->type);
       const unsigned num_components = glsl_get_vector_elements(dst_slice->type);
 
+      const nir_cmat_signed cmat_signed_mask =
+         nir_intrinsic_cmat_signed_mask(intrin);
+
+      assert(((cmat_signed_mask & NIR_CMAT_A_SIGNED) == 0) ==
+             ((cmat_signed_mask & NIR_CMAT_B_SIGNED) == 0));
+      assert(((cmat_signed_mask & NIR_CMAT_A_SIGNED) == 0) ==
+             ((cmat_signed_mask & NIR_CMAT_C_SIGNED) == 0));
+      assert(((cmat_signed_mask & NIR_CMAT_A_SIGNED) == 0) ==
+             ((cmat_signed_mask & NIR_CMAT_RESULT_SIGNED) == 0));
+
+      nir_alu_type src_type =
+         nir_get_nir_type_for_glsl_base_type(src_desc.element_type);
+      nir_alu_type dest_type =
+         nir_get_nir_type_for_glsl_base_type(dst_desc.element_type);
+
+      /* For integer types, the signedness is determined by flags on the
+       * muladd instruction. The types of the sources play no role. Adjust the
+       * types passed to the dpas_intel intrinsic to match.
+       */
+      if (nir_alu_type_get_base_type(src_type) == nir_type_uint ||
+          nir_alu_type_get_base_type(src_type) == nir_type_int) {
+         if ((cmat_signed_mask & NIR_CMAT_A_SIGNED) == 0) {
+            src_type = nir_alu_type_get_type_size(src_type) | nir_type_uint;
+            dest_type = nir_alu_type_get_type_size(dest_type) | nir_type_uint;
+         } else {
+            src_type = nir_alu_type_get_type_size(src_type) | nir_type_int;
+            dest_type = nir_alu_type_get_type_size(dest_type) | nir_type_int;
+         }
+      }
+
       nir_def *result =
          nir_dpas_intel(b,
                         packing_factor * glsl_base_type_get_bit_size(dst_desc.element_type),
+                        nir_load_deref(b, accum_slice),
                         nir_load_deref(b, A_slice),
                         nir_load_deref(b, B_slice),
-                        nir_load_deref(b, accum_slice),
-                        .dest_type = nir_get_nir_type_for_glsl_base_type(dst_desc.element_type),
-                        .src_type = nir_get_nir_type_for_glsl_base_type(src_desc.element_type),
+                        .dest_type = dest_type,
+                        .src_type = src_type,
                         .saturate = nir_intrinsic_saturate(intrin),
                         .cmat_signed_mask = nir_intrinsic_cmat_signed_mask(intrin),
                         .systolic_depth = 8,
diff --git a/src/intel/compiler/brw_nir_lower_ray_queries.c b/src/intel/compiler/brw_nir_lower_ray_queries.c
index bcade17e803..844b90da47b 100644
--- a/src/intel/compiler/brw_nir_lower_ray_queries.c
+++ b/src/intel/compiler/brw_nir_lower_ray_queries.c
@@ -543,8 +543,11 @@ brw_nir_lower_ray_queries(nir_shader *shader,
    };
 
    /* Map all query variable to internal type variables */
-   nir_foreach_function_temp_variable(var, state.impl)
+   nir_foreach_function_temp_variable(var, state.impl) {
+      if (!var->data.ray_query)
+         continue;
       register_opaque_var(var, &state);
+   }
    hash_table_foreach(state.queries, entry)
       create_internal_var(entry->data, &state);
 
diff --git a/src/intel/compiler/brw_reg.h b/src/intel/compiler/brw_reg.h
index 3bee5dde058..7e2243c4625 100644
--- a/src/intel/compiler/brw_reg.h
+++ b/src/intel/compiler/brw_reg.h
@@ -260,6 +260,39 @@ struct brw_reg {
    };
 };
 
+static inline unsigned
+phys_nr(const struct intel_device_info *devinfo, const struct brw_reg reg)
+{
+   if (devinfo->ver >= 20) {
+      if (reg.file == BRW_GENERAL_REGISTER_FILE)
+         return reg.nr / 2;
+      else if (reg.file == BRW_ARCHITECTURE_REGISTER_FILE &&
+               reg.nr >= BRW_ARF_ACCUMULATOR &&
+               reg.nr < BRW_ARF_FLAG)
+         return BRW_ARF_ACCUMULATOR + (reg.nr - BRW_ARF_ACCUMULATOR) / 2;
+      else
+         return reg.nr;
+   } else {
+      return reg.nr;
+   }
+}
+
+static inline unsigned
+phys_subnr(const struct intel_device_info *devinfo, const struct brw_reg reg)
+{
+   if (devinfo->ver >= 20) {
+      if (reg.file == BRW_GENERAL_REGISTER_FILE ||
+          (reg.file == BRW_ARCHITECTURE_REGISTER_FILE &&
+           reg.nr >= BRW_ARF_ACCUMULATOR &&
+           reg.nr < BRW_ARF_FLAG))
+         return (reg.nr & 1) * REG_SIZE + reg.subnr;
+      else
+         return reg.subnr;
+   } else {
+      return reg.subnr;
+   }
+}
+
 static inline bool
 brw_regs_equal(const struct brw_reg *a, const struct brw_reg *b)
 {
diff --git a/src/intel/compiler/brw_shader.cpp b/src/intel/compiler/brw_shader.cpp
index c500f5e2b4f..e36389f724d 100644
--- a/src/intel/compiler/brw_shader.cpp
+++ b/src/intel/compiler/brw_shader.cpp
@@ -531,6 +531,8 @@ brw_instruction_name(const struct brw_isa_info *isa, enum opcode op)
       return "btd_spawn_logical";
    case SHADER_OPCODE_BTD_RETIRE_LOGICAL:
       return "btd_retire_logical";
+   case SHADER_OPCODE_READ_MASK_REG:
+      return "read_mask_reg";
    case SHADER_OPCODE_READ_SR_REG:
       return "read_sr_reg";
    }
diff --git a/src/intel/dev/intel_device_info.c b/src/intel/dev/intel_device_info.c
index 1ed313d71a2..c94982a3666 100644
--- a/src/intel/dev/intel_device_info.c
+++ b/src/intel/dev/intel_device_info.c
@@ -1667,6 +1667,8 @@ intel_get_device_info_from_fd(int fd, struct intel_device_info *devinfo)
       break;
    case INTEL_KMD_TYPE_XE:
       ret = intel_device_info_xe_get_info_from_fd(fd, devinfo);
+      if (devinfo->verx10 < 200)
+         mesa_logw("Support for this platform is experimental with Xe KMD, bug reports may be ignored.");
       break;
    default:
       ret = false;
diff --git a/src/intel/dev/intel_kmd.c b/src/intel/dev/intel_kmd.c
index cca27ce67ed..63bb443424a 100644
--- a/src/intel/dev/intel_kmd.c
+++ b/src/intel/dev/intel_kmd.c
@@ -37,10 +37,8 @@ intel_get_kmd_type(int fd)
 
    if (strcmp(version->name, "i915") == 0)
       type = INTEL_KMD_TYPE_I915;
-#ifdef INTEL_XE_KMD_SUPPORTED
    else if (strcmp(version->name, "xe") == 0)
       type = INTEL_KMD_TYPE_XE;
-#endif
 
    drmFreeVersion(version);
    return type;
diff --git a/src/intel/isl/isl.c b/src/intel/isl/isl.c
index 3b06af225cd..3793ea5d929 100644
--- a/src/intel/isl/isl.c
+++ b/src/intel/isl/isl.c
@@ -175,6 +175,8 @@ isl_device_setup_mocs(struct isl_device *dev)
          dev->mocs.external = 5 << 1;
          /* UC */
          dev->mocs.uncached = 1 << 1;
+         dev->mocs.blitter_dst = 1 << 1;
+         dev->mocs.blitter_src = 1 << 1;
       } else {
          /* TC=1/LLC Only, LeCC=1/UC, LRUM=0, L3CC=3/WB */
          dev->mocs.external = 61 << 1;
@@ -185,6 +187,10 @@ isl_device_setup_mocs(struct isl_device *dev)
 
          /* L1 - HDC:L1 + L3 + LLC */
          dev->mocs.l1_hdc_l3_llc = 48 << 1;
+
+         /* Uncached */
+         dev->mocs.blitter_dst = 3 << 1;
+         dev->mocs.blitter_src = 3 << 1;
       }
       /* Protected is just an additional flag. */
       dev->mocs.protected_mask = 1 << 0;
@@ -267,6 +273,12 @@ isl_mocs(const struct isl_device *dev, isl_surf_usage_flags_t usage,
    uint32_t mask = (usage & ISL_SURF_USAGE_PROTECTED_BIT) ?
       dev->mocs.protected_mask : 0;
 
+   if (usage & ISL_SURF_USAGE_BLITTER_SRC_BIT)
+      return dev->mocs.blitter_src | mask;
+
+   if (usage & ISL_SURF_USAGE_BLITTER_DST_BIT)
+      return dev->mocs.blitter_dst | mask;
+
    if (external)
       return dev->mocs.external | mask;
 
@@ -279,7 +291,7 @@ isl_mocs(const struct isl_device *dev, isl_surf_usage_flags_t usage,
          return dev->mocs.internal | mask;
 
       if (usage & ISL_SURF_USAGE_CPB_BIT)
-         return dev->mocs.internal;
+         return dev->mocs.internal | mask;
 
       /* Using L1:HDC for storage buffers breaks Vulkan memory model
        * tests that use shader atomics.  This isn't likely to work out,
@@ -4058,6 +4070,13 @@ isl_swizzle_supports_rendering(const struct intel_device_info *devinfo,
        *
        *    "For Render Target, this field MUST be programmed to
        *    value = SCS_ALPHA."
+       *
+       * Bspec 57023: RENDER_SURFACE_STATE:: Shader Channel Select Red
+       *
+       *    "Render Target messages do not support swapping of colors with
+       *    alpha. The Red, Green, or Blue Shader Channel Selects do not
+       *    support SCS_ALPHA. The Shader Channel Select Alpha does not support
+       *    SCS_RED, SCS_GREEN, or SCS_BLUE."
        */
       return (swizzle.r == ISL_CHANNEL_SELECT_RED ||
               swizzle.r == ISL_CHANNEL_SELECT_GREEN ||
diff --git a/src/intel/isl/isl.h b/src/intel/isl/isl.h
index 0870aa590e5..1a6424528a6 100644
--- a/src/intel/isl/isl.h
+++ b/src/intel/isl/isl.h
@@ -1142,6 +1142,8 @@ typedef uint64_t isl_surf_usage_flags_t;
 #define ISL_SURF_USAGE_STREAM_OUT_BIT          (1u << 18)
 #define ISL_SURF_USAGE_2D_3D_COMPATIBLE_BIT    (1u << 19)
 #define ISL_SURF_USAGE_SPARSE_BIT              (1u << 20)
+#define ISL_SURF_USAGE_BLITTER_DST_BIT         (1u << 22)
+#define ISL_SURF_USAGE_BLITTER_SRC_BIT         (1u << 23)
 /** @} */
 
 /**
diff --git a/src/intel/isl/isl_emit_depth_stencil.c b/src/intel/isl/isl_emit_depth_stencil.c
index a914b8796a3..48e5343b9b0 100644
--- a/src/intel/isl/isl_emit_depth_stencil.c
+++ b/src/intel/isl/isl_emit_depth_stencil.c
@@ -199,6 +199,9 @@ isl_genX(emit_depth_stencil_hiz_s)(const struct isl_device *dev, void *batch,
 #if GFX_VER == 12
       db.ControlSurfaceEnable = db.DepthBufferCompressionEnable =
          isl_aux_usage_has_ccs(info->hiz_usage);
+#endif
+#if GFX_VER >= 12
+      db.NullPageCoherencyEnable = info->depth_surf->usage & ISL_SURF_USAGE_SPARSE_BIT;
 #endif
    }
 
@@ -270,6 +273,9 @@ isl_genX(emit_depth_stencil_hiz_s)(const struct isl_device *dev, void *batch,
 #if GFX_VER >= 8
       sb.SurfaceQPitch =
          isl_surf_get_array_pitch_el_rows(info->stencil_surf) >> 2;
+#endif
+#if GFX_VER >= 12
+      sb.NullPageCoherencyEnable = info->stencil_surf->usage & ISL_SURF_USAGE_SPARSE_BIT;
 #endif
    } else {
 #if GFX_VER >= 12
diff --git a/src/intel/tools/intel_sanitize_gpu.c b/src/intel/tools/intel_sanitize_gpu.c
index a3332507090..dc99e1b7960 100644
--- a/src/intel/tools/intel_sanitize_gpu.c
+++ b/src/intel/tools/intel_sanitize_gpu.c
@@ -22,6 +22,7 @@
  */
 
 #undef _FILE_OFFSET_BITS /* prevent #define open open64 */
+#undef _TIME_BITS
 
 #include <string.h>
 #include <stdlib.h>
diff --git a/src/intel/vulkan/anv_allocator.c b/src/intel/vulkan/anv_allocator.c
index 769bbd848a7..14b30b114ea 100644
--- a/src/intel/vulkan/anv_allocator.c
+++ b/src/intel/vulkan/anv_allocator.c
@@ -1029,6 +1029,9 @@ anv_state_stream_alloc(struct anv_state_stream *stream,
 
       stream->block = anv_state_pool_alloc_no_vg(stream->state_pool,
                                                  block_size, PAGE_SIZE);
+      if (stream->block.alloc_size == 0)
+         return ANV_STATE_NULL;
+
       util_dynarray_append(&stream->all_blocks,
                            struct anv_state, stream->block);
       VG(VALGRIND_MAKE_MEM_NOACCESS(stream->block.map, block_size));
diff --git a/src/intel/vulkan/anv_batch_chain.c b/src/intel/vulkan/anv_batch_chain.c
index 0dfdd582a4b..75f696cb1e7 100644
--- a/src/intel/vulkan/anv_batch_chain.c
+++ b/src/intel/vulkan/anv_batch_chain.c
@@ -1389,8 +1389,6 @@ anv_queue_submit_sparse_bind_locked(struct anv_queue *queue,
       return vk_queue_set_lost(&queue->vk, "Sparse binding not supported");
    }
 
-   device->using_sparse = true;
-
    assert(submit->command_buffer_count == 0);
 
    if (INTEL_DEBUG(DEBUG_SPARSE)) {
diff --git a/src/intel/vulkan/anv_blorp.c b/src/intel/vulkan/anv_blorp.c
index ce5c63e7c31..da786b8b265 100644
--- a/src/intel/vulkan/anv_blorp.c
+++ b/src/intel/vulkan/anv_blorp.c
@@ -136,8 +136,34 @@ anv_blorp_batch_finish(struct blorp_batch *batch)
    blorp_batch_finish(batch);
 }
 
+static isl_surf_usage_flags_t
+get_usage_flag_for_cmd_buffer(const struct anv_cmd_buffer *cmd_buffer,
+                              bool is_dest)
+{
+   isl_surf_usage_flags_t usage;
+
+   switch (cmd_buffer->queue_family->engine_class) {
+   case INTEL_ENGINE_CLASS_RENDER:
+      usage = is_dest ? ISL_SURF_USAGE_RENDER_TARGET_BIT :
+                        ISL_SURF_USAGE_TEXTURE_BIT;
+      break;
+   case INTEL_ENGINE_CLASS_COMPUTE:
+      usage = is_dest ? ISL_SURF_USAGE_STORAGE_BIT :
+                        ISL_SURF_USAGE_TEXTURE_BIT;
+      break;
+   case INTEL_ENGINE_CLASS_COPY:
+      usage = is_dest ? ISL_SURF_USAGE_BLITTER_DST_BIT :
+                        ISL_SURF_USAGE_BLITTER_SRC_BIT;
+      break;
+   default:
+      unreachable("Unhandled engine class");
+   }
+
+   return usage;
+}
+
 static void
-get_blorp_surf_for_anv_address(struct anv_device *device,
+get_blorp_surf_for_anv_address(struct anv_cmd_buffer *cmd_buffer,
                                struct anv_address address,
                                uint32_t width, uint32_t height,
                                uint32_t row_pitch, enum isl_format format,
@@ -146,19 +172,19 @@ get_blorp_surf_for_anv_address(struct anv_device *device,
                                struct isl_surf *isl_surf)
 {
    bool ok UNUSED;
+   isl_surf_usage_flags_t usage =
+      get_usage_flag_for_cmd_buffer(cmd_buffer, is_dest);
 
    *blorp_surf = (struct blorp_surf) {
       .surf = isl_surf,
       .addr = {
          .buffer = address.bo,
          .offset = address.offset,
-         .mocs = anv_mocs(device, address.bo,
-                          is_dest ? ISL_SURF_USAGE_RENDER_TARGET_BIT
-                                  : ISL_SURF_USAGE_TEXTURE_BIT),
+         .mocs = anv_mocs(cmd_buffer->device, address.bo, usage),
       },
    };
 
-   ok = isl_surf_init(&device->isl_dev, isl_surf,
+   ok = isl_surf_init(&cmd_buffer->device->isl_dev, isl_surf,
                      .dim = ISL_SURF_DIM_2D,
                      .format = format,
                      .width = width,
@@ -168,14 +194,13 @@ get_blorp_surf_for_anv_address(struct anv_device *device,
                      .array_len = 1,
                      .samples = 1,
                      .row_pitch_B = row_pitch,
-                     .usage = is_dest ? ISL_SURF_USAGE_RENDER_TARGET_BIT
-                                      : ISL_SURF_USAGE_TEXTURE_BIT,
+                     .usage = usage,
                      .tiling_flags = ISL_TILING_LINEAR_BIT);
    assert(ok);
 }
 
 static void
-get_blorp_surf_for_anv_buffer(struct anv_device *device,
+get_blorp_surf_for_anv_buffer(struct anv_cmd_buffer *cmd_buffer,
                               struct anv_buffer *buffer, uint64_t offset,
                               uint32_t width, uint32_t height,
                               uint32_t row_pitch, enum isl_format format,
@@ -183,7 +208,7 @@ get_blorp_surf_for_anv_buffer(struct anv_device *device,
                               struct blorp_surf *blorp_surf,
                               struct isl_surf *isl_surf)
 {
-   get_blorp_surf_for_anv_address(device,
+   get_blorp_surf_for_anv_address(cmd_buffer,
                                   anv_address_add(buffer->address, offset),
                                   width, height, row_pitch, format,
                                   is_dest, blorp_surf, isl_surf);
@@ -222,10 +247,9 @@ get_blorp_surf_for_anv_image(const struct anv_cmd_buffer *cmd_buffer,
                                           cmd_buffer->queue_family->queueFlags);
    }
 
-   isl_surf_usage_flags_t mocs_usage =
-      (usage & VK_IMAGE_USAGE_TRANSFER_DST_BIT) ?
-      ISL_SURF_USAGE_RENDER_TARGET_BIT : ISL_SURF_USAGE_TEXTURE_BIT;
-
+   isl_surf_usage_flags_t isl_usage =
+      get_usage_flag_for_cmd_buffer(cmd_buffer,
+                                    usage & VK_IMAGE_USAGE_TRANSFER_DST_BIT);
    const struct anv_surface *surface = &image->planes[plane].primary_surface;
    const struct anv_address address =
       anv_image_address(image, &surface->memory_range);
@@ -235,7 +259,7 @@ get_blorp_surf_for_anv_image(const struct anv_cmd_buffer *cmd_buffer,
       .addr = {
          .buffer = address.bo,
          .offset = address.offset,
-         .mocs = anv_mocs(device, address.bo, mocs_usage),
+         .mocs = anv_mocs(device, address.bo, isl_usage),
       },
    };
 
@@ -251,7 +275,7 @@ get_blorp_surf_for_anv_image(const struct anv_cmd_buffer *cmd_buffer,
          blorp_surf->aux_addr = (struct blorp_address) {
             .buffer = aux_address.bo,
             .offset = aux_address.offset,
-            .mocs = anv_mocs(device, aux_address.bo, 0),
+            .mocs = anv_mocs(device, aux_address.bo, isl_usage),
          };
       }
 
@@ -622,7 +646,7 @@ copy_buffer_to_image(struct anv_cmd_buffer *cmd_buffer,
       isl_format_for_size(linear_fmtl->bpb / 8);
 
    struct isl_surf buffer_isl_surf;
-   get_blorp_surf_for_anv_buffer(cmd_buffer->device,
+   get_blorp_surf_for_anv_buffer(cmd_buffer,
                                  anv_buffer, region->bufferOffset,
                                  buffer_extent.width, buffer_extent.height,
                                  buffer_layout.row_stride_B, buffer_format,
@@ -982,13 +1006,13 @@ copy_buffer(struct anv_device *device,
       .buffer = src_buffer->address.bo,
       .offset = src_buffer->address.offset + region->srcOffset,
       .mocs = anv_mocs(device, src_buffer->address.bo,
-                       ISL_SURF_USAGE_TEXTURE_BIT),
+                       blorp_batch_isl_copy_usage(batch, false /* is_dest */)),
    };
    struct blorp_address dst = {
       .buffer = dst_buffer->address.bo,
       .offset = dst_buffer->address.offset + region->dstOffset,
       .mocs = anv_mocs(device, dst_buffer->address.bo,
-                       ISL_SURF_USAGE_RENDER_TARGET_BIT),
+                       blorp_batch_isl_copy_usage(batch, true /* is_dest */)),
    };
 
    blorp_buffer_copy(batch, src, dst, region->size);
@@ -1055,14 +1079,16 @@ void anv_CmdUpdateBuffer(
       struct blorp_address src = {
          .buffer = cmd_buffer->device->dynamic_state_pool.block_pool.bo,
          .offset = tmp_data.offset,
-         .mocs = isl_mocs(&cmd_buffer->device->isl_dev,
-                          ISL_SURF_USAGE_TEXTURE_BIT, false)
+         .mocs = anv_mocs(cmd_buffer->device, NULL,
+                          get_usage_flag_for_cmd_buffer(cmd_buffer,
+                                                        false /* is_dest */))
       };
       struct blorp_address dst = {
          .buffer = dst_buffer->address.bo,
          .offset = dst_buffer->address.offset + dstOffset,
          .mocs = anv_mocs(cmd_buffer->device, dst_buffer->address.bo,
-                          ISL_SURF_USAGE_RENDER_TARGET_BIT),
+                          get_usage_flag_for_cmd_buffer(cmd_buffer,
+                                                        true /* is_dest */)),
       };
 
       blorp_buffer_copy(&batch, src, dst, copy_size);
@@ -1104,12 +1130,13 @@ anv_cmd_buffer_fill_area(struct anv_cmd_buffer *cmd_buffer,
 
    const uint64_t max_fill_size = MAX_SURFACE_DIM * MAX_SURFACE_DIM * bs;
    while (size >= max_fill_size) {
-      get_blorp_surf_for_anv_address(cmd_buffer->device,
+      get_blorp_surf_for_anv_address(cmd_buffer,
                                      (struct anv_address) {
                                         .bo = address.bo, .offset = offset,
                                      },
                                      MAX_SURFACE_DIM, MAX_SURFACE_DIM,
-                                     MAX_SURFACE_DIM * bs, isl_format, true,
+                                     MAX_SURFACE_DIM * bs, isl_format,
+                                     true /* is_dest */,
                                      &surf, &isl_surf);
 
       blorp_clear(&batch, &surf, isl_format, ISL_SWIZZLE_IDENTITY,
@@ -1123,12 +1150,13 @@ anv_cmd_buffer_fill_area(struct anv_cmd_buffer *cmd_buffer,
    assert(height < MAX_SURFACE_DIM);
    if (height != 0) {
       const uint64_t rect_fill_size = height * MAX_SURFACE_DIM * bs;
-      get_blorp_surf_for_anv_address(cmd_buffer->device,
+      get_blorp_surf_for_anv_address(cmd_buffer,
                                      (struct anv_address) {
                                         .bo = address.bo, .offset = offset,
                                      },
                                      MAX_SURFACE_DIM, height,
-                                     MAX_SURFACE_DIM * bs, isl_format, true,
+                                     MAX_SURFACE_DIM * bs, isl_format,
+                                     true /* is_dest */,
                                      &surf, &isl_surf);
 
       blorp_clear(&batch, &surf, isl_format, ISL_SWIZZLE_IDENTITY,
@@ -1140,12 +1168,13 @@ anv_cmd_buffer_fill_area(struct anv_cmd_buffer *cmd_buffer,
 
    if (size != 0) {
       const uint32_t width = size / bs;
-      get_blorp_surf_for_anv_address(cmd_buffer->device,
+      get_blorp_surf_for_anv_address(cmd_buffer,
                                      (struct anv_address) {
                                         .bo = address.bo, .offset = offset,
                                      },
                                      width, 1,
-                                     width * bs, isl_format, true,
+                                     width * bs, isl_format,
+                                     true /* is_dest */,
                                      &surf, &isl_surf);
 
       blorp_clear(&batch, &surf, isl_format, ISL_SWIZZLE_IDENTITY,
@@ -2106,12 +2135,14 @@ void anv_CmdClearAttachments(
    anv_blorp_batch_finish(&batch);
 }
 
-void
+static void
 anv_image_msaa_resolve(struct anv_cmd_buffer *cmd_buffer,
                        const struct anv_image *src_image,
+                       enum isl_format src_format_override,
                        enum isl_aux_usage src_aux_usage,
                        uint32_t src_level, uint32_t src_base_layer,
                        const struct anv_image *dst_image,
+                       enum isl_format dst_format_override,
                        enum isl_aux_usage dst_aux_usage,
                        uint32_t dst_level, uint32_t dst_base_layer,
                        VkImageAspectFlagBits aspect,
@@ -2164,9 +2195,9 @@ anv_image_msaa_resolve(struct anv_cmd_buffer *cmd_buffer,
    for (uint32_t l = 0; l < layer_count; l++) {
       blorp_blit(&batch,
                  &src_surf, src_level, src_base_layer + l,
-                 ISL_FORMAT_UNSUPPORTED, ISL_SWIZZLE_IDENTITY,
+                 src_format_override, ISL_SWIZZLE_IDENTITY,
                  &dst_surf, dst_level, dst_base_layer + l,
-                 ISL_FORMAT_UNSUPPORTED, ISL_SWIZZLE_IDENTITY,
+                 dst_format_override, ISL_SWIZZLE_IDENTITY,
                  src_x, src_y, src_x + width, src_y + height,
                  dst_x, dst_y, dst_x + width, dst_y + height,
                  filter, false, false);
@@ -2175,6 +2206,96 @@ anv_image_msaa_resolve(struct anv_cmd_buffer *cmd_buffer,
    anv_blorp_batch_finish(&batch);
 }
 
+static enum blorp_filter
+vk_to_blorp_resolve_mode(VkResolveModeFlagBits vk_mode)
+{
+   switch (vk_mode) {
+   case VK_RESOLVE_MODE_SAMPLE_ZERO_BIT:
+      return BLORP_FILTER_SAMPLE_0;
+   case VK_RESOLVE_MODE_AVERAGE_BIT:
+      return BLORP_FILTER_AVERAGE;
+   case VK_RESOLVE_MODE_MIN_BIT:
+      return BLORP_FILTER_MIN_SAMPLE;
+   case VK_RESOLVE_MODE_MAX_BIT:
+      return BLORP_FILTER_MAX_SAMPLE;
+   default:
+      return BLORP_FILTER_NONE;
+   }
+}
+
+void
+anv_attachment_msaa_resolve(struct anv_cmd_buffer *cmd_buffer,
+                            const struct anv_attachment *att,
+                            VkImageLayout layout,
+                            VkImageAspectFlagBits aspect)
+{
+   struct anv_cmd_graphics_state *gfx = &cmd_buffer->state.gfx;
+   const struct anv_image_view *src_iview = att->iview;
+   const struct anv_image_view *dst_iview = att->resolve_iview;
+
+   enum isl_aux_usage src_aux_usage =
+      anv_layout_to_aux_usage(cmd_buffer->device->info,
+                              src_iview->image, aspect,
+                              VK_IMAGE_USAGE_TRANSFER_SRC_BIT,
+                              layout,
+                              cmd_buffer->queue_family->queueFlags);
+
+   enum isl_aux_usage dst_aux_usage =
+      anv_layout_to_aux_usage(cmd_buffer->device->info,
+                              dst_iview->image, aspect,
+                              VK_IMAGE_USAGE_TRANSFER_DST_BIT,
+                              att->resolve_layout,
+                              cmd_buffer->queue_family->queueFlags);
+
+   enum blorp_filter filter = vk_to_blorp_resolve_mode(att->resolve_mode);
+
+   /* Depth/stencil should not use their view format for resolve because they
+    * go in pairs.
+    */
+   enum isl_format src_format = ISL_FORMAT_UNSUPPORTED;
+   enum isl_format dst_format = ISL_FORMAT_UNSUPPORTED;
+   if (!(aspect & (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT))) {
+      src_format = src_iview->planes[0].isl.format;
+      dst_format = dst_iview->planes[0].isl.format;
+   }
+
+   const VkRect2D render_area = gfx->render_area;
+   if (gfx->view_mask == 0) {
+      anv_image_msaa_resolve(cmd_buffer,
+                             src_iview->image, src_format, src_aux_usage,
+                             src_iview->planes[0].isl.base_level,
+                             src_iview->planes[0].isl.base_array_layer,
+                             dst_iview->image, dst_format, dst_aux_usage,
+                             dst_iview->planes[0].isl.base_level,
+                             dst_iview->planes[0].isl.base_array_layer,
+                             aspect,
+                             render_area.offset.x, render_area.offset.y,
+                             render_area.offset.x, render_area.offset.y,
+                             render_area.extent.width,
+                             render_area.extent.height,
+                             gfx->layer_count, filter);
+   } else {
+      uint32_t res_view_mask = gfx->view_mask;
+      while (res_view_mask) {
+         int i = u_bit_scan(&res_view_mask);
+
+         anv_image_msaa_resolve(cmd_buffer,
+                                src_iview->image, src_format, src_aux_usage,
+                                src_iview->planes[0].isl.base_level,
+                                src_iview->planes[0].isl.base_array_layer + i,
+                                dst_iview->image, dst_format, dst_aux_usage,
+                                dst_iview->planes[0].isl.base_level,
+                                dst_iview->planes[0].isl.base_array_layer + i,
+                                aspect,
+                                render_area.offset.x, render_area.offset.y,
+                                render_area.offset.x, render_area.offset.y,
+                                render_area.extent.width,
+                                render_area.extent.height,
+                                1, filter);
+      }
+   }
+}
+
 static void
 resolve_image(struct anv_cmd_buffer *cmd_buffer,
               struct anv_image *src_image,
@@ -2206,10 +2327,10 @@ resolve_image(struct anv_cmd_buffer *cmd_buffer,
                                  cmd_buffer->queue_family->queueFlags);
 
       anv_image_msaa_resolve(cmd_buffer,
-                             src_image, src_aux_usage,
+                             src_image, ISL_FORMAT_UNSUPPORTED, src_aux_usage,
                              region->srcSubresource.mipLevel,
                              region->srcSubresource.baseArrayLayer,
-                             dst_image, dst_aux_usage,
+                             dst_image, ISL_FORMAT_UNSUPPORTED, dst_aux_usage,
                              region->dstSubresource.mipLevel,
                              region->dstSubresource.baseArrayLayer,
                              (1 << aspect_bit),
diff --git a/src/intel/vulkan/anv_cmd_buffer.c b/src/intel/vulkan/anv_cmd_buffer.c
index 2ef884e5382..c6420ff4c8a 100644
--- a/src/intel/vulkan/anv_cmd_buffer.c
+++ b/src/intel/vulkan/anv_cmd_buffer.c
@@ -821,7 +821,7 @@ anv_cmd_buffer_bind_descriptor_set(struct anv_cmd_buffer *cmd_buffer,
          assert((offset & ~ANV_DESCRIPTOR_SET_OFFSET_MASK) == 0);
          push->desc_surface_offsets[set_index] &= ~ANV_DESCRIPTOR_SET_OFFSET_MASK;
          push->desc_surface_offsets[set_index] |= offset;
-         push->desc_sampler_offsets[set_index] |=
+         push->desc_sampler_offsets[set_index] =
             anv_address_physical(set->desc_sampler_addr) -
             cmd_buffer->device->physical->va.dynamic_state_pool.addr;
 
diff --git a/src/intel/vulkan/anv_device.c b/src/intel/vulkan/anv_device.c
index 56473427dbe..86b5f1ebcc1 100644
--- a/src/intel/vulkan/anv_device.c
+++ b/src/intel/vulkan/anv_device.c
@@ -944,9 +944,10 @@ get_properties_1_1(const struct anv_physical_device *pdevice,
    p->maxMultiviewViewCount      = 16;
    p->maxMultiviewInstanceIndex  = UINT32_MAX / 16;
    /* Our protected implementation is a memory encryption mechanism, it
-    * doesn't page fault.
+    * shouldn't page fault, but it hangs the HW so in terms of user visibility
+    * it's similar to a fault.
     */
-   p->protectedNoFault           = true;
+   p->protectedNoFault           = false;
    /* This value doesn't matter for us today as our per-stage descriptors are
     * the real limit.
     */
@@ -2165,6 +2166,14 @@ anv_physical_device_try_create(struct vk_instance *vk_instance,
       goto fail_fd;
    }
 
+   /* Disable Wa_16013994831 on Gfx12.0 because we found other cases where we
+    * need to always disable preemption :
+    *    - https://gitlab.freedesktop.org/mesa/mesa/-/issues/5963
+    *    - https://gitlab.freedesktop.org/mesa/mesa/-/issues/5662
+    */
+   if (devinfo.verx10 == 120)
+      BITSET_CLEAR(devinfo.workarounds, INTEL_WA_16013994831);
+
    if (!devinfo.has_context_isolation) {
       result = vk_errorf(instance, VK_ERROR_INCOMPATIBLE_DRIVER,
                          "Vulkan requires context isolation for %s", devinfo.name);
@@ -2231,7 +2240,7 @@ anv_physical_device_try_create(struct vk_instance *vk_instance,
       device->flush_astc_ldr_void_extent_denorms =
          device->has_astc_ldr && !device->emu_astc_ldr;
    }
-   device->disable_fcv = intel_device_info_is_mtl(&device->info) ||
+   device->disable_fcv = device->info.verx10 >= 125 ||
                          instance->disable_fcv;
 
    result = anv_physical_device_init_heaps(device, fd);
@@ -3541,7 +3550,7 @@ VkResult anv_CreateDevice(
    if (result != VK_SUCCESS)
       goto fail_trtt;
 
-   struct vk_pipeline_cache_create_info pcc_info = { };
+   struct vk_pipeline_cache_create_info pcc_info = { .weak_ref = true, };
    device->default_pipeline_cache =
       vk_pipeline_cache_create(&device->vk, &pcc_info, NULL);
    if (!device->default_pipeline_cache) {
@@ -3554,9 +3563,12 @@ VkResult anv_CreateDevice(
     * shaders to remain resident while it runs. Therefore, we need a special
     * cache just for BLORP/RT that's forced to always be enabled.
     */
-   pcc_info.force_enable = true;
+   struct vk_pipeline_cache_create_info internal_pcc_info = {
+      .force_enable = true,
+      .weak_ref = false,
+   };
    device->internal_cache =
-      vk_pipeline_cache_create(&device->vk, &pcc_info, NULL);
+      vk_pipeline_cache_create(&device->vk, &internal_pcc_info, NULL);
    if (device->internal_cache == NULL) {
       result = vk_error(device, VK_ERROR_OUT_OF_HOST_MEMORY);
       goto fail_default_pipeline_cache;
@@ -3699,8 +3711,7 @@ VkResult anv_CreateDevice(
    pthread_mutex_destroy(&device->mutex);
  fail_vmas:
    util_vma_heap_finish(&device->vma_trtt);
-   if (!device->physical->indirect_descriptors)
-      util_vma_heap_finish(&device->vma_samplers);
+   util_vma_heap_finish(&device->vma_samplers);
    util_vma_heap_finish(&device->vma_desc);
    util_vma_heap_finish(&device->vma_hi);
    util_vma_heap_finish(&device->vma_lo);
@@ -3815,8 +3826,7 @@ void anv_DestroyDevice(
    anv_bo_cache_finish(&device->bo_cache);
 
    util_vma_heap_finish(&device->vma_trtt);
-   if (!device->physical->indirect_descriptors)
-      util_vma_heap_finish(&device->vma_samplers);
+   util_vma_heap_finish(&device->vma_samplers);
    util_vma_heap_finish(&device->vma_desc);
    util_vma_heap_finish(&device->vma_hi);
    util_vma_heap_finish(&device->vma_lo);
@@ -4057,7 +4067,7 @@ VkResult anv_AllocateMemory(
    if (mem->vk.alloc_flags & VK_MEMORY_ALLOCATE_DEVICE_ADDRESS_BIT)
       alloc_flags |= ANV_BO_ALLOC_CLIENT_VISIBLE_ADDRESS;
 
-   if (mem->vk.alloc_flags & VK_MEMORY_PROPERTY_PROTECTED_BIT)
+   if (mem_type->propertyFlags & VK_MEMORY_PROPERTY_PROTECTED_BIT)
       alloc_flags |= ANV_BO_ALLOC_PROTECTED;
 
    /* For now, always allocated AUX-TT aligned memory, regardless of dedicated
diff --git a/src/intel/vulkan/anv_formats.c b/src/intel/vulkan/anv_formats.c
index 8d652e0c8f8..65c000398b7 100644
--- a/src/intel/vulkan/anv_formats.c
+++ b/src/intel/vulkan/anv_formats.c
@@ -1077,6 +1077,12 @@ anv_format_supports_usage(
        */
    }
 
+   if (usage_flags & VK_IMAGE_USAGE_FRAGMENT_SHADING_RATE_ATTACHMENT_BIT_KHR) {
+      if (!(format_feature_flags & VK_FORMAT_FEATURE_2_FRAGMENT_SHADING_RATE_ATTACHMENT_BIT_KHR)) {
+         return false;
+      }
+   }
+
    return true;
 }
 
@@ -1464,6 +1470,10 @@ anv_get_image_format_properties(
       }
    }
 
+   if ((info->usage & VK_IMAGE_USAGE_FRAGMENT_SHADING_RATE_ATTACHMENT_BIT_KHR) &&
+       !devinfo->has_coarse_pixel_primitive_and_cb)
+      goto unsupported;
+
    /* From the bspec section entitled "Surface Layout and Tiling",
     * Gfx9 has a 256 GB limitation and Gfx11+ has a 16 TB limitation.
     */
diff --git a/src/intel/vulkan/anv_image.c b/src/intel/vulkan/anv_image.c
index 6ead10079b2..dcb4fbcc940 100644
--- a/src/intel/vulkan/anv_image.c
+++ b/src/intel/vulkan/anv_image.c
@@ -1312,11 +1312,6 @@ add_all_surfaces_implicit_layout(
       if (result != VK_SUCCESS)
          return result;
 
-      /* Disable aux if image supports export without modifiers. */
-      if (image->vk.external_handle_types != 0 &&
-          image->vk.tiling != VK_IMAGE_TILING_DRM_FORMAT_MODIFIER_EXT)
-         continue;
-
       result = add_aux_surface_if_supported(device, image, plane, plane_format,
                                             format_list_info,
                                             ANV_OFFSET_IMPLICIT, plane_stride,
@@ -1673,6 +1668,11 @@ anv_image_init(struct anv_device *device, struct anv_image *image,
       isl_extra_usage_flags |= ISL_SURF_USAGE_DISABLE_AUX_BIT;
    }
 
+   /* Disable aux if image supports export without modifiers. */
+   if (image->vk.external_handle_types != 0 &&
+       image->vk.tiling != VK_IMAGE_TILING_DRM_FORMAT_MODIFIER_EXT)
+      isl_extra_usage_flags |= ISL_SURF_USAGE_DISABLE_AUX_BIT;
+
    const isl_tiling_flags_t isl_tiling_flags =
       choose_isl_tiling_flags(device->info, create_info, isl_mod_info,
                               image->vk.wsi_legacy_scanout);
@@ -3607,8 +3607,7 @@ anv_fill_buffer_view_surface_state(struct anv_device *device,
 {
    anv_fill_buffer_surface_state(device,
                                  state->state_data.data,
-                                 format, swizzle,
-                                 ISL_SURF_USAGE_TEXTURE_BIT,
+                                 format, swizzle, usage,
                                  address, range, stride);
 
    if (state->state.map)
diff --git a/src/intel/vulkan/anv_nir_apply_pipeline_layout.c b/src/intel/vulkan/anv_nir_apply_pipeline_layout.c
index e55e4e51391..44e0b7cb247 100644
--- a/src/intel/vulkan/anv_nir_apply_pipeline_layout.c
+++ b/src/intel/vulkan/anv_nir_apply_pipeline_layout.c
@@ -594,8 +594,16 @@ build_res_index(nir_builder *b,
       }
 
    const uint32_t desc_bti = state->set[set].binding[binding].surface_offset;
-   assert(bind_layout->descriptor_surface_stride % 8 == 0);
-   const uint32_t desc_stride = bind_layout->descriptor_surface_stride / 8;
+   /* We don't care about the stride field for inline uniforms (see
+    * build_desc_addr_for_res_index), but for anything else we should be
+    * aligned to 8 bytes because we store a multiple of 8 in the packed info
+    * to be able to encode a stride up to 2040 (8 * 255).
+    */
+   assert(bind_layout->type == VK_DESCRIPTOR_TYPE_INLINE_UNIFORM_BLOCK ||
+          bind_layout->descriptor_surface_stride % 8 == 0);
+   const uint32_t desc_stride =
+      bind_layout->type == VK_DESCRIPTOR_TYPE_INLINE_UNIFORM_BLOCK ? 0 :
+      bind_layout->descriptor_surface_stride / 8;
 
       nir_def *packed =
          nir_ior_imm(b,
@@ -735,7 +743,7 @@ build_desc_addr_for_res_index(nir_builder *b,
 static nir_def *
 build_desc_addr_for_binding(nir_builder *b,
                             unsigned set, unsigned binding,
-                            nir_def *array_index,
+                            nir_def *array_index, unsigned plane,
                             const struct apply_pipeline_layout_state *state)
 {
    const struct anv_descriptor_set_binding_layout *bind_layout =
@@ -751,6 +759,10 @@ build_desc_addr_for_binding(nir_builder *b,
                                    array_index,
                                    bind_layout->descriptor_surface_stride),
                       bind_layout->descriptor_surface_offset);
+      if (plane != 0) {
+         desc_offset = nir_iadd_imm(
+            b, desc_offset, plane * bind_layout->descriptor_data_surface_size);
+      }
 
       return nir_vec4(b, nir_unpack_64_2x32_split_x(b, set_addr),
                          nir_unpack_64_2x32_split_y(b, set_addr),
@@ -758,14 +770,21 @@ build_desc_addr_for_binding(nir_builder *b,
                          desc_offset);
    }
 
-   case nir_address_format_32bit_index_offset:
+   case nir_address_format_32bit_index_offset: {
+      nir_def *desc_offset =
+         nir_iadd_imm(b,
+                      nir_imul_imm(b,
+                                   array_index,
+                                   bind_layout->descriptor_surface_stride),
+                      bind_layout->descriptor_surface_offset);
+      if (plane != 0) {
+         desc_offset = nir_iadd_imm(
+            b, desc_offset, plane * bind_layout->descriptor_data_surface_size);
+      }
       return nir_vec2(b,
                       nir_imm_int(b, state->set[set].desc_offset),
-                      nir_iadd_imm(b,
-                                   nir_imul_imm(b,
-                                                array_index,
-                                                bind_layout->descriptor_surface_stride),
-                                   bind_layout->descriptor_surface_offset));
+                      desc_offset);
+   }
 
    default:
       unreachable("Unhandled address format");
@@ -819,7 +838,8 @@ build_surface_index_for_binding(nir_builder *b,
          set_offset = nir_imm_int(b, 0xdeaddead);
 
          nir_def *desc_addr =
-            build_desc_addr_for_binding(b, set, binding, array_index, state);
+            build_desc_addr_for_binding(b, set, binding, array_index,
+                                        plane, state);
 
          surface_index =
             build_load_descriptor_mem(b, desc_addr, 0, 1, 32, state);
@@ -900,7 +920,8 @@ build_sampler_handle_for_binding(nir_builder *b,
          set_offset = nir_imm_int(b, 0xdeaddead);
 
          nir_def *desc_addr =
-            build_desc_addr_for_binding(b, set, binding, array_index, state);
+            build_desc_addr_for_binding(b, set, binding, array_index,
+                                        plane, state);
 
          /* This is anv_sampled_image_descriptor, the sampler handle is always
           * in component 1.
@@ -1376,7 +1397,8 @@ lower_load_accel_struct_desc(nir_builder *b,
 
    struct res_index_defs res = unpack_res_index(b, res_index);
    nir_def *desc_addr =
-      build_desc_addr_for_binding(b, set, binding, res.array_index, state);
+      build_desc_addr_for_binding(b, set, binding, res.array_index,
+                                  0 /* plane */, state);
 
    /* Acceleration structure descriptors are always uint64_t */
    nir_def *desc = build_load_descriptor_mem(b, desc_addr, 0, 1, 64, state);
@@ -1605,7 +1627,7 @@ lower_image_size_intrinsic(nir_builder *b, nir_intrinsic_instr *intrin,
    }
 
    nir_def *desc_addr = build_desc_addr_for_binding(
-      b, set, binding, array_index, state);
+      b, set, binding, array_index, 0 /* plane */, state);
 
    b->cursor = nir_after_instr(&intrin->instr);
 
@@ -1975,6 +1997,34 @@ add_push_entry(struct anv_pipeline_push_map *push_map,
    };
 }
 
+static bool
+binding_should_use_surface_binding_table(const struct apply_pipeline_layout_state *state,
+                                         const struct anv_descriptor_set_binding_layout *binding)
+{
+   if ((binding->data & ANV_DESCRIPTOR_BTI_SURFACE_STATE) == 0)
+      return false;
+
+   if (state->pdevice->always_use_bindless &&
+       (binding->data & ANV_DESCRIPTOR_SURFACE))
+      return false;
+
+   return true;
+}
+
+static bool
+binding_should_use_sampler_binding_table(const struct apply_pipeline_layout_state *state,
+                                         const struct anv_descriptor_set_binding_layout *binding)
+{
+   if ((binding->data & ANV_DESCRIPTOR_BTI_SAMPLER_STATE) == 0)
+      return false;
+
+   if (state->pdevice->always_use_bindless &&
+       (binding->data & ANV_DESCRIPTOR_SAMPLER))
+      return false;
+
+   return true;
+}
+
 void
 anv_nir_apply_pipeline_layout(nir_shader *shader,
                               const struct anv_physical_device *pdevice,
@@ -2146,7 +2196,7 @@ anv_nir_apply_pipeline_layout(nir_shader *shader,
       state.set[set].binding[b].surface_offset = BINDLESS_OFFSET;
       state.set[set].binding[b].sampler_offset = BINDLESS_OFFSET;
 
-      if (binding->data & ANV_DESCRIPTOR_BTI_SURFACE_STATE) {
+      if (binding_should_use_surface_binding_table(&state, binding)) {
          if (map->surface_count + array_size * array_multiplier > MAX_BINDING_TABLE_SIZE ||
              anv_descriptor_requires_bindless(pdevice, binding, false) ||
              brw_shader_stage_requires_bindless_resources(shader->info.stage)) {
@@ -2177,7 +2227,7 @@ anv_nir_apply_pipeline_layout(nir_shader *shader,
          assert(map->surface_count <= MAX_BINDING_TABLE_SIZE);
       }
 
-      if (binding->data & ANV_DESCRIPTOR_BTI_SAMPLER_STATE) {
+      if (binding_should_use_sampler_binding_table(&state, binding)) {
          if (map->sampler_count + array_size * array_multiplier > MAX_SAMPLER_TABLE_SIZE ||
              anv_descriptor_requires_bindless(pdevice, binding, true) ||
              brw_shader_stage_requires_bindless_resources(shader->info.stage)) {
diff --git a/src/intel/vulkan/anv_nir_push_descriptor_analysis.c b/src/intel/vulkan/anv_nir_push_descriptor_analysis.c
index 2e1c75dbd75..62eb6088381 100644
--- a/src/intel/vulkan/anv_nir_push_descriptor_analysis.c
+++ b/src/intel/vulkan/anv_nir_push_descriptor_analysis.c
@@ -126,18 +126,17 @@ anv_nir_loads_push_desc_buffer(nir_shader *nir,
             if (intrin->intrinsic != nir_intrinsic_load_ubo)
                continue;
 
-            const nir_const_value *const_bt_idx =
-               nir_src_as_const_value(intrin->src[0]);
-            if (const_bt_idx == NULL)
+            const unsigned bt_idx =
+               brw_nir_ubo_surface_index_get_bti(intrin->src[0]);
+            if (bt_idx == UINT32_MAX)
                continue;
 
-            const unsigned bt_idx = const_bt_idx[0].u32;
-
             const struct anv_pipeline_binding *binding =
                &bind_map->surface_to_descriptor[bt_idx];
             if (binding->set == ANV_DESCRIPTOR_SET_DESCRIPTORS &&
-                binding->index == push_set)
+                binding->index == push_set) {
                return true;
+            }
          }
       }
    }
@@ -162,6 +161,7 @@ anv_nir_push_desc_ubo_fully_promoted(nir_shader *nir,
    if (push_set_layout == NULL)
       return 0;
 
+   /* Assume every UBO can be promoted first. */
    uint32_t ubos_fully_promoted = 0;
    for (uint32_t b = 0; b < push_set_layout->binding_count; b++) {
       const struct anv_descriptor_set_binding_layout *bind_layout =
@@ -174,6 +174,10 @@ anv_nir_push_desc_ubo_fully_promoted(nir_shader *nir,
          ubos_fully_promoted |= BITFIELD_BIT(bind_layout->descriptor_index);
    }
 
+   /* For each load_ubo intrinsic, if the descriptor index or the offset is
+    * not a constant, we could not promote to push constant. Then check the
+    * offset + size against the push ranges.
+    */
    nir_foreach_function_impl(impl, nir) {
       nir_foreach_block(block, impl) {
          nir_foreach_instr(instr, block) {
@@ -184,45 +188,65 @@ anv_nir_push_desc_ubo_fully_promoted(nir_shader *nir,
             if (intrin->intrinsic != nir_intrinsic_load_ubo)
                continue;
 
-            if (!brw_nir_ubo_surface_index_is_pushable(intrin->src[0]))
+            /* Don't check the load_ubo from descriptor buffers */
+            nir_intrinsic_instr *resource =
+               intrin->src[0].ssa->parent_instr->type == nir_instr_type_intrinsic ?
+               nir_instr_as_intrinsic(intrin->src[0].ssa->parent_instr) : NULL;
+            if (resource == NULL || resource->intrinsic != nir_intrinsic_resource_intel)
                continue;
 
-            const unsigned bt_idx =
-               brw_nir_ubo_surface_index_get_bti(intrin->src[0]);
+            /* Skip load_ubo not loading from the push descriptor */
+            if (nir_intrinsic_desc_set(resource) != push_set)
+               continue;
 
-            /* Skip if this isn't a load from push descriptor buffer. */
-            const struct anv_pipeline_binding *binding =
-               &bind_map->surface_to_descriptor[bt_idx];
-            if (binding->set != push_set)
+            uint32_t binding = nir_intrinsic_binding(resource);
+
+            /* If we have indirect indexing in the binding, no push promotion
+             * in possible for the entire binding.
+             */
+            if (!nir_src_is_const(resource->src[1])) {
+               for (uint32_t i = 0; i < push_set_layout->binding[binding].array_size; i++) {
+                  ubos_fully_promoted &=
+                     ~BITFIELD_BIT(push_set_layout->binding[binding].descriptor_index + i);
+               }
                continue;
+            }
 
-            const uint32_t desc_idx =
-               push_set_layout->binding[binding->binding].descriptor_index;
-            assert(desc_idx < MAX_PUSH_DESCRIPTORS);
+            const nir_const_value *const_bt_id =
+               nir_src_as_const_value(resource->src[1]);
+            uint32_t bt_id = const_bt_id[0].u32;
 
-            bool promoted = false;
+            const struct anv_pipeline_binding *pipe_bind =
+               &bind_map->surface_to_descriptor[bt_id];
+
+            const uint32_t desc_idx =
+               push_set_layout->binding[binding].descriptor_index;
 
             /* If the offset in the entry is dynamic, we can't tell if
              * promoted or not.
              */
             const nir_const_value *const_load_offset =
                nir_src_as_const_value(intrin->src[1]);
-            if (const_load_offset != NULL) {
-               /* Check if the load was promoted to a push constant. */
-               const unsigned load_offset = const_load_offset[0].u32;
-               const int load_bytes = nir_intrinsic_dest_components(intrin) *
-                  (intrin->def.bit_size / 8);
-
-               for (unsigned i = 0; i < ARRAY_SIZE(bind_map->push_ranges); i++) {
-                  if (bind_map->push_ranges[i].set == binding->set &&
-                      bind_map->push_ranges[i].index == desc_idx &&
-                      bind_map->push_ranges[i].start * 32 <= load_offset &&
-                      (bind_map->push_ranges[i].start +
-                       bind_map->push_ranges[i].length) * 32 >=
-                      (load_offset + load_bytes)) {
-                     promoted = true;
-                     break;
-                  }
+            if (const_load_offset == NULL) {
+               ubos_fully_promoted &= ~BITFIELD_BIT(desc_idx);
+               continue;
+            }
+
+            /* Check if the load was promoted to a push constant. */
+            const unsigned load_offset = const_load_offset[0].u32;
+            const int load_bytes = nir_intrinsic_dest_components(intrin) *
+               (intrin->def.bit_size / 8);
+
+            bool promoted = false;
+            for (unsigned i = 0; i < ARRAY_SIZE(bind_map->push_ranges); i++) {
+               if (bind_map->push_ranges[i].set == pipe_bind->set &&
+                   bind_map->push_ranges[i].index == desc_idx &&
+                   bind_map->push_ranges[i].start * 32 <= load_offset &&
+                   (bind_map->push_ranges[i].start +
+                    bind_map->push_ranges[i].length) * 32 >=
+                   (load_offset + load_bytes)) {
+                  promoted = true;
+                  break;
                }
             }
 
diff --git a/src/intel/vulkan/anv_pipeline.c b/src/intel/vulkan/anv_pipeline.c
index fe08cf44b71..4fcbe1e1604 100644
--- a/src/intel/vulkan/anv_pipeline.c
+++ b/src/intel/vulkan/anv_pipeline.c
@@ -839,7 +839,7 @@ anv_pipeline_hash_ray_tracing_combined_shader(struct anv_ray_tracing_pipeline *p
    _mesa_sha1_final(&ctx, sha1_out);
 }
 
-static nir_shader *
+static VkResult
 anv_pipeline_stage_get_nir(struct anv_pipeline *pipeline,
                            struct vk_pipeline_cache *cache,
                            void *mem_ctx,
@@ -849,25 +849,35 @@ anv_pipeline_stage_get_nir(struct anv_pipeline *pipeline,
       pipeline->device->physical->compiler;
    const nir_shader_compiler_options *nir_options =
       compiler->nir_options[stage->stage];
-   nir_shader *nir;
 
-   nir = anv_device_search_for_nir(pipeline->device, cache,
-                                   nir_options,
-                                   stage->shader_sha1,
-                                   mem_ctx);
-   if (nir) {
-      assert(nir->info.stage == stage->stage);
-      return nir;
+   stage->nir = anv_device_search_for_nir(pipeline->device, cache,
+                                          nir_options,
+                                          stage->shader_sha1,
+                                          mem_ctx);
+   if (stage->nir) {
+      assert(stage->nir->info.stage == stage->stage);
+      return VK_SUCCESS;
    }
 
-   nir = anv_shader_stage_to_nir(pipeline->device, stage->info,
-                                 stage->key.base.robust_flags, mem_ctx);
-   if (nir) {
-      anv_device_upload_nir(pipeline->device, cache, nir, stage->shader_sha1);
-      return nir;
+   /* VkPipelineShaderStageCreateInfo:
+    *
+    *    "If a pipeline is not found, pipeline compilation is not possible and
+    *     the implementation must fail as specified by
+    *     VK_PIPELINE_CREATE_FAIL_ON_PIPELINE_COMPILE_REQUIRED_BIT."
+    */
+   if (vk_pipeline_shader_stage_has_identifier(stage->info))
+      return VK_PIPELINE_COMPILE_REQUIRED;
+
+   stage->nir = anv_shader_stage_to_nir(pipeline->device, stage->info,
+                                        stage->key.base.robust_flags, mem_ctx);
+   if (stage->nir) {
+      anv_device_upload_nir(pipeline->device, cache,
+                            stage->nir, stage->shader_sha1);
+      return VK_SUCCESS;
    }
 
-   return NULL;
+   return vk_errorf(&pipeline->device->vk, VK_ERROR_UNKNOWN,
+                    "Unable to load NIR");
 }
 
 static const struct vk_ycbcr_conversion_state *
@@ -1728,7 +1738,7 @@ anv_pipeline_account_shader(struct anv_pipeline *pipeline,
 
    if (shader->push_desc_info.used_set_buffer) {
       pipeline->use_push_descriptor_buffer |=
-         BITFIELD_BIT(mesa_to_vk_shader_stage(shader->stage));
+         mesa_to_vk_shader_stage(shader->stage);
    }
    if (shader->push_desc_info.used_descriptors &
        ~shader->push_desc_info.fully_promoted_ubo_descriptors)
@@ -2014,10 +2024,10 @@ anv_graphics_pipeline_load_nir(struct anv_graphics_base_pipeline *pipeline,
        * an imported library for the same stage.
        */
       if (stages[s].imported.bin == NULL) {
-         stages[s].nir = anv_pipeline_stage_get_nir(&pipeline->base, cache,
-                                                    mem_ctx, &stages[s]);
-         if (stages[s].nir == NULL)
-            return vk_error(pipeline, VK_ERROR_UNKNOWN);
+         VkResult result = anv_pipeline_stage_get_nir(&pipeline->base, cache,
+                                                      mem_ctx, &stages[s]);
+         if (result != VK_SUCCESS)
+            return result;
       } else {
          stages[s].nir = need_clone ?
                          nir_shader_clone(mem_ctx, stages[s].imported.nir) :
@@ -2220,9 +2230,9 @@ anv_graphics_pipeline_compile(struct anv_graphics_base_pipeline *pipeline,
                vk_perf(VK_LOG_OBJS(cache ? &cache->base :
                                    &pipeline->base.device->vk.base),
                        "Found all ISA shaders in the cache but not all NIR shaders.");
+            } else {
+               anv_graphics_lib_retain_shaders(pipeline, stages, false /* will_compile */);
             }
-
-            anv_graphics_lib_retain_shaders(pipeline, stages, false /* will_compile */);
          }
 
          if (result == VK_SUCCESS)
@@ -2621,10 +2631,11 @@ anv_pipeline_compile_cs(struct anv_compute_pipeline *pipeline,
          .binding = UINT32_MAX,
       };
 
-      stage.nir = anv_pipeline_stage_get_nir(&pipeline->base, cache, mem_ctx, &stage);
-      if (stage.nir == NULL) {
+      VkResult result = anv_pipeline_stage_get_nir(&pipeline->base, cache,
+                                                   mem_ctx, &stage);
+      if (result != VK_SUCCESS) {
          ralloc_free(mem_ctx);
-         return vk_error(pipeline, VK_ERROR_UNKNOWN);
+         return result;
       }
 
       anv_pipeline_nir_preprocess(&pipeline->base, &stage);
@@ -2781,20 +2792,12 @@ VkResult anv_CreateComputePipelines(
                                                  &pCreateInfos[i],
                                                  pAllocator, &pPipelines[i]);
 
-      if (res == VK_SUCCESS)
-         continue;
-
-      /* Bail out on the first error != VK_PIPELINE_COMPILE_REQUIRED as it
-       * is not obvious what error should be report upon 2 different failures.
-       * */
-      result = res;
-      if (res != VK_PIPELINE_COMPILE_REQUIRED)
-         break;
-
-      pPipelines[i] = VK_NULL_HANDLE;
-
-      if (flags & VK_PIPELINE_CREATE_2_EARLY_RETURN_ON_FAILURE_BIT_KHR)
-         break;
+      if (res != VK_SUCCESS) {
+         result = res;
+         if (flags & VK_PIPELINE_CREATE_2_EARLY_RETURN_ON_FAILURE_BIT_KHR)
+            break;
+         pPipelines[i] = VK_NULL_HANDLE;
+      }
    }
 
    for (; i < count; i++)
@@ -3317,20 +3320,12 @@ VkResult anv_CreateGraphicsPipelines(
                                             pAllocator, &pPipelines[i]);
       }
 
-      if (res == VK_SUCCESS)
-         continue;
-
-      /* Bail out on the first error != VK_PIPELINE_COMPILE_REQUIRED as it
-       * is not obvious what error should be report upon 2 different failures.
-       * */
-      result = res;
-      if (res != VK_PIPELINE_COMPILE_REQUIRED)
-         break;
-
-      pPipelines[i] = VK_NULL_HANDLE;
-
-      if (flags & VK_PIPELINE_CREATE_2_EARLY_RETURN_ON_FAILURE_BIT_KHR)
-         break;
+      if (res != VK_SUCCESS) {
+         result = res;
+         if (flags & VK_PIPELINE_CREATE_2_EARLY_RETURN_ON_FAILURE_BIT_KHR)
+            break;
+         pPipelines[i] = VK_NULL_HANDLE;
+      }
    }
 
    for (; i < count; i++)
@@ -3649,10 +3644,11 @@ anv_pipeline_compile_ray_tracing(struct anv_ray_tracing_pipeline *pipeline,
 
       int64_t stage_start = os_time_get_nano();
 
-      stages[i].nir = anv_pipeline_stage_get_nir(&pipeline->base, cache,
-                                                 tmp_pipeline_ctx, &stages[i]);
-      if (stages[i].nir == NULL)
-         return vk_error(pipeline, VK_ERROR_OUT_OF_HOST_MEMORY);
+      VkResult result = anv_pipeline_stage_get_nir(&pipeline->base, cache,
+                                                   tmp_pipeline_ctx,
+                                                   &stages[i]);
+      if (result != VK_SUCCESS)
+         return result;
 
       anv_pipeline_nir_preprocess(&pipeline->base, &stages[i]);
 
@@ -4154,19 +4150,12 @@ anv_CreateRayTracingPipelinesKHR(
                                                      &pCreateInfos[i],
                                                      pAllocator, &pPipelines[i]);
 
-      if (res == VK_SUCCESS)
-         continue;
-
-      /* Bail out on the first error as it is not obvious what error should be
-       * report upon 2 different failures. */
-      result = res;
-      if (result != VK_PIPELINE_COMPILE_REQUIRED)
-         break;
-
-      pPipelines[i] = VK_NULL_HANDLE;
-
-      if (flags & VK_PIPELINE_CREATE_2_EARLY_RETURN_ON_FAILURE_BIT_KHR)
-         break;
+      if (res != VK_SUCCESS) {
+         result = res;
+         if (flags & VK_PIPELINE_CREATE_2_EARLY_RETURN_ON_FAILURE_BIT_KHR)
+            break;
+         pPipelines[i] = VK_NULL_HANDLE;
+      }
    }
 
    for (; i < createInfoCount; i++)
@@ -4252,7 +4241,7 @@ VkResult anv_GetPipelineExecutableStatisticsKHR(
    switch (pipeline->type) {
    case ANV_PIPELINE_GRAPHICS:
    case ANV_PIPELINE_GRAPHICS_LIB: {
-      prog_data = anv_pipeline_to_graphics(pipeline)->base.shaders[exe->stage]->prog_data;
+      prog_data = anv_pipeline_to_graphics_base(pipeline)->shaders[exe->stage]->prog_data;
       break;
    }
    case ANV_PIPELINE_COMPUTE: {
diff --git a/src/intel/vulkan/anv_private.h b/src/intel/vulkan/anv_private.h
index 74442f212c4..251919c2efd 100644
--- a/src/intel/vulkan/anv_private.h
+++ b/src/intel/vulkan/anv_private.h
@@ -1667,6 +1667,10 @@ struct anv_device {
     struct anv_bo_pool                          batch_bo_pool;
     /** Memory pool for utrace timestamp buffers */
     struct anv_bo_pool                          utrace_bo_pool;
+    /**
+     * Size of the timestamp captured for utrace.
+     */
+    uint32_t                                     utrace_timestamp_size;
     /** Memory pool for BVH build buffers */
     struct anv_bo_pool                          bvh_bo_pool;
 
@@ -1856,13 +1860,13 @@ struct anv_device {
        struct list_head in_flight_batches;
     } trtt;
 
-    /* This is true if the user ever bound a sparse resource to memory. This
-     * is used for a workaround that makes every memoryBarrier flush more
-     * things than it should. Many applications request for the sparse
-     * featuers to be enabled but don't use them, and some create sparse
-     * resources but never use them.
+    /* Number of sparse resources that currently exist. This is used for a
+     * workaround that makes every memoryBarrier flush more things than it
+     * should. Some workloads create and then immediately destroy sparse
+     * resources when they start, so just counting if a sparse resource was
+     * ever created is not enough.
      */
-    bool                                         using_sparse;
+    uint32_t num_sparse_resources;
 
     struct anv_device_astc_emu                   astc_emu;
 };
@@ -3180,6 +3184,12 @@ struct anv_push_constants {
    /** Dynamic offsets for dynamic UBOs and SSBOs */
    uint32_t dynamic_offsets[MAX_DYNAMIC_BUFFERS];
 
+   /* Robust access pushed registers. */
+   uint64_t push_reg_mask[MESA_SHADER_STAGES];
+
+   /** Ray query globals (RT_DISPATCH_GLOBALS) */
+   uint64_t ray_query_globals;
+
    union {
       struct {
          /** Dynamic MSAA value */
@@ -3200,16 +3210,12 @@ struct anv_push_constants {
           *
           * This is never set by software but is implicitly filled out when
           * uploading the push constants for compute shaders.
+          *
+          * This *MUST* be the last field of the anv_push_constants structure.
           */
          uint32_t subgroup_id;
       } cs;
    };
-
-   /* Robust access pushed registers. */
-   uint64_t push_reg_mask[MESA_SHADER_STAGES];
-
-   /** Ray query globals (RT_DISPATCH_GLOBALS) */
-   uint64_t ray_query_globals;
 };
 
 struct anv_surface_state {
@@ -4372,11 +4378,19 @@ struct anv_ray_tracing_pipeline {
    }
 
 ANV_DECL_PIPELINE_DOWNCAST(graphics, ANV_PIPELINE_GRAPHICS)
-ANV_DECL_PIPELINE_DOWNCAST(graphics_base, ANV_PIPELINE_GRAPHICS)
 ANV_DECL_PIPELINE_DOWNCAST(graphics_lib, ANV_PIPELINE_GRAPHICS_LIB)
 ANV_DECL_PIPELINE_DOWNCAST(compute, ANV_PIPELINE_COMPUTE)
 ANV_DECL_PIPELINE_DOWNCAST(ray_tracing, ANV_PIPELINE_RAY_TRACING)
 
+/* Can't use the macro because we need to handle both types. */
+static inline struct anv_graphics_base_pipeline *
+anv_pipeline_to_graphics_base(struct anv_pipeline *pipeline)
+{
+   assert(pipeline->type == ANV_PIPELINE_GRAPHICS ||
+          pipeline->type == ANV_PIPELINE_GRAPHICS_LIB);
+   return (struct anv_graphics_base_pipeline *) pipeline;
+}
+
 static inline bool
 anv_pipeline_has_stage(const struct anv_graphics_pipeline *pipeline,
                        gl_shader_stage stage)
@@ -5142,19 +5156,10 @@ anv_image_clear_depth_stencil(struct anv_cmd_buffer *cmd_buffer,
                               VkRect2D area,
                               float depth_value, uint8_t stencil_value);
 void
-anv_image_msaa_resolve(struct anv_cmd_buffer *cmd_buffer,
-                       const struct anv_image *src_image,
-                       enum isl_aux_usage src_aux_usage,
-                       uint32_t src_level, uint32_t src_base_layer,
-                       const struct anv_image *dst_image,
-                       enum isl_aux_usage dst_aux_usage,
-                       uint32_t dst_level, uint32_t dst_base_layer,
-                       VkImageAspectFlagBits aspect,
-                       uint32_t src_x, uint32_t src_y,
-                       uint32_t dst_x, uint32_t dst_y,
-                       uint32_t width, uint32_t height,
-                       uint32_t layer_count,
-                       enum blorp_filter filter);
+anv_attachment_msaa_resolve(struct anv_cmd_buffer *cmd_buffer,
+                            const struct anv_attachment *att,
+                            VkImageLayout layout,
+                            VkImageAspectFlagBits aspect);
 void
 anv_image_hiz_op(struct anv_cmd_buffer *cmd_buffer,
                  const struct anv_image *image,
diff --git a/src/intel/vulkan/anv_sparse.c b/src/intel/vulkan/anv_sparse.c
index 180cf7e2c5d..1357b8e5824 100644
--- a/src/intel/vulkan/anv_sparse.c
+++ b/src/intel/vulkan/anv_sparse.c
@@ -707,6 +707,7 @@ anv_init_sparse_bindings(struct anv_device *device,
       return res;
    }
 
+   p_atomic_inc(&device->num_sparse_resources);
    return VK_SUCCESS;
 }
 
@@ -720,6 +721,8 @@ anv_free_sparse_bindings(struct anv_device *device,
    sparse_debug("%s: address:0x%016"PRIx64" size:0x%08"PRIx64"\n",
                 __func__, sparse->address, sparse->size);
 
+   p_atomic_dec(&device->num_sparse_resources);
+
    struct anv_vm_bind unbind = {
       .bo = 0,
       .address = sparse->address,
diff --git a/src/intel/vulkan/anv_util.c b/src/intel/vulkan/anv_util.c
index 9a87e91b39d..fabadf5d1ae 100644
--- a/src/intel/vulkan/anv_util.c
+++ b/src/intel/vulkan/anv_util.c
@@ -89,7 +89,7 @@ anv_dump_pipe_bits(enum anv_pipe_bits bits, FILE *f)
        bits & ANV_PIPE_END_OF_PIPE_SYNC_BIT)
       fputs("+cs_stall ", f);
    if (bits & ANV_PIPE_UNTYPED_DATAPORT_CACHE_FLUSH_BIT)
-      fputs("+utdp_flush", f);
+      fputs("+utdp_flush ", f);
    if (bits & ANV_PIPE_CCS_CACHE_FLUSH_BIT)
       fputs("+ccs_flush ", f);
 }
diff --git a/src/intel/vulkan/anv_utrace.c b/src/intel/vulkan/anv_utrace.c
index 56d445958a8..b5b20182a12 100644
--- a/src/intel/vulkan/anv_utrace.c
+++ b/src/intel/vulkan/anv_utrace.c
@@ -49,7 +49,7 @@ union anv_utrace_timestamp {
     *        [2] = 32b Context Timestamp End
     *        [3] = 32b Global Timestamp End"
     */
-   uint32_t compute_walker[4];
+   uint32_t compute_walker[8];
 };
 
 static uint32_t
@@ -499,6 +499,8 @@ anv_device_utrace_init(struct anv_device *device)
                                  intel_engines_class_to_string(queue->family->engine_class),
                                  queue->vk.index_in_family);
    }
+
+   device->utrace_timestamp_size = sizeof(union anv_utrace_timestamp);
 }
 
 void
diff --git a/src/intel/vulkan/genX_blorp_exec.c b/src/intel/vulkan/genX_blorp_exec.c
index 1cdd16cdf1a..c099a8dedfe 100644
--- a/src/intel/vulkan/genX_blorp_exec.c
+++ b/src/intel/vulkan/genX_blorp_exec.c
@@ -417,7 +417,9 @@ blorp_exec_on_compute(struct blorp_batch *batch,
 
    blorp_exec(batch, params);
 
+   cmd_buffer->state.descriptors_dirty |= VK_SHADER_STAGE_COMPUTE_BIT;
    cmd_buffer->state.push_constants_dirty |= VK_SHADER_STAGE_COMPUTE_BIT;
+   cmd_buffer->state.compute.pipeline_dirty = true;
 }
 
 static void
@@ -427,7 +429,7 @@ blorp_exec_on_blitter(struct blorp_batch *batch,
    assert(batch->flags & BLORP_BATCH_USE_BLITTER);
 
    struct anv_cmd_buffer *cmd_buffer = batch->driver_batch;
-   assert(cmd_buffer->queue_family->queueFlags == VK_QUEUE_TRANSFER_BIT);
+   assert(anv_cmd_buffer_is_blitter_queue(cmd_buffer));
 
    blorp_exec(batch, params);
 }
diff --git a/src/intel/vulkan/genX_cmd_buffer.c b/src/intel/vulkan/genX_cmd_buffer.c
index eb3be7de5ad..e799b6df39b 100644
--- a/src/intel/vulkan/genX_cmd_buffer.c
+++ b/src/intel/vulkan/genX_cmd_buffer.c
@@ -2928,6 +2928,16 @@ genX(batch_emit_pipe_control_write)(struct anv_batch *batch,
       };
    }
 
+   /* SKL PRMs, Volume 7: 3D-Media-GPGPU, Programming Restrictions for
+    * PIPE_CONTROL, Flush Types:
+    *   "Requires stall bit ([20] of DW) set for all GPGPU Workloads."
+    * For newer platforms this is documented in the PIPE_CONTROL instruction
+    * page.
+    */
+   if (current_pipeline == GPGPU &&
+       (bits & ANV_PIPE_TEXTURE_CACHE_INVALIDATE_BIT))
+      bits |= ANV_PIPE_CS_STALL_BIT;
+
 #if INTEL_NEEDS_WA_1409600907
    /* Wa_1409600907: "PIPE_CONTROL with Depth Stall Enable bit must
     * be set with any PIPE_CONTROL with Depth Flush Enable bit set.
@@ -3333,6 +3343,10 @@ anv_use_generated_draws(const struct anv_cmd_buffer *cmd_buffer, uint32_t count)
    const struct anv_graphics_pipeline *pipeline =
       anv_pipeline_to_graphics(cmd_buffer->state.gfx.base.pipeline);
 
+   /* We cannot generate readable commands in protected mode. */
+   if (cmd_buffer->vk.pool->flags & VK_COMMAND_POOL_CREATE_PROTECTED_BIT)
+      return false;
+
    /* Limit generated draws to pipelines without HS stage. This makes things
     * simpler for implementing Wa_1306463417, Wa_16011107343.
     */
@@ -3343,6 +3357,33 @@ anv_use_generated_draws(const struct anv_cmd_buffer *cmd_buffer, uint32_t count)
    return count >= device->physical->instance->generated_indirect_threshold;
 }
 
+static void
+genX(cmd_buffer_set_protected_memory)(struct anv_cmd_buffer *cmd_buffer,
+                                      bool enabled)
+{
+#if GFX_VER >= 12
+   if (enabled) {
+      anv_batch_emit(&cmd_buffer->batch, GENX(MI_SET_APPID), appid) {
+         /* Default value for single session. */
+         appid.ProtectedMemoryApplicationID = cmd_buffer->device->protected_session_id;
+         appid.ProtectedMemoryApplicationIDType = DISPLAY_APP;
+      }
+   }
+   anv_batch_emit(&cmd_buffer->batch, GENX(PIPE_CONTROL), pc) {
+      pc.PipeControlFlushEnable = true;
+      pc.DCFlushEnable = true;
+      pc.RenderTargetCacheFlushEnable = true;
+      pc.CommandStreamerStallEnable = true;
+      if (enabled)
+         pc.ProtectedMemoryEnable = true;
+      else
+         pc.ProtectedMemoryDisable = true;
+   }
+#else
+   unreachable("Protected content not supported");
+#endif
+}
+
 VkResult
 genX(BeginCommandBuffer)(
     VkCommandBuffer                             commandBuffer,
@@ -3417,19 +3458,8 @@ genX(BeginCommandBuffer)(
 
 #if GFX_VER >= 12
    if (cmd_buffer->vk.level == VK_COMMAND_BUFFER_LEVEL_PRIMARY &&
-       cmd_buffer->vk.pool->flags & VK_COMMAND_POOL_CREATE_PROTECTED_BIT) {
-      anv_batch_emit(&cmd_buffer->batch, GENX(MI_SET_APPID), appid) {
-         /* Default value for single session. */
-         appid.ProtectedMemoryApplicationID = cmd_buffer->device->protected_session_id;
-         appid.ProtectedMemoryApplicationIDType = DISPLAY_APP;
-      }
-      anv_batch_emit(&cmd_buffer->batch, GENX(PIPE_CONTROL), pc) {
-         pc.CommandStreamerStallEnable = true;
-         pc.DCFlushEnable = true;
-         pc.RenderTargetCacheFlushEnable = true;
-         pc.ProtectedMemoryEnable = true;
-      }
-   }
+       cmd_buffer->vk.pool->flags & VK_COMMAND_POOL_CREATE_PROTECTED_BIT)
+      genX(cmd_buffer_set_protected_memory)(cmd_buffer, true);
 #endif
 
    genX(cmd_buffer_emit_state_base_address)(cmd_buffer);
@@ -3643,14 +3673,8 @@ end_command_buffer(struct anv_cmd_buffer *cmd_buffer)
 
 #if GFX_VER >= 12
    if (cmd_buffer->vk.level == VK_COMMAND_BUFFER_LEVEL_PRIMARY &&
-       cmd_buffer->vk.pool->flags & VK_COMMAND_POOL_CREATE_PROTECTED_BIT) {
-      anv_batch_emit(&cmd_buffer->batch, GENX(PIPE_CONTROL), pc) {
-         pc.CommandStreamerStallEnable = true;
-         pc.DCFlushEnable = true;
-         pc.RenderTargetCacheFlushEnable = true;
-         pc.ProtectedMemoryDisable = true;
-      }
-   }
+       cmd_buffer->vk.pool->flags & VK_COMMAND_POOL_CREATE_PROTECTED_BIT)
+      genX(cmd_buffer_set_protected_memory)(cmd_buffer, false);
 #endif
 
    trace_intel_end_cmd_buffer(&cmd_buffer->trace, cmd_buffer->vk.level);
@@ -3689,14 +3713,16 @@ cmd_buffer_emit_copy_ts_buffer(struct u_trace_context *utctx,
                                void *ts_to, uint32_t to_offset,
                                uint32_t count)
 {
+   struct anv_device *device =
+      container_of(utctx, struct anv_device, ds.trace_context);
    struct anv_memcpy_state *memcpy_state = cmdstream;
    struct anv_address from_addr = (struct anv_address) {
-      .bo = ts_from, .offset = from_offset * sizeof(uint64_t) };
+      .bo = ts_from, .offset = from_offset * device->utrace_timestamp_size };
    struct anv_address to_addr = (struct anv_address) {
-      .bo = ts_to, .offset = to_offset * sizeof(uint64_t) };
+      .bo = ts_to, .offset = to_offset * device->utrace_timestamp_size };
 
    genX(emit_so_memcpy)(memcpy_state, to_addr, from_addr,
-                        count * sizeof(uint64_t));
+                        count * device->utrace_timestamp_size);
 }
 
 void
@@ -4072,6 +4098,7 @@ anv_pipe_invalidate_bits_for_access_flags(struct anv_cmd_buffer *cmd_buffer,
           * tile cache flush to make sure any previous write is not going to
           * create WaW hazards.
           */
+         pipe_bits |= ANV_PIPE_DATA_CACHE_FLUSH_BIT;
          pipe_bits |= ANV_PIPE_TILE_CACHE_FLUSH_BIT;
          break;
       case VK_ACCESS_2_SHADER_STORAGE_READ_BIT:
@@ -4393,7 +4420,8 @@ cmd_buffer_barrier(struct anv_cmd_buffer *cmd_buffer,
       /* There's no way of knowing if this memory barrier is related to sparse
        * buffers! This is pretty horrible.
        */
-      if (device->using_sparse && mask_is_write(src_flags))
+      if (mask_is_write(src_flags) &&
+          p_atomic_read(&device->num_sparse_resources) > 0)
          apply_sparse_flushes = true;
    }
 
@@ -5240,7 +5268,8 @@ emit_indirect_draws(struct anv_cmd_buffer *cmd_buffer,
    UNUSED const struct intel_device_info *devinfo = cmd_buffer->device->info;
    UNUSED const bool aligned_stride =
       (indirect_data_stride == 0 ||
-       indirect_data_stride == sizeof(VkDrawIndirectCommand));
+       (!indexed && indirect_data_stride == sizeof(VkDrawIndirectCommand)) ||
+       (indexed && indirect_data_stride == sizeof(VkDrawIndexedIndirectCommand)));
    UNUSED const bool execute_indirect_supported =
       execute_indirect_draw_supported(cmd_buffer);
 
@@ -5289,7 +5318,7 @@ emit_indirect_draws(struct anv_cmd_buffer *cmd_buffer,
 #if GFX_VERx10 >= 125
          genX(emit_breakpoint)(&cmd_buffer->batch, cmd_buffer->device, true);
          anv_batch_emit(&cmd_buffer->batch, GENX(EXECUTE_INDIRECT_DRAW), ind) {
-            ind.ArgumentFormat             = DRAW;
+            ind.ArgumentFormat             = indexed ? DRAWINDEXED : DRAW;
             ind.TBIMREnabled               = cmd_buffer->state.gfx.dyn_state.use_tbimr;
             ind.PredicateEnable            =
                cmd_buffer->state.conditional_render_enabled;
@@ -8249,86 +8278,6 @@ cmd_buffer_mark_attachment_written(struct anv_cmd_buffer *cmd_buffer,
    }
 }
 
-static enum blorp_filter
-vk_to_blorp_resolve_mode(VkResolveModeFlagBits vk_mode)
-{
-   switch (vk_mode) {
-   case VK_RESOLVE_MODE_SAMPLE_ZERO_BIT:
-      return BLORP_FILTER_SAMPLE_0;
-   case VK_RESOLVE_MODE_AVERAGE_BIT:
-      return BLORP_FILTER_AVERAGE;
-   case VK_RESOLVE_MODE_MIN_BIT:
-      return BLORP_FILTER_MIN_SAMPLE;
-   case VK_RESOLVE_MODE_MAX_BIT:
-      return BLORP_FILTER_MAX_SAMPLE;
-   default:
-      return BLORP_FILTER_NONE;
-   }
-}
-
-static void
-cmd_buffer_resolve_msaa_attachment(struct anv_cmd_buffer *cmd_buffer,
-                                   const struct anv_attachment *att,
-                                   VkImageLayout layout,
-                                   VkImageAspectFlagBits aspect)
-{
-   struct anv_cmd_graphics_state *gfx = &cmd_buffer->state.gfx;
-   const struct anv_image_view *src_iview = att->iview;
-   const struct anv_image_view *dst_iview = att->resolve_iview;
-
-   enum isl_aux_usage src_aux_usage =
-      anv_layout_to_aux_usage(cmd_buffer->device->info,
-                              src_iview->image, aspect,
-                              VK_IMAGE_USAGE_TRANSFER_SRC_BIT,
-                              layout,
-                              cmd_buffer->queue_family->queueFlags);
-
-   enum isl_aux_usage dst_aux_usage =
-      anv_layout_to_aux_usage(cmd_buffer->device->info,
-                              dst_iview->image, aspect,
-                              VK_IMAGE_USAGE_TRANSFER_DST_BIT,
-                              att->resolve_layout,
-                              cmd_buffer->queue_family->queueFlags);
-
-   enum blorp_filter filter = vk_to_blorp_resolve_mode(att->resolve_mode);
-
-   const VkRect2D render_area = gfx->render_area;
-   if (gfx->view_mask == 0) {
-      anv_image_msaa_resolve(cmd_buffer,
-                             src_iview->image, src_aux_usage,
-                             src_iview->planes[0].isl.base_level,
-                             src_iview->planes[0].isl.base_array_layer,
-                             dst_iview->image, dst_aux_usage,
-                             dst_iview->planes[0].isl.base_level,
-                             dst_iview->planes[0].isl.base_array_layer,
-                             aspect,
-                             render_area.offset.x, render_area.offset.y,
-                             render_area.offset.x, render_area.offset.y,
-                             render_area.extent.width,
-                             render_area.extent.height,
-                             gfx->layer_count, filter);
-   } else {
-      uint32_t res_view_mask = gfx->view_mask;
-      while (res_view_mask) {
-         int i = u_bit_scan(&res_view_mask);
-
-         anv_image_msaa_resolve(cmd_buffer,
-                                src_iview->image, src_aux_usage,
-                                src_iview->planes[0].isl.base_level,
-                                src_iview->planes[0].isl.base_array_layer + i,
-                                dst_iview->image, dst_aux_usage,
-                                dst_iview->planes[0].isl.base_level,
-                                dst_iview->planes[0].isl.base_array_layer + i,
-                                aspect,
-                                render_area.offset.x, render_area.offset.y,
-                                render_area.offset.x, render_area.offset.y,
-                                render_area.extent.width,
-                                render_area.extent.height,
-                                1, filter);
-      }
-   }
-}
-
 void genX(CmdEndRendering)(
     VkCommandBuffer                             commandBuffer)
 {
@@ -8370,8 +8319,9 @@ void genX(CmdEndRendering)(
                                 "MSAA resolve");
    }
 
-   if (gfx->depth_att.resolve_mode != VK_RESOLVE_MODE_NONE ||
-       gfx->stencil_att.resolve_mode != VK_RESOLVE_MODE_NONE) {
+   if (!(gfx->rendering_flags & VK_RENDERING_SUSPENDING_BIT) &&
+       (gfx->depth_att.resolve_mode != VK_RESOLVE_MODE_NONE ||
+        gfx->stencil_att.resolve_mode != VK_RESOLVE_MODE_NONE)) {
       /* We are about to do some MSAA resolves.  We need to flush so that the
        * result of writes to the MSAA depth attachments show up in the sampler
        * when we blit to the single-sampled resolve target.
@@ -8388,8 +8338,8 @@ void genX(CmdEndRendering)(
           (gfx->rendering_flags & VK_RENDERING_SUSPENDING_BIT))
          continue;
 
-      cmd_buffer_resolve_msaa_attachment(cmd_buffer, att, att->layout,
-                                         VK_IMAGE_ASPECT_COLOR_BIT);
+      anv_attachment_msaa_resolve(cmd_buffer, att, att->layout,
+                                  VK_IMAGE_ASPECT_COLOR_BIT);
    }
 
    if (gfx->depth_att.resolve_mode != VK_RESOLVE_MODE_NONE &&
@@ -8407,9 +8357,9 @@ void genX(CmdEndRendering)(
                               VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL,
                               false /* will_full_fast_clear */);
 
-      cmd_buffer_resolve_msaa_attachment(cmd_buffer, &gfx->depth_att,
-                                         VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL,
-                                         VK_IMAGE_ASPECT_DEPTH_BIT);
+      anv_attachment_msaa_resolve(cmd_buffer, &gfx->depth_att,
+                                  VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL,
+                                  VK_IMAGE_ASPECT_DEPTH_BIT);
 
       /* Transition the source back to the original layout.  This seems a bit
        * inefficient but, since HiZ resolves aren't destructive, going from
@@ -8425,9 +8375,9 @@ void genX(CmdEndRendering)(
 
    if (gfx->stencil_att.resolve_mode != VK_RESOLVE_MODE_NONE &&
        !(gfx->rendering_flags & VK_RENDERING_SUSPENDING_BIT)) {
-      cmd_buffer_resolve_msaa_attachment(cmd_buffer, &gfx->stencil_att,
-                                         gfx->stencil_att.layout,
-                                         VK_IMAGE_ASPECT_STENCIL_BIT);
+      anv_attachment_msaa_resolve(cmd_buffer, &gfx->stencil_att,
+                                  gfx->stencil_att.layout,
+                                  VK_IMAGE_ASPECT_STENCIL_BIT);
    }
 
 
diff --git a/src/intel/vulkan/genX_gfx_state.c b/src/intel/vulkan/genX_gfx_state.c
index 66af91f8f62..aebf85b2c24 100644
--- a/src/intel/vulkan/genX_gfx_state.c
+++ b/src/intel/vulkan/genX_gfx_state.c
@@ -68,7 +68,7 @@ static const uint32_t genX(vk_to_intel_blend_op)[] = {
 static void
 genX(streamout_prologue)(struct anv_cmd_buffer *cmd_buffer)
 {
-#if GFX_VERx10 >= 120
+#if INTEL_WA_16013994831_GFX_VER
    /* Wa_16013994831 - Disable preemption during streamout, enable back
     * again if XFB not used by the current pipeline.
     *
@@ -292,6 +292,15 @@ genX(rasterization_mode)(VkPolygonMode raster_mode,
    }
 }
 
+static bool
+is_src1_blend_factor(enum GENX(3D_Color_Buffer_Blend_Factor) factor)
+{
+   return factor == BLENDFACTOR_SRC1_COLOR ||
+          factor == BLENDFACTOR_SRC1_ALPHA ||
+          factor == BLENDFACTOR_INV_SRC1_COLOR ||
+          factor == BLENDFACTOR_INV_SRC1_ALPHA;
+}
+
 #if GFX_VERx10 == 125
 /**
  * Return the dimensions of the current rendering area, defined as the
@@ -998,6 +1007,16 @@ genX(cmd_buffer_flush_gfx_runtime_state)(struct anv_cmd_buffer *cmd_buffer)
                dyn->cb.attachments[i].dst_alpha_blend_factor];
          }
 
+         /* Replace and Src1 value by 1.0 if dual source blending is not
+          * enabled.
+          */
+         if (wm_prog_data && !wm_prog_data->dual_src_blend) {
+            if (is_src1_blend_factor(SourceBlendFactor))
+               SourceBlendFactor = BLENDFACTOR_ONE;
+            if (is_src1_blend_factor(DestinationBlendFactor))
+               DestinationBlendFactor = BLENDFACTOR_ONE;
+         }
+
          if (instance->intel_enable_wa_14018912822 &&
              intel_needs_workaround(cmd_buffer->device->info, 14018912822) &&
              pipeline->rasterization_samples > 1) {
@@ -1028,7 +1047,7 @@ genX(cmd_buffer_flush_gfx_runtime_state)(struct anv_cmd_buffer *cmd_buffer)
       SET(PS_BLEND, ps_blend.ColorBufferBlendEnable, GET(blend.rts[0].ColorBufferBlendEnable));
       SET(PS_BLEND, ps_blend.SourceAlphaBlendFactor, GET(blend.rts[0].SourceAlphaBlendFactor));
       SET(PS_BLEND, ps_blend.DestinationAlphaBlendFactor, gfx->alpha_blend_zero ?
-                                                          BLENDFACTOR_CONST_COLOR :
+                                                          BLENDFACTOR_CONST_ALPHA :
                                                           GET(blend.rts[0].DestinationAlphaBlendFactor));
       SET(PS_BLEND, ps_blend.SourceBlendFactor, GET(blend.rts[0].SourceBlendFactor));
       SET(PS_BLEND, ps_blend.DestinationBlendFactor, gfx->color_blend_zero ?
@@ -1189,12 +1208,12 @@ genX(cmd_buffer_flush_gfx_runtime_state)(struct anv_cmd_buffer *cmd_buffer)
        * number of viewport programmed previously was larger than what we need
        * now, no need to reemit we can just keep the old programmed values.
        */
-      if (BITSET_SET(hw_state->dirty, ANV_GFX_STATE_VIEWPORT_SF_CLIP) ||
+      if (BITSET_TEST(hw_state->dirty, ANV_GFX_STATE_VIEWPORT_SF_CLIP) ||
           hw_state->vp_sf_clip.count < dyn->vp.viewport_count) {
          hw_state->vp_sf_clip.count = dyn->vp.viewport_count;
          BITSET_SET(hw_state->dirty, ANV_GFX_STATE_VIEWPORT_SF_CLIP);
       }
-      if (BITSET_SET(hw_state->dirty, ANV_GFX_STATE_VIEWPORT_CC) ||
+      if (BITSET_TEST(hw_state->dirty, ANV_GFX_STATE_VIEWPORT_CC) ||
           hw_state->vp_cc.count < dyn->vp.viewport_count) {
          hw_state->vp_cc.count = dyn->vp.viewport_count;
          BITSET_SET(hw_state->dirty, ANV_GFX_STATE_VIEWPORT_CC);
@@ -1260,7 +1279,7 @@ genX(cmd_buffer_flush_gfx_runtime_state)(struct anv_cmd_buffer *cmd_buffer)
        * number of viewport programmed previously was larger than what we need
        * now, no need to reemit we can just keep the old programmed values.
        */
-      if (BITSET_SET(hw_state->dirty, ANV_GFX_STATE_SCISSOR) ||
+      if (BITSET_TEST(hw_state->dirty, ANV_GFX_STATE_SCISSOR) ||
           hw_state->scissor.count < dyn->vp.scissor_count) {
          hw_state->scissor.count = dyn->vp.scissor_count;
          BITSET_SET(hw_state->dirty, ANV_GFX_STATE_SCISSOR);
diff --git a/src/intel/vulkan/genX_gpu_memcpy.c b/src/intel/vulkan/genX_gpu_memcpy.c
index 70b0851850f..92ed6f13ac2 100644
--- a/src/intel/vulkan/genX_gpu_memcpy.c
+++ b/src/intel/vulkan/genX_gpu_memcpy.c
@@ -80,7 +80,9 @@ emit_common_so_memcpy(struct anv_batch *batch, struct anv_device *device,
       anv_batch_emit(batch, GENX(3DSTATE_MESH_CONTROL), mesh);
       anv_batch_emit(batch, GENX(3DSTATE_TASK_CONTROL), task);
    }
+#endif
 
+#if INTEL_WA_16013994831_GFX_VER
    /* Wa_16013994831 - Disable preemption during streamout. */
    if (intel_needs_workaround(device->info, 16013994831))
       genX(batch_set_preemption)(batch, device->info, _3D, false);
diff --git a/src/intel/vulkan/genX_init_state.c b/src/intel/vulkan/genX_init_state.c
index da994697c7e..df8aaa44bce 100644
--- a/src/intel/vulkan/genX_init_state.c
+++ b/src/intel/vulkan/genX_init_state.c
@@ -158,12 +158,15 @@ genX(emit_slice_hashing_state)(struct anv_device *device,
    }
 
    /* TODO: Figure out FCV support for other platforms
-    * Testing indicates that FCV is broken on MTL, but works fine on DG2.
-    * Let's disable FCV on MTL for now till we figure out what's wrong.
+    * Testing indicates that FCV is broken gfx125.
+    * Let's disable FCV for now till we figure out what's wrong.
     *
     * Alternatively, it can be toggled off via drirc option 'anv_disable_fcv'.
     *
     * Ref: https://gitlab.freedesktop.org/mesa/mesa/-/issues/9987
+    * Ref: https://gitlab.freedesktop.org/mesa/mesa/-/issues/10318
+    * Ref: https://gitlab.freedesktop.org/mesa/mesa/-/issues/10795
+    * Ref: Internal issue 1480 about Unreal Engine 5.1
     */
    anv_batch_emit(batch, GENX(3DSTATE_3D_MODE), mode) {
       mode.SliceHashingTableEnable = true;
diff --git a/src/intel/vulkan/genX_query.c b/src/intel/vulkan/genX_query.c
index e4f10721d28..b6bc348b5e2 100644
--- a/src/intel/vulkan/genX_query.c
+++ b/src/intel/vulkan/genX_query.c
@@ -554,7 +554,8 @@ VkResult genX(GetQueryPoolResults)(
          while (statistics) {
             UNUSED uint32_t stat = u_bit_scan(&statistics);
             if (write_results) {
-               uint64_t result = slot[idx * 2 + 2] - slot[idx * 2 + 1];
+               /* If a query is not available but VK_QUERY_RESULT_PARTIAL_BIT is set, write 0. */
+               uint64_t result = available ? slot[idx * 2 + 2] - slot[idx * 2 + 1] : 0;
                cpu_write_query_result(pData, flags, idx, result);
             }
             idx++;
@@ -565,11 +566,17 @@ VkResult genX(GetQueryPoolResults)(
 
       case VK_QUERY_TYPE_TRANSFORM_FEEDBACK_STREAM_EXT: {
          uint64_t *slot = query_slot(pool, firstQuery + i);
-         if (write_results)
-            cpu_write_query_result(pData, flags, idx, slot[2] - slot[1]);
+         if (write_results) {
+            /* If a query is not available but VK_QUERY_RESULT_PARTIAL_BIT is set, write 0. */
+            uint64_t result = available ? slot[2] - slot[1] : 0;
+            cpu_write_query_result(pData, flags, idx, result);
+         }
          idx++;
-         if (write_results)
-            cpu_write_query_result(pData, flags, idx, slot[4] - slot[3]);
+         if (write_results) {
+            /* If a query is not available but VK_QUERY_RESULT_PARTIAL_BIT is set, write 0. */
+            uint64_t result = available ? slot[4] - slot[3] : 0;
+            cpu_write_query_result(pData, flags, idx, result);
+         }
          idx++;
          break;
       }
diff --git a/src/intel/vulkan/genX_simple_shader.c b/src/intel/vulkan/genX_simple_shader.c
index 9adbf8cac36..2fc1a548d9b 100644
--- a/src/intel/vulkan/genX_simple_shader.c
+++ b/src/intel/vulkan/genX_simple_shader.c
@@ -545,6 +545,7 @@ genX(emit_simple_shader_dispatch)(struct anv_simple_shader *state,
 #if GFX_VERx10 >= 125
       anv_batch_emit(batch, GENX(COMPUTE_WALKER), cw) {
          cw.SIMDSize                       = dispatch.simd_size / 16;
+         cw.MessageSIMD                    = dispatch.simd_size / 16,
          cw.IndirectDataStartAddress       = push_state.offset;
          cw.IndirectDataLength             = push_state.alloc_size;
          cw.LocalXMaximum                  = prog_data->local_size[0] - 1;
diff --git a/src/intel/vulkan/genX_video.c b/src/intel/vulkan/genX_video.c
index 8eacbf12eef..5156e93a601 100644
--- a/src/intel/vulkan/genX_video.c
+++ b/src/intel/vulkan/genX_video.c
@@ -63,10 +63,46 @@ genX(CmdEndVideoCodingKHR)(VkCommandBuffer commandBuffer,
    cmd_buffer->video.params = NULL;
 }
 
+/*
+ * The default scan order of scaling lists is up-right-diagonal
+ * according to the spec. But the device requires raster order,
+ * so we need to convert from the passed scaling lists.
+ */
+static void
+anv_h265_matrix_from_uprightdiagonal(StdVideoH265ScalingLists *out_sl,
+                                     const StdVideoH265ScalingLists *sl)
+{
+  uint8_t i, j;
+
+  for (i = 0; i < 6; i++) {
+     for (j = 0; j < STD_VIDEO_H265_SCALING_LIST_4X4_NUM_ELEMENTS; j++)
+        out_sl->ScalingList4x4[i][vl_zscan_h265_up_right_diagonal_16[j]] =
+           sl->ScalingList4x4[i][j];
+
+     for (j = 0; j < STD_VIDEO_H265_SCALING_LIST_8X8_NUM_ELEMENTS; j++)
+        out_sl->ScalingList8x8[i][vl_zscan_h265_up_right_diagonal[j]] =
+           sl->ScalingList8x8[i][j];
+
+     for (j = 0; j < STD_VIDEO_H265_SCALING_LIST_16X16_NUM_ELEMENTS; j++)
+        out_sl->ScalingList16x16[i][vl_zscan_h265_up_right_diagonal[j]] =
+           sl->ScalingList16x16[i][j];
+  }
+
+  for (i = 0; i < STD_VIDEO_H265_SCALING_LIST_32X32_NUM_LISTS; i++) {
+     for (j = 0; j < STD_VIDEO_H265_SCALING_LIST_32X32_NUM_ELEMENTS; j++)
+        out_sl->ScalingList32x32[i][vl_zscan_h265_up_right_diagonal[j]] =
+           sl->ScalingList32x32[i][j];
+  }
+}
+
 static void
 scaling_list(struct anv_cmd_buffer *cmd_buffer,
              const StdVideoH265ScalingLists *scaling_list)
 {
+   StdVideoH265ScalingLists out_sl = {0, };
+
+   anv_h265_matrix_from_uprightdiagonal(&out_sl, scaling_list);
+
    /* 4x4, 8x8, 16x16, 32x32 */
    for (uint8_t size = 0; size < 4; size++) {
       /* Intra, Inter */
@@ -89,22 +125,22 @@ scaling_list(struct anv_cmd_buffer *cmd_buffer,
                   for (uint8_t i = 0; i < 4; i++)
                      for (uint8_t j = 0; j < 4; j++)
                         qm.QuantizerMatrix8x8[4 * i + j] =
-                           scaling_list->ScalingList4x4[3 * pred + color][4 * i + j];
+                           out_sl.ScalingList4x4[3 * pred + color][4 * i + j];
                } else if (size == 1) {
                   for (uint8_t i = 0; i < 8; i++)
                      for (uint8_t j = 0; j < 8; j++)
                         qm.QuantizerMatrix8x8[8 * i + j] =
-                           scaling_list->ScalingList8x8[3 * pred + color][8 * i + j];
+                           out_sl.ScalingList8x8[3 * pred + color][8 * i + j];
                } else if (size == 2) {
                   for (uint8_t i = 0; i < 8; i++)
                      for (uint8_t j = 0; j < 8; j++)
                         qm.QuantizerMatrix8x8[8 * i + j] =
-                           scaling_list->ScalingList16x16[3 * pred + color][8 * i + j];
+                           out_sl.ScalingList16x16[3 * pred + color][8 * i + j];
                } else if (size == 3) {
                   for (uint8_t i = 0; i < 8; i++)
                      for (uint8_t j = 0; j < 8; j++)
                         qm.QuantizerMatrix8x8[8 * i + j] =
-                           scaling_list->ScalingList32x32[pred][8 * i + j];
+                           out_sl.ScalingList32x32[pred][8 * i + j];
                }
             }
          }
diff --git a/src/intel/vulkan/grl/meson.build b/src/intel/vulkan/grl/meson.build
index 1bf4c3b4e9f..d611a871b30 100644
--- a/src/intel/vulkan/grl/meson.build
+++ b/src/intel/vulkan/grl/meson.build
@@ -201,6 +201,6 @@ libgrl = static_library(
 idep_grl = declare_dependency(
   link_with : libgrl,
   dependencies : libgrl_deps,
-  sources : grl_metakernel_h,
+  sources : [grl_metakernel_h, grl_cl_kernel_h],
   include_directories : include_directories('include', 'gpu'),
 )
diff --git a/src/intel/vulkan/i915/anv_queue.c b/src/intel/vulkan/i915/anv_queue.c
index f2b8f848b8d..173cf7b2a3a 100644
--- a/src/intel/vulkan/i915/anv_queue.c
+++ b/src/intel/vulkan/i915/anv_queue.c
@@ -58,8 +58,13 @@ anv_i915_create_engine(struct anv_device *device,
    } else if (device->physical->has_vm_control) {
       assert(pCreateInfo->queueFamilyIndex < physical->queue.family_count);
       enum intel_engine_class engine_classes[1];
+      enum intel_gem_create_context_flags flags = 0;
+
       engine_classes[0] = queue_family->engine_class;
-      if (!intel_gem_create_context_engines(device->fd, 0 /* flags */,
+      if (pCreateInfo->flags & VK_DEVICE_QUEUE_CREATE_PROTECTED_BIT)
+         flags |= INTEL_GEM_CREATE_CONTEXT_EXT_PROTECTED_FLAG;
+
+      if (!intel_gem_create_context_engines(device->fd, flags,
                                             physical->engine_info,
                                             1, engine_classes,
                                             device->vm_id,
@@ -74,7 +79,7 @@ anv_i915_create_engine(struct anv_device *device,
           queue_family->engine_class == INTEL_ENGINE_CLASS_COMPUTE) {
          uint32_t *context_id = (uint32_t *)&queue->companion_rcs_id;
          engine_classes[0] = INTEL_ENGINE_CLASS_RENDER;
-         if (!intel_gem_create_context_engines(device->fd, 0 /* flags */,
+         if (!intel_gem_create_context_engines(device->fd, flags,
                                                physical->engine_info,
                                                1, engine_classes,
                                                device->vm_id,
diff --git a/src/intel/vulkan/layers/anv_android_layer.c b/src/intel/vulkan/layers/anv_android_layer.c
index e36eb820ab6..4a7d0dd7170 100644
--- a/src/intel/vulkan/layers/anv_android_layer.c
+++ b/src/intel/vulkan/layers/anv_android_layer.c
@@ -38,7 +38,8 @@ android_CreateImageView(VkDevice _device,
     * format.
     */
    if (fmt && fmt->layout == UTIL_FORMAT_LAYOUT_ASTC &&
-       device->info->verx10 >= 125) {
+       device->info->verx10 >= 125 &&
+       !(device->physical->has_astc_ldr || device->physical->emu_astc_ldr)) {
       return vk_errorf(device, VK_ERROR_OUT_OF_HOST_MEMORY,
                        "ASTC format not supported (%s).", __func__);
    }
diff --git a/src/intel/vulkan/xe/anv_batch_chain.c b/src/intel/vulkan/xe/anv_batch_chain.c
index 22ccaa4763a..c0d931c4c02 100644
--- a/src/intel/vulkan/xe/anv_batch_chain.c
+++ b/src/intel/vulkan/xe/anv_batch_chain.c
@@ -114,9 +114,15 @@ xe_exec_process_syncs(struct anv_queue *queue,
                       struct drm_xe_sync **ret, uint32_t *ret_count)
 {
    struct anv_device *device = queue->device;
-   uint32_t num_syncs = wait_count + signal_count + extra_sync_count +
-                        (utrace_submit ? 1 : 0) +
-                        ((queue->sync && !is_companion_rcs_queue) ? 1 : 0);
+   /* Signal the utrace sync only if it doesn't have a batch. Otherwise the
+    * it's the utrace batch that should signal its own sync.
+    */
+   const bool has_utrace_sync = utrace_submit &&
+                                util_dynarray_num_elements(&utrace_submit->batch_bos, struct anv_bo *) == 0;
+   const uint32_t num_syncs = wait_count + signal_count + extra_sync_count +
+                              (has_utrace_sync ? 1 : 0) +
+                              ((queue->sync && !is_companion_rcs_queue) ? 1 : 0);
+
    if (!num_syncs)
       return VK_SUCCESS;
 
@@ -128,12 +134,7 @@ xe_exec_process_syncs(struct anv_queue *queue,
 
    uint32_t count = 0;
 
-   /* Signal the utrace sync only if it doesn't have a batch. Otherwise the
-    * it's the utrace batch that should signal its own sync.
-    */
-   if (utrace_submit &&
-       util_dynarray_num_elements(&utrace_submit->batch_bos,
-                                  struct anv_bo *) == 0) {
+   if (has_utrace_sync) {
       struct drm_xe_sync *xe_sync = &xe_syncs[count++];
 
       xe_exec_fill_sync(xe_sync, utrace_submit->sync, 0, TYPE_SIGNAL);
@@ -191,7 +192,7 @@ xe_execute_trtt_batch(struct anv_sparse_submission *submit,
    struct anv_queue *queue = submit->queue;
    struct anv_device *device = queue->device;
    struct anv_trtt *trtt = &device->trtt;
-   VkResult result;
+   VkResult result = VK_SUCCESS;
 
    struct drm_xe_sync extra_sync = {
       .type = DRM_XE_SYNC_TYPE_TIMELINE_SYNCOBJ,
@@ -220,18 +221,22 @@ xe_execute_trtt_batch(struct anv_sparse_submission *submit,
    };
 
    if (!device->info->no_hw) {
-      if (intel_ioctl(device->fd, DRM_IOCTL_XE_EXEC, &exec))
-         return vk_device_set_lost(&device->vk, "XE_EXEC failed: %m");
+      if (intel_ioctl(device->fd, DRM_IOCTL_XE_EXEC, &exec)) {
+         result = vk_device_set_lost(&device->vk, "XE_EXEC failed: %m");
+         goto out;
+      }
    }
 
    if (queue->sync) {
       result = vk_sync_wait(&device->vk, queue->sync, 0,
                             VK_SYNC_WAIT_COMPLETE, UINT64_MAX);
       if (result != VK_SUCCESS)
-         return vk_queue_set_lost(&queue->vk, "trtt sync wait failed");
+         result = vk_queue_set_lost(&queue->vk, "trtt sync wait failed");
    }
 
-   return VK_SUCCESS;
+out:
+   vk_free(&device->vk.alloc, xe_syncs);
+   return result;
 }
 
 VkResult
diff --git a/src/intel/vulkan_hasvk/anv_image.c b/src/intel/vulkan_hasvk/anv_image.c
index 7112eca4624..29a01f1a40f 100644
--- a/src/intel/vulkan_hasvk/anv_image.c
+++ b/src/intel/vulkan_hasvk/anv_image.c
@@ -1031,11 +1031,6 @@ add_all_surfaces_implicit_layout(
             return result;
       }
 
-      /* Disable aux if image supports export without modifiers. */
-      if (image->vk.external_handle_types != 0 &&
-          image->vk.tiling != VK_IMAGE_TILING_DRM_FORMAT_MODIFIER_EXT)
-         continue;
-
       result = add_aux_surface_if_supported(device, image, plane, plane_format,
                                             format_list_info,
                                             ANV_OFFSET_IMPLICIT, stride,
@@ -1214,7 +1209,7 @@ alloc_private_binding(struct anv_device *device,
 
 VkResult
 anv_image_init(struct anv_device *device, struct anv_image *image,
-               const struct anv_image_create_info *create_info)
+               struct anv_image_create_info *create_info)
 {
    const VkImageCreateInfo *pCreateInfo = create_info->vk_info;
    const struct VkImageDrmFormatModifierExplicitCreateInfoEXT *mod_explicit_info = NULL;
@@ -1276,6 +1271,11 @@ anv_image_init(struct anv_device *device, struct anv_image *image,
    image->disjoint = image->n_planes > 1 &&
                      (pCreateInfo->flags & VK_IMAGE_CREATE_DISJOINT_BIT);
 
+   /* Disable aux if image supports export without modifiers. */
+   if (image->vk.external_handle_types != 0 &&
+       image->vk.tiling != VK_IMAGE_TILING_DRM_FORMAT_MODIFIER_EXT)
+      create_info->isl_extra_usage_flags |= ISL_SURF_USAGE_DISABLE_AUX_BIT;
+
    const isl_tiling_flags_t isl_tiling_flags =
       choose_isl_tiling_flags(device->info, create_info, isl_mod_info,
                               image->vk.wsi_legacy_scanout);
diff --git a/src/intel/vulkan_hasvk/anv_private.h b/src/intel/vulkan_hasvk/anv_private.h
index c163c58abc3..dff8955fd42 100644
--- a/src/intel/vulkan_hasvk/anv_private.h
+++ b/src/intel/vulkan_hasvk/anv_private.h
@@ -3639,7 +3639,7 @@ struct anv_image_create_info {
 };
 
 VkResult anv_image_init(struct anv_device *device, struct anv_image *image,
-                        const struct anv_image_create_info *create_info);
+                        struct anv_image_create_info *create_info);
 
 void anv_image_finish(struct anv_image *image);
 
diff --git a/src/mesa/main/dlist.c b/src/mesa/main/dlist.c
index 63feef2b097..8c32ed80d79 100644
--- a/src/mesa/main/dlist.c
+++ b/src/mesa/main/dlist.c
@@ -1220,7 +1220,7 @@ dlist_alloc(struct gl_context *ctx, OpCode opcode, GLuint bytes, bool align8)
       ctx->ListState.CurrentPos++;
    }
 
-   if (ctx->ListState.CurrentPos + numNodes + contNodes > BLOCK_SIZE) {
+   if (ctx->ListState.CurrentPos + numNodes + contNodes >= BLOCK_SIZE) {
       /* This block is full.  Allocate a new block and chain to it */
       Node *newblock;
       Node *n = ctx->ListState.CurrentBlock + ctx->ListState.CurrentPos;
diff --git a/src/mesa/main/fbobject.c b/src/mesa/main/fbobject.c
index 1c94a5e94ac..63aefb1c45e 100644
--- a/src/mesa/main/fbobject.c
+++ b/src/mesa/main/fbobject.c
@@ -2664,6 +2664,16 @@ _mesa_base_fbo_format(const struct gl_context *ctx, GLenum internalFormat)
    case GL_RGB565:
       return _mesa_is_gles(ctx) || ctx->Extensions.ARB_ES2_compatibility
          ? GL_RGB : 0;
+
+   case GL_BGRA:
+      /* EXT_texture_format_BGRA8888 only adds this as color-renderable for
+       * GLES 2 and later
+       */
+      if (_mesa_has_EXT_texture_format_BGRA8888(ctx) && _mesa_is_gles2(ctx))
+         return GL_RGBA;
+      else
+         return 0;
+
    default:
       return 0;
    }
diff --git a/src/mesa/main/formatquery.c b/src/mesa/main/formatquery.c
index ae59413d345..396fa0a4014 100644
--- a/src/mesa/main/formatquery.c
+++ b/src/mesa/main/formatquery.c
@@ -1112,6 +1112,12 @@ _mesa_GetInternalformativ(GLenum target, GLenum internalformat, GLenum pname,
       if (get_pname == 0)
          goto end;
 
+      /* if the resource is unsupported, zero is returned */
+      if (!st_QueryTextureFormatSupport(ctx, target, internalformat)) {
+         buffer[0] = 0;
+         break;
+      }
+
       _mesa_GetIntegerv(get_pname, buffer);
       break;
    }
@@ -1123,6 +1129,12 @@ _mesa_GetInternalformativ(GLenum target, GLenum internalformat, GLenum pname,
       if (!_mesa_is_array_texture(target))
          goto end;
 
+      /* if the resource is unsupported, zero is returned */
+      if (!st_QueryTextureFormatSupport(ctx, target, internalformat)) {
+         buffer[0] = 0;
+         break;
+      }
+
       _mesa_GetIntegerv(GL_MAX_ARRAY_TEXTURE_LAYERS, buffer);
       break;
 
@@ -1137,6 +1149,12 @@ _mesa_GetInternalformativ(GLenum target, GLenum internalformat, GLenum pname,
       unsigned i;
       GLint current_value;
 
+      /* if the resource is unsupported, zero is returned */
+      if (!st_QueryTextureFormatSupport(ctx, target, internalformat)) {
+         buffer[0] = 0;
+         break;
+      }
+
       /* Combining the dimensions. Note that for array targets, this would
        * automatically include the value of MAX_LAYERS, as that value is
        * returned as MAX_HEIGHT or MAX_DEPTH */
@@ -1515,6 +1533,14 @@ _mesa_GetInternalformativ(GLenum target, GLenum internalformat, GLenum pname,
       if (targetIndex < 0 || targetIndex == TEXTURE_BUFFER_INDEX)
          goto end;
 
+      /* If the resource is not supported for image textures,
+       * or if image textures are not supported, NONE is returned.
+       */
+      if (!st_QueryTextureFormatSupport(ctx, target, internalformat)) {
+         buffer[0] = GL_NONE;
+         break;
+      }
+
       /* From spec: "Equivalent to calling GetTexParameter with <value> set
        * to IMAGE_FORMAT_COMPATIBILITY_TYPE."
        *
diff --git a/src/mesa/main/framebuffer.c b/src/mesa/main/framebuffer.c
index 962cd7c5334..281c3ee2bd5 100644
--- a/src/mesa/main/framebuffer.c
+++ b/src/mesa/main/framebuffer.c
@@ -972,13 +972,36 @@ _mesa_get_color_read_type(struct gl_context *ctx,
       GLenum data_type;
       GLuint comps;
 
-      _mesa_uncompressed_format_to_type_and_comps(format, &data_type, &comps);
-
+      _mesa_uncompressed_format_to_type_and_comps(format, &data_type,
+                                                  &comps);
+      if (_mesa_is_gles(ctx)) {
+         /* GLES allows only a limited set of format/type combinations for
+            reading, namely the ones specified in table 8.2 of the GLES 3.2
+            spec. In particular *_REV types are not allowed.  The
+            EXT_read_format_bgra extension does add some *_REV types, but
+            only in conjunction with BGRA formats, and we return BGRA 
+            from _mesa_get_color_read_format for very few cases. Work
+            around that here.
+            Note that EXT_texture_type_2_10_10_10_REV does add support
+            for that texture type and RGBA, so exclude that from our test.
+         */
+         GLenum data_format = _mesa_get_color_read_format(ctx, fb, caller);
+         if (data_format == GL_RGBA) {
+            switch (data_type) {
+            case GL_UNSIGNED_SHORT_4_4_4_4_REV:
+               data_type = GL_UNSIGNED_SHORT_4_4_4_4;
+               break;
+            case GL_UNSIGNED_SHORT_1_5_5_5_REV:
+               data_type = GL_UNSIGNED_SHORT_5_5_5_1;
+               break;
+            default:
+               break;
+            }
+         }
+      }
       return data_type;
    }
 }
-
-
 /**
  * Returns the read renderbuffer for the specified format.
  */
diff --git a/src/mesa/main/glthread_draw.c b/src/mesa/main/glthread_draw.c
index 4e55d8c25ab..b47e44fdb53 100644
--- a/src/mesa/main/glthread_draw.c
+++ b/src/mesa/main/glthread_draw.c
@@ -790,7 +790,8 @@ draw_elements(GLuint drawid, GLenum mode, GLsizei count, GLenum type,
        ctx->Dispatch.Current == ctx->Dispatch.ContextLost ||
        /* This will just generate GL_INVALID_OPERATION, as it should. */
        ctx->GLThread.inside_begin_end ||
-       ctx->GLThread.ListMode) {
+       ctx->GLThread.ListMode ||
+       mode >= 32 || !((1u << mode) & ctx->SupportedPrimMask)) {
       if (instance_count == 1 && baseinstance == 0 && drawid == 0) {
          int cmd_size = sizeof(struct marshal_cmd_DrawElementsBaseVertex);
          struct marshal_cmd_DrawElementsBaseVertex *cmd =
@@ -1059,7 +1060,8 @@ _mesa_marshal_MultiDrawElementsBaseVertex(GLenum mode, const GLsizei *count,
     */
    if (draw_count > 0 && is_index_type_valid(type) &&
        ctx->Dispatch.Current != ctx->Dispatch.ContextLost &&
-       !ctx->GLThread.inside_begin_end) {
+       !ctx->GLThread.inside_begin_end &&
+       !(mode >= 32 || !((1u << mode) & ctx->SupportedPrimMask))) {
       user_buffer_mask = _mesa_is_desktop_gl_core(ctx) ? 0 : get_user_buffer_mask(ctx);
       has_user_indices = vao->CurrentElementBufferName == 0;
    }
diff --git a/src/mesa/main/shaderapi.c b/src/mesa/main/shaderapi.c
index faf41935cb7..4732b5d897a 100644
--- a/src/mesa/main/shaderapi.c
+++ b/src/mesa/main/shaderapi.c
@@ -2363,6 +2363,10 @@ _mesa_ShaderBinary(GLint n, const GLuint* shaders, GLenum binaryformat,
    GET_CURRENT_CONTEXT(ctx);
    struct gl_shader **sh;
 
+   /* no binary data can be loaded if length==0 */
+   if (!length)
+      binary = NULL;
+
    /* Page 68, section 7.2 'Shader Binaries" of the of the OpenGL ES 3.1, and
     * page 88 of the OpenGL 4.5 specs state:
     *
diff --git a/src/mesa/main/teximage.c b/src/mesa/main/teximage.c
index 023a975919f..8f3f51fc645 100644
--- a/src/mesa/main/teximage.c
+++ b/src/mesa/main/teximage.c
@@ -2472,6 +2472,15 @@ copytexture_error_check( struct gl_context *ctx, GLuint dimensions,
       case GL_RGB10_A2:
          break;
 
+      case GL_RED:
+      case GL_RG:
+         /* GL_EXT_texture_rg adds support for GL_RED and GL_RG as an internal
+          * format
+          */
+         if (_mesa_has_EXT_texture_rg(ctx))
+            break;
+
+      FALLTHROUGH;
       default:
          _mesa_error(ctx, GL_INVALID_ENUM,
                      "glCopyTexImage%dD(internalFormat=%s)", dimensions,
@@ -4561,22 +4570,33 @@ copyteximage(struct gl_context *ctx, GLuint dims, struct gl_texture_object *texO
                return;
          }
       }
-      /* From Page 139 of OpenGL ES 3.0 spec:
-       *    "If internalformat is sized, the internal format of the new texel
-       *    array is internalformat, and this is also the new texel array’s
-       *    effective internal format. If the component sizes of internalformat
-       *    do not exactly match the corresponding component sizes of the source
-       *    buffer’s effective internal format, described below, an
-       *    INVALID_OPERATION error is generated. If internalformat is unsized,
-       *    the internal format of the new texel array is the effective internal
-       *    format of the source buffer, and this is also the new texel array’s
-       *    effective internal format.
-       */
-      else if (formats_differ_in_component_sizes (texFormat, rb->Format)) {
+      else {
+         /* From Page 139 of OpenGL ES 3.0 spec:
+         *    "If internalformat is sized, the internal format of the new texel
+         *    array is internalformat, and this is also the new texel array’s
+         *    effective internal format. If the component sizes of internalformat
+         *    do not exactly match the corresponding component sizes of the source
+         *    buffer’s effective internal format, described below, an
+         *    INVALID_OPERATION error is generated. If internalformat is unsized,
+         *    the internal format of the new texel array is the effective internal
+         *    format of the source buffer, and this is also the new texel array’s
+         *    effective internal format.
+         */
+         enum pipe_format rb_format = st_choose_format(ctx->st, rb->InternalFormat,
+                                                       GL_NONE, GL_NONE,
+                                                       PIPE_TEXTURE_2D, 0, 0, 0,
+                                                       false, false);
+         enum pipe_format new_format = st_choose_format(ctx->st, internalFormat,
+                                                        GL_NONE, GL_NONE,
+                                                        PIPE_TEXTURE_2D, 0, 0, 0,
+                                                        false, false);
+         /* this comparison must be done on the API format, not the driver format */
+         if (formats_differ_in_component_sizes (new_format, rb_format)) {
             _mesa_error(ctx, GL_INVALID_OPERATION,
                         "glCopyTexImage%uD(component size changed in"
                         " internal format)", dims);
             return;
+         }
       }
    }
 
diff --git a/src/mesa/main/vdpau.c b/src/mesa/main/vdpau.c
index b648dfbdac2..a9d1e97fd31 100644
--- a/src/mesa/main/vdpau.c
+++ b/src/mesa/main/vdpau.c
@@ -39,6 +39,7 @@
 #include "glformats.h"
 #include "texobj.h"
 #include "teximage.h"
+#include "textureview.h"
 #include "api_exec_decl.h"
 
 #include "state_tracker/st_cb_texture.h"
@@ -179,7 +180,7 @@ register_surface(struct gl_context *ctx, GLboolean isOutput,
       }
 
       /* This will disallow respecifying the storage. */
-      tex->Immutable = GL_TRUE;
+      _mesa_set_texture_view_state(ctx, tex, target, 1);
       _mesa_unlock_texture(ctx, tex);
 
       _mesa_reference_texobj(&surf->textures[i], tex);
diff --git a/src/mesa/state_tracker/st_cb_copyimage.c b/src/mesa/state_tracker/st_cb_copyimage.c
index 2bf95a66358..f3ab37fcbdc 100644
--- a/src/mesa/state_tracker/st_cb_copyimage.c
+++ b/src/mesa/state_tracker/st_cb_copyimage.c
@@ -282,7 +282,10 @@ blit(struct pipe_context *pipe,
    blit.src.box = *src_box;
    u_box_3d(dstx, dsty, dstz, src_box->width, src_box->height,
             src_box->depth, &blit.dst.box);
-   blit.mask = PIPE_MASK_RGBA;
+   if (util_format_is_depth_or_stencil(dst_format))
+      blit.mask = PIPE_MASK_ZS;
+   else
+      blit.mask = PIPE_MASK_RGBA;
    blit.filter = PIPE_TEX_FILTER_NEAREST;
 
    pipe->blit(pipe, &blit);
diff --git a/src/mesa/state_tracker/st_context.c b/src/mesa/state_tracker/st_context.c
index ad0608c2d6f..2843715e210 100644
--- a/src/mesa/state_tracker/st_context.c
+++ b/src/mesa/state_tracker/st_context.c
@@ -987,17 +987,17 @@ st_destroy_context(struct st_context *st)
 
    st_destroy_program_variants(st);
 
-   st_context_free_zombie_objects(st);
-
-   simple_mtx_destroy(&st->zombie_sampler_views.mutex);
-   simple_mtx_destroy(&st->zombie_shaders.mutex);
-
    /* Do not release debug_output yet because it might be in use by other threads.
     * These threads will be terminated by _mesa_free_context_data and
     * st_destroy_context_priv.
     */
    _mesa_free_context_data(ctx, false);
 
+   st_context_free_zombie_objects(st);
+
+   simple_mtx_destroy(&st->zombie_sampler_views.mutex);
+   simple_mtx_destroy(&st->zombie_shaders.mutex);
+
    /* This will free the st_context too, so 'st' must not be accessed
     * afterwards. */
    st_destroy_context_priv(st, true);
diff --git a/src/mesa/state_tracker/st_format.c b/src/mesa/state_tracker/st_format.c
index 6dcd5db202f..4a92723d303 100644
--- a/src/mesa/state_tracker/st_format.c
+++ b/src/mesa/state_tracker/st_format.c
@@ -1333,6 +1333,21 @@ st_ChooseTextureFormat(struct gl_context *ctx, GLenum target,
       is_renderbuffer = true;
    } else {
       pTarget = gl_target_to_pipe(target);
+      if (internalFormat == format) {
+         if (internalFormat == GL_RGBA) {
+            /* with GL_RGBA, these are effectively aliases to required formats */
+            switch (type) {
+            case GL_UNSIGNED_SHORT_5_5_5_1:
+            case GL_UNSIGNED_SHORT_4_4_4_4:
+            case GL_UNSIGNED_INT_8_8_8_8:
+               is_renderbuffer = true;
+               break;
+            default: break;
+            }
+         } else if (internalFormat == GL_RGB && type == GL_UNSIGNED_SHORT_5_6_5) {
+            is_renderbuffer = true;
+         }
+      }
    }
 
    if (target == GL_TEXTURE_1D || target == GL_TEXTURE_1D_ARRAY) {
@@ -1351,7 +1366,9 @@ st_ChooseTextureFormat(struct gl_context *ctx, GLenum target,
    bindings = PIPE_BIND_SAMPLER_VIEW;
    if (_mesa_is_depth_or_stencil_format(internalFormat))
       bindings |= PIPE_BIND_DEPTH_STENCIL;
-   else if (is_renderbuffer || internalFormat == 3 || internalFormat == 4 ||
+   else if (is_renderbuffer)
+      bindings |= PIPE_BIND_RENDER_TARGET;
+   else if (internalFormat == 3 || internalFormat == 4 ||
             internalFormat == GL_RGB || internalFormat == GL_RGBA ||
             internalFormat == GL_RGBA2 ||
             internalFormat == GL_RGB4 || internalFormat == GL_RGBA4 ||
@@ -1507,6 +1524,49 @@ st_QuerySamplesForFormat(struct gl_context *ctx, GLenum target,
    return num_sample_counts;
 }
 
+/* check whether any texture can be allocated for a given format */
+bool
+st_QueryTextureFormatSupport(struct gl_context *ctx, GLenum target, GLenum internalFormat)
+{
+   struct st_context *st = st_context(ctx);
+
+   /* If an sRGB framebuffer is unsupported, sRGB formats behave like linear
+    * formats.
+    */
+   if (!ctx->Extensions.EXT_sRGB) {
+      internalFormat = _mesa_get_linear_internalformat(internalFormat);
+   }
+
+   /* multisample textures need >= 2 samples */
+   unsigned min_samples = target == GL_TEXTURE_2D_MULTISAMPLE ||
+                          target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY ? 1 : 0;
+   unsigned max_samples = min_samples ? 16 : 1;
+
+   /* compressed textures will be allocated as e.g., RGBA8, so check that instead */
+   enum pipe_format pf = st_choose_format(st, internalFormat, GL_NONE, GL_NONE,
+                                          PIPE_TEXTURE_2D, 0, 0, 0,
+                                          false, false);
+   if (util_format_is_compressed(pf)) {
+      enum pipe_format fmts[2] = {0};
+      pf = st_mesa_format_to_pipe_format(st, st_pipe_format_to_mesa_format(pf));
+      fmts[0] = pf;
+      for (unsigned i = max_samples; i > min_samples; i >>= 1) {
+         if (find_supported_format(st->screen, fmts, PIPE_TEXTURE_2D,
+                                   i, i, PIPE_BIND_SAMPLER_VIEW, false))
+            return true;
+      }
+      return false;
+   }
+   for (unsigned i = max_samples; i > min_samples; i >>= 1) {
+      if (st_choose_format(st, internalFormat, GL_NONE, GL_NONE,
+                           PIPE_TEXTURE_2D, i, i, PIPE_BIND_SAMPLER_VIEW,
+                           false, false))
+         return true;
+   }
+
+   return false;
+}
+
 
 /**
  * ARB_internalformat_query2 driver hook.
diff --git a/src/mesa/state_tracker/st_format.h b/src/mesa/state_tracker/st_format.h
index 9692a0bc583..21b1a7bb4bf 100644
--- a/src/mesa/state_tracker/st_format.h
+++ b/src/mesa/state_tracker/st_format.h
@@ -70,7 +70,8 @@ extern mesa_format
 st_ChooseTextureFormat(struct gl_context * ctx, GLenum target,
                        GLint internalFormat,
                        GLenum format, GLenum type);
-
+bool
+st_QueryTextureFormatSupport(struct gl_context *ctx, GLenum target, GLenum internalFormat);
 void
 st_QueryInternalFormat(struct gl_context *ctx, GLenum target,
                        GLenum internalFormat, GLenum pname, GLint *params);
diff --git a/src/mesa/state_tracker/st_pbo_compute.c b/src/mesa/state_tracker/st_pbo_compute.c
index e1bb5fee321..b114a546d6a 100644
--- a/src/mesa/state_tracker/st_pbo_compute.c
+++ b/src/mesa/state_tracker/st_pbo_compute.c
@@ -844,7 +844,7 @@ add_spec_data(struct pbo_async_data *async, struct pbo_data *pd)
    struct pbo_spec_async_data *spec;
    struct set_entry *entry = _mesa_set_search_or_add(&async->specialized, pd, &found);
    if (!found) {
-      spec = calloc(1, sizeof(struct pbo_async_data));
+      spec = calloc(1, sizeof(struct pbo_spec_async_data));
       util_queue_fence_init(&spec->fence);
       memcpy(spec->data, pd, sizeof(struct pbo_data));
       entry->key = spec;
diff --git a/src/microsoft/vulkan/dzn_cmd_buffer.c b/src/microsoft/vulkan/dzn_cmd_buffer.c
index 8f3b114064f..d251d3fe653 100644
--- a/src/microsoft/vulkan/dzn_cmd_buffer.c
+++ b/src/microsoft/vulkan/dzn_cmd_buffer.c
@@ -3100,6 +3100,7 @@ dzn_cmd_buffer_update_pipeline(struct dzn_cmd_buffer *cmdbuf, uint32_t bindpoint
    ID3D12PipelineState *old_pipeline_state =
       cmdbuf->state.pipeline ? cmdbuf->state.pipeline->state : NULL;
 
+   uint32_t view_instance_mask = 0;
    if (cmdbuf->state.bindpoint[bindpoint].dirty & DZN_CMD_BINDPOINT_DIRTY_PIPELINE) {
       if (cmdbuf->state.bindpoint[bindpoint].root_sig != pipeline->root.sig) {
          cmdbuf->state.bindpoint[bindpoint].root_sig = pipeline->root.sig;
@@ -3135,9 +3136,9 @@ dzn_cmd_buffer_update_pipeline(struct dzn_cmd_buffer *cmdbuf, uint32_t bindpoint
          ID3D12GraphicsCommandList1_IASetPrimitiveTopology(cmdbuf->cmdlist, gfx->ia.topology);
          dzn_graphics_pipeline_get_state(gfx, &cmdbuf->state.pipeline_variant);
          if (gfx->multiview.native_view_instancing)
-            ID3D12GraphicsCommandList1_SetViewInstanceMask(cmdbuf->cmdlist, gfx->multiview.view_mask);
+            view_instance_mask = gfx->multiview.view_mask;
          else
-            ID3D12GraphicsCommandList1_SetViewInstanceMask(cmdbuf->cmdlist, 1);
+            view_instance_mask = 1;
 
          if (gfx->zsa.dynamic_depth_bias && gfx->use_gs_for_polygon_mode_point)
             cmdbuf->state.bindpoint[bindpoint].dirty |= DZN_CMD_BINDPOINT_DIRTY_SYSVALS;
@@ -3150,6 +3151,11 @@ dzn_cmd_buffer_update_pipeline(struct dzn_cmd_buffer *cmdbuf, uint32_t bindpoint
       ID3D12GraphicsCommandList1_SetPipelineState(cmdbuf->cmdlist, pipeline->state);
       cmdbuf->state.pipeline = pipeline;
    }
+
+   /* Deferring this until after the pipeline has been set due to an NVIDIA driver bug
+    * when view instancing mask is set with no pipeline bound. */
+   if (view_instance_mask)
+      ID3D12GraphicsCommandList1_SetViewInstanceMask(cmdbuf->cmdlist, view_instance_mask);
 }
 
 static void
diff --git a/src/nouveau/compiler/nak.h b/src/nouveau/compiler/nak.h
index 5a3cf249401..3ace1e5cfed 100644
--- a/src/nouveau/compiler/nak.h
+++ b/src/nouveau/compiler/nak.h
@@ -32,6 +32,8 @@ nak_nir_options(const struct nak_compiler *nak);
 void nak_optimize_nir(nir_shader *nir, const struct nak_compiler *nak);
 void nak_preprocess_nir(nir_shader *nir, const struct nak_compiler *nak);
 
+PRAGMA_DIAGNOSTIC_PUSH
+PRAGMA_DIAGNOSTIC_ERROR(-Wpadded)
 struct nak_fs_key {
    bool zs_self_dep;
 
@@ -40,6 +42,8 @@ struct nak_fs_key {
     */
    bool force_sample_shading;
 
+   uint8_t _pad;
+
    /**
     * The constant buffer index and offset at which the sample locations table lives.
     * Each sample location is two 4-bit unorm values packed into an 8-bit value
@@ -48,6 +52,9 @@ struct nak_fs_key {
    uint8_t sample_locations_cb;
    uint32_t sample_locations_offset;
 };
+PRAGMA_DIAGNOSTIC_POP
+static_assert(sizeof(struct nak_fs_key) == 8, "This struct has no holes");
+
 
 void nak_postprocess_nir(nir_shader *nir, const struct nak_compiler *nak,
                          nir_variable_mode robust2_modes,
diff --git a/src/nouveau/vulkan/nvk_cmd_buffer.c b/src/nouveau/vulkan/nvk_cmd_buffer.c
index b8828b7d350..0230bfccb44 100644
--- a/src/nouveau/vulkan/nvk_cmd_buffer.c
+++ b/src/nouveau/vulkan/nvk_cmd_buffer.c
@@ -580,8 +580,13 @@ nvk_CmdBindDescriptorSets(VkCommandBuffer commandBuffer,
          vk_to_nvk_descriptor_set_layout(pipeline_layout->set_layouts[set_idx]);
 
       if (desc->sets[set_idx] != set) {
-         desc->root.sets[set_idx] = nvk_descriptor_set_addr(set);
-         desc->set_sizes[set_idx] = set->size;
+         if (set != NULL) {
+            desc->root.sets[set_idx] = nvk_descriptor_set_addr(set);
+            desc->set_sizes[set_idx] = set->size;
+         } else {
+            desc->root.sets[set_idx] = 0;
+            desc->set_sizes[set_idx] = 0;
+         }
          desc->sets[set_idx] = set;
          desc->sets_dirty |= BITFIELD_BIT(set_idx);
 
diff --git a/src/nouveau/vulkan/nvk_cmd_draw.c b/src/nouveau/vulkan/nvk_cmd_draw.c
index 56b16b2e898..d8e4e625f7e 100644
--- a/src/nouveau/vulkan/nvk_cmd_draw.c
+++ b/src/nouveau/vulkan/nvk_cmd_draw.c
@@ -62,27 +62,19 @@ nvk_mme_set_priv_reg(struct mme_builder *b)
    mme_mthd(b, NV9097_SET_FALCON04);
    mme_emit(b, mme_load(b));
 
-   mme_if(b, ieq, s26, mme_imm(2)) {
-      struct mme_value loop_cond = mme_mov(b, mme_zero());
-      mme_while(b, ine, loop_cond, mme_imm(1)) {
-         mme_state_to(b, loop_cond, NV9097_SET_MME_SHADOW_SCRATCH(0));
-         mme_mthd(b, NV9097_NO_OPERATION);
-         mme_emit(b, mme_zero());
-      };
-   }
-
-   mme_if(b, ine, s26, mme_imm(2)) {
-      mme_loop(b, mme_imm(10)) {
-         mme_mthd(b, NV9097_NO_OPERATION);
-         mme_emit(b, mme_zero());
-      }
-   }
+   struct mme_value loop_cond = mme_mov(b, mme_zero());
+   mme_while(b, ine, loop_cond, mme_imm(1)) {
+      mme_state_to(b, loop_cond, NV9097_SET_MME_SHADOW_SCRATCH(0));
+      mme_mthd(b, NV9097_NO_OPERATION);
+      mme_emit(b, mme_zero());
+   };
 }
 
 VkResult
 nvk_queue_init_context_draw_state(struct nvk_queue *queue)
 {
    struct nvk_device *dev = nvk_queue_device(queue);
+   struct nvk_physical_device *pdev = nvk_device_physical(dev);
 
    uint32_t push_data[2048];
    struct nv_push push;
@@ -138,15 +130,71 @@ nvk_queue_init_context_draw_state(struct nvk_queue *queue)
     * For generations with firmware support for our `SET_PRIV_REG` mme method
     * we simply use that. On older generations we'll let the kernel do it.
     * Starting with GSP we have to do it via the firmware anyway.
+    *
+    * This clears bit 3 of gr_gpcs_tpcs_sm_disp_ctrl
     */
    if (dev->pdev->info.cls_eng3d >= MAXWELL_B) {
-      unsigned reg = dev->pdev->info.cls_eng3d >= VOLTA_A ? 0x419ba4 : 0x419f78;
+      unsigned reg = pdev->info.cls_eng3d >= VOLTA_A ? 0x419ba4 : 0x419f78;
       P_1INC(p, NV9097, CALL_MME_MACRO(NVK_MME_SET_PRIV_REG));
       P_INLINE_DATA(p, 0);
       P_INLINE_DATA(p, BITFIELD_BIT(3));
       P_INLINE_DATA(p, reg);
    }
 
+   /* Disable Out Of Range Address exceptions
+    *
+    * From the SPH documentation:
+    *
+    *    "The SPH fields StoreReqStart and StoreReqEnd set a range of
+    *    attributes whose corresponding Odmap values of ST or ST_LAST are
+    *    treated as ST_REQ. Normally, for an attribute whose Omap bit is TRUE
+    *    and Odmap value is ST, when the shader writes data to this output, it
+    *    can not count on being able to read it back, since the next
+    *    downstream shader might have its Imap bit FALSE, thereby causing the
+    *    Bmap bit to be FALSE. By including a ST type of attribute in the
+    *    range of StoreReqStart and StoreReqEnd, the attribute’s Odmap value
+    *    is treated as ST_REQ, so an Omap bit being TRUE causes the Bmap bit
+    *    to be TRUE. This guarantees the shader program can output the value
+    *    and then read it back later. This will save register space."
+    *
+    * It's unclear exactly what's going on but this seems to imply that the
+    * hardware actually ANDs the output mask of one shader stage together with
+    * the input mask of the subsequent shader stage to determine which values
+    * are actually used.
+    *
+    * In the case when we have an empty fragment shader, it seems the hardware
+    * doesn't allocate any output memory for final geometry stage at all and
+    * so any writes to outputs from the final shader stage generates an Out Of
+    * Range Address exception.  We could fix this by eliminating unused
+    * outputs via cross-stage linking but that won't work in the case of
+    * VK_EXT_shader_object and VK_EXT_graphics_pipeline_library fast-link.
+    * Instead, the easiest solution is to just disable the exception.
+    *
+    * NOTE (Faith):
+    *
+    *    This above analysis is 100% conjecture on my part based on a creative
+    *    reading of the SPH docs and what I saw when trying to run certain
+    *    OpenGL CTS tests on NVK + Zink.  Without access to NVIDIA HW
+    *    engineers, have no way of verifying this analysis.
+    *
+    *    The CTS test in question is:
+    *
+    *    KHR-GL46.tessellation_shader.tessellation_control_to_tessellation_evaluation.gl_tessLevel
+    *
+    * This should also prevent any issues with array overruns on I/O arrays.
+    * Before, they would get an exception and kill the context whereas now
+    * they should gently get ignored.
+    *
+    * This clears bit 14 of gr_gpcs_tpcs_sms_hww_warp_esr_report_mask
+    */
+   if (dev->pdev->info.cls_eng3d >= MAXWELL_B) {
+      unsigned reg = pdev->info.cls_eng3d >= VOLTA_A ? 0x419ea8 : 0x419e44;
+      P_1INC(p, NV9097, CALL_MME_MACRO(NVK_MME_SET_PRIV_REG));
+      P_INLINE_DATA(p, 0);
+      P_INLINE_DATA(p, BITFIELD_BIT(14));
+      P_INLINE_DATA(p, reg);
+   }
+
    P_IMMD(p, NV9097, SET_RENDER_ENABLE_C, MODE_TRUE);
 
    P_IMMD(p, NV9097, SET_Z_COMPRESSION, ENABLE_TRUE);
@@ -917,7 +965,9 @@ nvk_CmdEndRendering(VkCommandBuffer commandBuffer)
 
    if (need_resolve) {
       struct nv_push *p = nvk_cmd_buffer_push(cmd, 2);
-      P_IMMD(p, NV9097, WAIT_FOR_IDLE, 0);
+      P_IMMD(p, NVA097, INVALIDATE_TEXTURE_DATA_CACHE, {
+         .lines = LINES_ALL,
+      });
 
       nvk_meta_resolve_rendering(cmd, &vk_render);
    }
@@ -1388,13 +1438,15 @@ nvk_flush_ms_state(struct nvk_cmd_buffer *cmd)
       });
    }
 
-   if (BITSET_TEST(dyn->dirty, MESA_VK_DYNAMIC_MS_SAMPLE_LOCATIONS) ||
+   if (BITSET_TEST(dyn->dirty, MESA_VK_DYNAMIC_MS_RASTERIZATION_SAMPLES) ||
+       BITSET_TEST(dyn->dirty, MESA_VK_DYNAMIC_MS_SAMPLE_LOCATIONS) ||
        BITSET_TEST(dyn->dirty, MESA_VK_DYNAMIC_MS_SAMPLE_LOCATIONS_ENABLE)) {
       const struct vk_sample_locations_state *sl;
       if (dyn->ms.sample_locations_enable) {
          sl = dyn->ms.sample_locations;
       } else {
-         sl = vk_standard_sample_locations_state(dyn->ms.rasterization_samples);
+         const uint32_t samples = MAX2(1, dyn->ms.rasterization_samples);
+         sl = vk_standard_sample_locations_state(samples);
       }
 
       for (uint32_t i = 0; i < sl->per_pixel; i++) {
@@ -3170,44 +3222,60 @@ nvk_CmdBeginConditionalRenderingEXT(VkCommandBuffer commandBuffer,
    bool inverted = pConditionalRenderingBegin->flags &
       VK_CONDITIONAL_RENDERING_INVERTED_BIT_EXT;
 
-   if (addr & 0x3f || buffer->is_local) {
-      uint64_t tmp_addr;
-      VkResult result = nvk_cmd_buffer_cond_render_alloc(cmd, &tmp_addr);
-      if (result != VK_SUCCESS) {
-         vk_command_buffer_set_error(&cmd->vk, result);
-         return;
-      }
-
-      struct nv_push *p = nvk_cmd_buffer_push(cmd, 12);
-      P_MTHD(p, NV90B5, OFFSET_IN_UPPER);
-      P_NV90B5_OFFSET_IN_UPPER(p, addr >> 32);
-      P_NV90B5_OFFSET_IN_LOWER(p, addr & 0xffffffff);
-      P_NV90B5_OFFSET_OUT_UPPER(p, tmp_addr >> 32);
-      P_NV90B5_OFFSET_OUT_LOWER(p, tmp_addr & 0xffffffff);
-      P_NV90B5_PITCH_IN(p, 4);
-      P_NV90B5_PITCH_OUT(p, 4);
-      P_NV90B5_LINE_LENGTH_IN(p, 4);
-      P_NV90B5_LINE_COUNT(p, 1);
-
-      P_IMMD(p, NV90B5, LAUNCH_DMA, {
-            .data_transfer_type = DATA_TRANSFER_TYPE_PIPELINED,
-            .multi_line_enable = MULTI_LINE_ENABLE_TRUE,
-            .flush_enable = FLUSH_ENABLE_TRUE,
-            .src_memory_layout = SRC_MEMORY_LAYOUT_PITCH,
-            .dst_memory_layout = DST_MEMORY_LAYOUT_PITCH,
-         });
-      addr = tmp_addr;
+   /* From the Vulkan 1.3.280 spec:
+    *
+    *    "If the 32-bit value at offset in buffer memory is zero,
+    *     then the rendering commands are discarded,
+    *     otherwise they are executed as normal."
+    *
+    * The hardware compare a 64-bit value, as such we are required to copy it.
+    */
+   uint64_t tmp_addr;
+   VkResult result = nvk_cmd_buffer_cond_render_alloc(cmd, &tmp_addr);
+   if (result != VK_SUCCESS) {
+      vk_command_buffer_set_error(&cmd->vk, result);
+      return;
    }
 
-   struct nv_push *p = nvk_cmd_buffer_push(cmd, 12);
+   struct nv_push *p = nvk_cmd_buffer_push(cmd, 26);
+
+   P_MTHD(p, NV90B5, OFFSET_IN_UPPER);
+   P_NV90B5_OFFSET_IN_UPPER(p, addr >> 32);
+   P_NV90B5_OFFSET_IN_LOWER(p, addr & 0xffffffff);
+   P_NV90B5_OFFSET_OUT_UPPER(p, tmp_addr >> 32);
+   P_NV90B5_OFFSET_OUT_LOWER(p, tmp_addr & 0xffffffff);
+   P_NV90B5_PITCH_IN(p, 4);
+   P_NV90B5_PITCH_OUT(p, 4);
+   P_NV90B5_LINE_LENGTH_IN(p, 4);
+   P_NV90B5_LINE_COUNT(p, 1);
+
+   P_IMMD(p, NV90B5, SET_REMAP_COMPONENTS, {
+      .dst_x = DST_X_SRC_X,
+      .dst_y = DST_Y_SRC_X,
+      .dst_z = DST_Z_NO_WRITE,
+      .dst_w = DST_W_NO_WRITE,
+      .component_size = COMPONENT_SIZE_ONE,
+      .num_src_components = NUM_SRC_COMPONENTS_ONE,
+      .num_dst_components = NUM_DST_COMPONENTS_TWO,
+   });
+
+   P_IMMD(p, NV90B5, LAUNCH_DMA, {
+      .data_transfer_type = DATA_TRANSFER_TYPE_PIPELINED,
+      .multi_line_enable = MULTI_LINE_ENABLE_TRUE,
+      .flush_enable = FLUSH_ENABLE_TRUE,
+      .src_memory_layout = SRC_MEMORY_LAYOUT_PITCH,
+      .dst_memory_layout = DST_MEMORY_LAYOUT_PITCH,
+      .remap_enable = REMAP_ENABLE_TRUE,
+   });
+
    P_MTHD(p, NV9097, SET_RENDER_ENABLE_A);
-   P_NV9097_SET_RENDER_ENABLE_A(p, addr >> 32);
-   P_NV9097_SET_RENDER_ENABLE_B(p, addr & 0xfffffff0);
+   P_NV9097_SET_RENDER_ENABLE_A(p, tmp_addr >> 32);
+   P_NV9097_SET_RENDER_ENABLE_B(p, tmp_addr & 0xfffffff0);
    P_NV9097_SET_RENDER_ENABLE_C(p, inverted ? MODE_RENDER_IF_EQUAL : MODE_RENDER_IF_NOT_EQUAL);
 
    P_MTHD(p, NV90C0, SET_RENDER_ENABLE_A);
-   P_NV90C0_SET_RENDER_ENABLE_A(p, addr >> 32);
-   P_NV90C0_SET_RENDER_ENABLE_B(p, addr & 0xfffffff0);
+   P_NV90C0_SET_RENDER_ENABLE_A(p, tmp_addr >> 32);
+   P_NV90C0_SET_RENDER_ENABLE_B(p, tmp_addr & 0xfffffff0);
    P_NV90C0_SET_RENDER_ENABLE_C(p, inverted ? MODE_RENDER_IF_EQUAL : MODE_RENDER_IF_NOT_EQUAL);
 }
 
diff --git a/src/nouveau/vulkan/nvk_cmd_meta.c b/src/nouveau/vulkan/nvk_cmd_meta.c
index 39135ae5967..bb354901222 100644
--- a/src/nouveau/vulkan/nvk_cmd_meta.c
+++ b/src/nouveau/vulkan/nvk_cmd_meta.c
@@ -130,6 +130,7 @@ nvk_meta_end(struct nvk_cmd_buffer *cmd,
 {
    if (save->desc0) {
       cmd->state.gfx.descriptors.sets[0] = save->desc0;
+      cmd->state.gfx.descriptors.set_sizes[0] = save->desc0->size;
       cmd->state.gfx.descriptors.root.sets[0] = nvk_descriptor_set_addr(save->desc0);
       cmd->state.gfx.descriptors.sets_dirty |= BITFIELD_BIT(0);
       cmd->state.gfx.descriptors.push_dirty &= ~BITFIELD_BIT(0);
diff --git a/src/nouveau/vulkan/nvk_instance.c b/src/nouveau/vulkan/nvk_instance.c
index 5340b1b6837..2fcea916fe5 100644
--- a/src/nouveau/vulkan/nvk_instance.c
+++ b/src/nouveau/vulkan/nvk_instance.c
@@ -155,6 +155,9 @@ nvk_DestroyInstance(VkInstance _instance,
    if (!instance)
       return;
 
+   driDestroyOptionCache(&instance->dri_options);
+   driDestroyOptionInfo(&instance->available_dri_options);
+
    vk_instance_finish(&instance->vk);
    vk_free(&instance->vk.alloc, instance);
 }
diff --git a/src/nouveau/vulkan/nvk_nir_lower_descriptors.c b/src/nouveau/vulkan/nvk_nir_lower_descriptors.c
index 98324ccf2e3..1f8b847cf2b 100644
--- a/src/nouveau/vulkan/nvk_nir_lower_descriptors.c
+++ b/src/nouveau/vulkan/nvk_nir_lower_descriptors.c
@@ -581,6 +581,9 @@ load_descriptor(nir_builder *b, unsigned num_components, unsigned bit_size,
          nir_iadd_imm(b, nir_imul_imm(b, index, binding_layout->stride),
                          binding_layout->offset + offset_B);
 
+      uint64_t max_desc_ubo_offset = binding_layout->offset +
+         binding_layout->array_size * binding_layout->stride;
+
       unsigned desc_align_mul = (1 << (ffs(binding_layout->stride) - 1));
       desc_align_mul = MIN2(desc_align_mul, 16);
       unsigned desc_align_offset = binding_layout->offset + offset_B;
@@ -593,7 +596,7 @@ load_descriptor(nir_builder *b, unsigned num_components, unsigned bit_size,
       int cbuf_idx = get_mapped_cbuf_idx(&cbuf_key, ctx);
 
       nir_def *desc;
-      if (cbuf_idx >= 0) {
+      if (cbuf_idx >= 0 && max_desc_ubo_offset <= NVK_MAX_CBUF_SIZE) {
          desc = nir_load_ubo(b, num_components, bit_size,
                              nir_imm_int(b, cbuf_idx),
                              desc_ubo_offset,
diff --git a/src/nouveau/vulkan/nvk_physical_device.c b/src/nouveau/vulkan/nvk_physical_device.c
index a9ef92e3106..e071a14168b 100644
--- a/src/nouveau/vulkan/nvk_physical_device.c
+++ b/src/nouveau/vulkan/nvk_physical_device.c
@@ -538,6 +538,9 @@ nvk_get_device_properties(const struct nvk_instance *instance,
                                                VK_SAMPLE_COUNT_4_BIT |
                                                VK_SAMPLE_COUNT_8_BIT;
 
+   uint64_t os_page_size = 4096;
+   os_get_page_size(&os_page_size);
+
    *properties = (struct vk_properties) {
       .apiVersion = nvk_get_vk_version(info),
       .driverVersion = vk_get_driver_version(),
@@ -614,7 +617,7 @@ nvk_get_device_properties(const struct nvk_instance *instance,
       .maxViewportDimensions = { 32768, 32768 },
       .viewportBoundsRange = { -65536, 65536 },
       .viewportSubPixelBits = 8,
-      .minMemoryMapAlignment = 64,
+      .minMemoryMapAlignment = os_page_size,
       .minTexelBufferOffsetAlignment = NVK_MIN_TEXEL_BUFFER_ALIGNMENT,
       .minUniformBufferOffsetAlignment = nvk_min_cbuf_alignment(info),
       .minStorageBufferOffsetAlignment = NVK_MIN_SSBO_ALIGNMENT,
diff --git a/src/nouveau/vulkan/nvk_queue_drm_nouveau.c b/src/nouveau/vulkan/nvk_queue_drm_nouveau.c
index 3c7653a48c0..86252bf2c7e 100644
--- a/src/nouveau/vulkan/nvk_queue_drm_nouveau.c
+++ b/src/nouveau/vulkan/nvk_queue_drm_nouveau.c
@@ -20,7 +20,7 @@
 
 #include <xf86drm.h>
 
-#define NVK_PUSH_MAX_SYNCS 16
+#define NVK_PUSH_MAX_SYNCS 256
 #define NVK_PUSH_MAX_BINDS 4096
 #define NVK_PUSH_MAX_PUSH 1024
 
diff --git a/src/nouveau/vulkan/nvk_shader.h b/src/nouveau/vulkan/nvk_shader.h
index dae7e6fef1b..970db3f5f82 100644
--- a/src/nouveau/vulkan/nvk_shader.h
+++ b/src/nouveau/vulkan/nvk_shader.h
@@ -39,8 +39,10 @@ struct nvk_cbuf {
    enum nvk_cbuf_type type;
    uint8_t desc_set;
    uint8_t dynamic_idx;
+   uint8_t _pad;
    uint32_t desc_offset;
 };
+static_assert(sizeof(struct nvk_cbuf) == 8, "This struct has no holes");
 
 struct nvk_cbuf_map {
    uint32_t cbuf_count;
diff --git a/src/nouveau/winsys/nouveau_bo.c b/src/nouveau/winsys/nouveau_bo.c
index 6ee022b14a3..591070de20d 100644
--- a/src/nouveau/winsys/nouveau_bo.c
+++ b/src/nouveau/winsys/nouveau_bo.c
@@ -10,6 +10,9 @@
 #include <sys/mman.h>
 #include <xf86drm.h>
 
+#include "nvidia/classes/cl9097.h"
+#include "nvidia/classes/clc597.h"
+
 static void
 bo_bind(struct nouveau_ws_device *dev,
         uint32_t handle, uint64_t addr,
@@ -170,9 +173,10 @@ nouveau_ws_bo_new_mapped(struct nouveau_ws_device *dev,
 }
 
 static struct nouveau_ws_bo *
-nouveau_ws_bo_new_locked(struct nouveau_ws_device *dev,
-                         uint64_t size, uint64_t align,
-                         enum nouveau_ws_bo_flags flags)
+nouveau_ws_bo_new_tiled_locked(struct nouveau_ws_device *dev,
+                               uint64_t size, uint64_t align,
+                               uint8_t pte_kind, uint16_t tile_mode,
+                               enum nouveau_ws_bo_flags flags)
 {
    struct drm_nouveau_gem_new req = {};
 
@@ -205,6 +209,9 @@ nouveau_ws_bo_new_locked(struct nouveau_ws_device *dev,
    if (flags & NOUVEAU_WS_BO_NO_SHARE)
       req.info.domain |= NOUVEAU_GEM_DOMAIN_NO_SHARE;
 
+   req.info.tile_flags = (uint32_t)pte_kind << 8;
+   req.info.tile_mode = tile_mode;
+
    req.info.size = size;
    req.align = align;
 
@@ -242,19 +249,29 @@ fail_gem_new:
 }
 
 struct nouveau_ws_bo *
-nouveau_ws_bo_new(struct nouveau_ws_device *dev,
-                  uint64_t size, uint64_t align,
-                  enum nouveau_ws_bo_flags flags)
+nouveau_ws_bo_new_tiled(struct nouveau_ws_device *dev,
+                        uint64_t size, uint64_t align,
+                        uint8_t pte_kind, uint16_t tile_mode,
+                        enum nouveau_ws_bo_flags flags)
 {
    struct nouveau_ws_bo *bo;
 
    simple_mtx_lock(&dev->bos_lock);
-   bo = nouveau_ws_bo_new_locked(dev, size, align, flags);
+   bo = nouveau_ws_bo_new_tiled_locked(dev, size, align,
+                                       pte_kind, tile_mode, flags);
    simple_mtx_unlock(&dev->bos_lock);
 
    return bo;
 }
 
+struct nouveau_ws_bo *
+nouveau_ws_bo_new(struct nouveau_ws_device *dev,
+                  uint64_t size, uint64_t align,
+                  enum nouveau_ws_bo_flags flags)
+{
+   return nouveau_ws_bo_new_tiled(dev, size, align, 0, 0, flags);
+}
+
 static struct nouveau_ws_bo *
 nouveau_ws_bo_from_dma_buf_locked(struct nouveau_ws_device *dev, int fd)
 {
@@ -265,8 +282,11 @@ nouveau_ws_bo_from_dma_buf_locked(struct nouveau_ws_device *dev, int fd)
 
    struct hash_entry *entry =
       _mesa_hash_table_search(dev->bos, (void *)(uintptr_t)handle);
-   if (entry != NULL)
-      return entry->data;
+   if (entry != NULL) {
+      struct nouveau_ws_bo *bo = entry->data;
+      nouveau_ws_bo_ref(bo);
+      return bo;
+   }
 
    /*
     * If we got here, no BO exists for the retrieved handle. If we error
diff --git a/src/nouveau/winsys/nouveau_bo.h b/src/nouveau/winsys/nouveau_bo.h
index d931bea44f9..14bd87a10cc 100644
--- a/src/nouveau/winsys/nouveau_bo.h
+++ b/src/nouveau/winsys/nouveau_bo.h
@@ -68,6 +68,11 @@ struct nouveau_ws_bo *nouveau_ws_bo_new_mapped(struct nouveau_ws_device *,
                                                enum nouveau_ws_bo_flags,
                                                enum nouveau_ws_bo_map_flags map_flags,
                                                void **map_out);
+struct nouveau_ws_bo *nouveau_ws_bo_new_tiled(struct nouveau_ws_device *,
+                                              uint64_t size, uint64_t align,
+                                              uint8_t pte_kind,
+                                              uint16_t tile_mode,
+                                              enum nouveau_ws_bo_flags);
 struct nouveau_ws_bo *nouveau_ws_bo_from_dma_buf(struct nouveau_ws_device *,
                                                  int fd);
 void nouveau_ws_bo_destroy(struct nouveau_ws_bo *);
diff --git a/src/nouveau/winsys/nouveau_device.c b/src/nouveau/winsys/nouveau_device.c
index 904a7ad6241..f1e0a3494c0 100644
--- a/src/nouveau/winsys/nouveau_device.c
+++ b/src/nouveau/winsys/nouveau_device.c
@@ -351,6 +351,7 @@ nouveau_ws_device_new(drmDevicePtr drm_device)
 out_err:
    if (device->has_vm_bind) {
       util_vma_heap_finish(&device->vma_heap);
+      util_vma_heap_finish(&device->bda_heap);
       simple_mtx_destroy(&device->vma_mutex);
    }
    if (ver)
@@ -372,9 +373,20 @@ nouveau_ws_device_destroy(struct nouveau_ws_device *device)
 
    if (device->has_vm_bind) {
       util_vma_heap_finish(&device->vma_heap);
+      util_vma_heap_finish(&device->bda_heap);
       simple_mtx_destroy(&device->vma_mutex);
    }
 
    close(device->fd);
    FREE(device);
 }
+
+bool
+nouveau_ws_device_has_tiled_bo(struct nouveau_ws_device *device)
+{
+   uint64_t has = 0;
+   if (nouveau_ws_param(device->fd, NOUVEAU_GETPARAM_HAS_VMA_TILEMODE, &has))
+      return false;
+
+   return has != 0;
+}
diff --git a/src/nouveau/winsys/nouveau_device.h b/src/nouveau/winsys/nouveau_device.h
index 413a2e827e1..0f4eb0cc71a 100644
--- a/src/nouveau/winsys/nouveau_device.h
+++ b/src/nouveau/winsys/nouveau_device.h
@@ -64,6 +64,8 @@ struct nouveau_ws_device {
 struct nouveau_ws_device *nouveau_ws_device_new(struct _drmDevice *drm_device);
 void nouveau_ws_device_destroy(struct nouveau_ws_device *);
 
+bool nouveau_ws_device_has_tiled_bo(struct nouveau_ws_device *device);
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/src/panfrost/ci/gitlab-ci.yml b/src/panfrost/ci/gitlab-ci.yml
index a2f15d22552..4faf3f2fe26 100644
--- a/src/panfrost/ci/gitlab-ci.yml
+++ b/src/panfrost/ci/gitlab-ci.yml
@@ -17,6 +17,7 @@
         - src/panfrost/ci/$PIGLIT_TRACES_FILE
         - src/panfrost/include/*
         - src/panfrost/lib/*
+        - src/panfrost/lib/genxml/*
         - src/panfrost/lib/kmod/*
         - src/panfrost/shared/*
         - src/panfrost/util/*
diff --git a/src/panfrost/ci/panfrost-g52-fails.txt b/src/panfrost/ci/panfrost-g52-fails.txt
index c52e7453f52..1081f02dbec 100644
--- a/src/panfrost/ci/panfrost-g52-fails.txt
+++ b/src/panfrost/ci/panfrost-g52-fails.txt
@@ -15,44 +15,11 @@ shaders@point-vertex-id gl_vertexid,Fail
 shaders@point-vertex-id gl_vertexid gl_instanceid divisor,Fail
 shaders@point-vertex-id gl_vertexid gl_instanceid,Fail
 spec@arb_base_instance@arb_base_instance-drawarrays,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 1024 d=s=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 1024 ds=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 1024 s=d=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 146 d=s=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 146 ds=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 146 s=d=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 273 d=s=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 273 ds=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 273 s=d=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 292 d=s=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 292 ds=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 292 s=d=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 585 d=s=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 585 ds=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 585 s=d=z32f_s8,Fail
-spec@arb_depth_buffer_float@fbo-depthstencil-gl_depth32f_stencil8-blit,Fail
-spec@arb_depth_buffer_float@fbo-depthstencil-gl_depth32f_stencil8-copypixels,Fail
-spec@arb_depth_buffer_float@fbo-depthstencil-gl_depth32f_stencil8-drawpixels-24_8,Fail
-spec@arb_depth_buffer_float@fbo-depthstencil-gl_depth32f_stencil8-drawpixels-32f_24_8_rev,Fail
-spec@arb_depth_buffer_float@fbo-depthstencil-gl_depth32f_stencil8-drawpixels-float-and-ushort,Fail
-spec@arb_depth_buffer_float@fbo-depthstencil-gl_depth32f_stencil8-readpixels-24_8,Fail
-spec@arb_depth_buffer_float@fbo-depthstencil-gl_depth32f_stencil8-readpixels-32f_24_8_rev,Fail
-spec@arb_depth_buffer_float@fbo-depthstencil-gl_depth32f_stencil8-readpixels-float-and-ushort,Fail
-spec@arb_depth_buffer_float@fbo-stencil-gl_depth32f_stencil8-blit,Fail
-spec@arb_depth_buffer_float@fbo-stencil-gl_depth32f_stencil8-copypixels,Fail
-spec@arb_depth_buffer_float@fbo-stencil-gl_depth32f_stencil8-drawpixels,Fail
-spec@arb_depth_buffer_float@fbo-stencil-gl_depth32f_stencil8-readpixels,Fail
 spec@arb_depth_clamp@depth-clamp-range,Fail
 spec@arb_direct_state_access@gettextureimage-formats,Fail
 spec@arb_direct_state_access@gettextureimage-formats init-by-rendering,Fail
-spec@arb_framebuffer_object@arb_framebuffer_object-depth-stencil-blit depth gl_depth32f_stencil8,Fail
-spec@arb_framebuffer_object@arb_framebuffer_object-depth-stencil-blit depth_stencil gl_depth32f_stencil8,Fail
-spec@arb_framebuffer_object@arb_framebuffer_object-depth-stencil-blit stencil gl_depth32f_stencil8,Fail
 spec@arb_framebuffer_object@fbo-luminance-alpha,Fail
 spec@arb_framebuffer_srgb@fbo-fast-clear,Fail
-spec@arb_pixel_buffer_object@fbo-pbo-readpixels-small,Fail
-spec@arb_pixel_buffer_object@fbo-pbo-readpixels-small@GL_DEPTH32F_STENCIL8-GL_DEPTH_STENCIL,Fail
-spec@arb_pixel_buffer_object@fbo-pbo-readpixels-small@GL_DEPTH32F_STENCIL8-GL_STENCIL_INDEX,Fail
 spec@arb_point_sprite@arb_point_sprite-mipmap,Fail
 spec@arb_sample_shading@samplemask 2@0.250000 mask_in_one,Fail
 spec@arb_sample_shading@samplemask 2@0.500000 mask_in_one,Fail
@@ -134,7 +101,6 @@ spec@arb_texture_multisample@arb_texture_multisample-dsa-texelfetch@Texture type
 spec@arb_texture_multisample@arb_texture_multisample-dsa-texelfetch@Texture type: GL_RGBA8,Fail
 spec@arb_texture_multisample@arb_texture_multisample-dsa-texelfetch@Texture type: GL_RGBA8I,Fail
 spec@arb_texture_multisample@arb_texture_multisample-dsa-texelfetch@Texture type: GL_SRGB8_ALPHA8,Fail
-spec@arb_texture_rectangle@1-1-linear-texture,Fail
 spec@arb_texture_rectangle@tex-miplevel-selection gl2:texture() 2drect,Crash
 spec@arb_texture_rectangle@tex-miplevel-selection gl2:texture() 2drectshadow,Crash
 spec@arb_texture_rectangle@tex-miplevel-selection gl2:textureproj 2drect,Crash
@@ -435,7 +401,6 @@ dEQP-VK.spirv_assembly.instruction.compute.workgroup_memory.float32,Crash
 dEQP-VK.spirv_assembly.instruction.compute.workgroup_memory.int32,Crash
 dEQP-VK.spirv_assembly.instruction.compute.workgroup_memory.uint32,Crash
 
-dEQP-VK.api.command_buffers.record_many_draws_secondary_2,Fail
 dEQP-VK.glsl.operator.sequence.no_side_effects.highp_bool_vec2_fragment,Fail
 dEQP-VK.glsl.operator.sequence.no_side_effects.highp_float_uint_fragment,Fail
 dEQP-VK.glsl.operator.sequence.no_side_effects.highp_vec4_ivec4_bvec4_fragment,Fail
diff --git a/src/panfrost/ci/panfrost-g57-fails.txt b/src/panfrost/ci/panfrost-g57-fails.txt
index 841918b7dea..1079cff9413 100644
--- a/src/panfrost/ci/panfrost-g57-fails.txt
+++ b/src/panfrost/ci/panfrost-g57-fails.txt
@@ -24,44 +24,11 @@ shaders@point-vertex-id gl_vertexid,Fail
 shaders@point-vertex-id gl_vertexid gl_instanceid divisor,Fail
 shaders@point-vertex-id gl_vertexid gl_instanceid,Fail
 spec@arb_base_instance@arb_base_instance-drawarrays,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 1024 d=s=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 1024 ds=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 1024 s=d=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 146 d=s=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 146 ds=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 146 s=d=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 273 d=s=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 273 ds=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 273 s=d=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 292 d=s=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 292 ds=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 292 s=d=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 585 d=s=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 585 ds=z32f_s8,Fail
-spec@arb_depth_buffer_float@depthstencil-render-miplevels 585 s=d=z32f_s8,Fail
-spec@arb_depth_buffer_float@fbo-depthstencil-gl_depth32f_stencil8-blit,Fail
-spec@arb_depth_buffer_float@fbo-depthstencil-gl_depth32f_stencil8-copypixels,Fail
-spec@arb_depth_buffer_float@fbo-depthstencil-gl_depth32f_stencil8-drawpixels-24_8,Fail
-spec@arb_depth_buffer_float@fbo-depthstencil-gl_depth32f_stencil8-drawpixels-32f_24_8_rev,Fail
-spec@arb_depth_buffer_float@fbo-depthstencil-gl_depth32f_stencil8-drawpixels-float-and-ushort,Fail
-spec@arb_depth_buffer_float@fbo-depthstencil-gl_depth32f_stencil8-readpixels-24_8,Fail
-spec@arb_depth_buffer_float@fbo-depthstencil-gl_depth32f_stencil8-readpixels-32f_24_8_rev,Fail
-spec@arb_depth_buffer_float@fbo-depthstencil-gl_depth32f_stencil8-readpixels-float-and-ushort,Fail
-spec@arb_depth_buffer_float@fbo-stencil-gl_depth32f_stencil8-blit,Fail
-spec@arb_depth_buffer_float@fbo-stencil-gl_depth32f_stencil8-copypixels,Fail
-spec@arb_depth_buffer_float@fbo-stencil-gl_depth32f_stencil8-drawpixels,Fail
-spec@arb_depth_buffer_float@fbo-stencil-gl_depth32f_stencil8-readpixels,Fail
 spec@arb_depth_clamp@depth-clamp-range,Fail
 spec@arb_direct_state_access@gettextureimage-formats,Fail
 spec@arb_direct_state_access@gettextureimage-formats init-by-rendering,Fail
-spec@arb_framebuffer_object@arb_framebuffer_object-depth-stencil-blit depth gl_depth32f_stencil8,Fail
-spec@arb_framebuffer_object@arb_framebuffer_object-depth-stencil-blit depth_stencil gl_depth32f_stencil8,Fail
-spec@arb_framebuffer_object@arb_framebuffer_object-depth-stencil-blit stencil gl_depth32f_stencil8,Fail
 spec@arb_framebuffer_object@fbo-luminance-alpha,Fail
 spec@arb_framebuffer_srgb@fbo-fast-clear,Fail
-spec@arb_pixel_buffer_object@fbo-pbo-readpixels-small,Fail
-spec@arb_pixel_buffer_object@fbo-pbo-readpixels-small@GL_DEPTH32F_STENCIL8-GL_DEPTH_STENCIL,Fail
-spec@arb_pixel_buffer_object@fbo-pbo-readpixels-small@GL_DEPTH32F_STENCIL8-GL_STENCIL_INDEX,Fail
 spec@arb_point_sprite@arb_point_sprite-mipmap,Fail
 spec@arb_sample_shading@samplemask 2@0.250000 mask_in_one,Fail
 spec@arb_sample_shading@samplemask 2@0.500000 mask_in_one,Fail
diff --git a/src/panfrost/compiler/bifrost_compile.c b/src/panfrost/compiler/bifrost_compile.c
index 90020640329..b4bad2f2c7f 100644
--- a/src/panfrost/compiler/bifrost_compile.c
+++ b/src/panfrost/compiler/bifrost_compile.c
@@ -837,15 +837,18 @@ bi_emit_fragment_out(bi_builder *b, nir_intrinsic_instr *instr)
       nir_alu_type T = nir_intrinsic_src_type(instr);
 
       bi_index rgba = bi_src_index(&instr->src[0]);
-      bi_index alpha = (T == nir_type_float16)
-                          ? bi_half(bi_extract(b, rgba, 1), true)
-                       : (T == nir_type_float32) ? bi_extract(b, rgba, 3)
-                                                 : bi_dontcare(b);
+      bi_index alpha;
 
-      /* Don't read out-of-bounds */
-      if (nir_src_num_components(instr->src[0]) < 4)
+      if (nir_src_num_components(instr->src[0]) < 4) {
+         /* Don't read out-of-bounds */
          alpha = bi_imm_f32(1.0);
-
+      } else if (T == nir_type_float16) {
+         alpha = bi_half(bi_extract(b, rgba, 1), true);
+      } else if (T == nir_type_float32) {
+         alpha = bi_extract(b, rgba, 3);
+      } else {
+         alpha = bi_dontcare(b);
+      }
       bi_emit_atest(b, alpha);
    }
 
@@ -3475,7 +3478,7 @@ bi_emit_tex_valhall(bi_builder *b, nir_tex_instr *instr)
             /* Copy XY (for 2D+) or XX (for 1D) */
             sregs[VALHALL_TEX_SREG_X_COORD] = index;
 
-            if (components >= 2)
+            if ((components == 2 && !instr->is_array) || components > 2)
                sregs[VALHALL_TEX_SREG_Y_COORD] = bi_extract(b, index, 1);
 
             if (components == 3 && !instr->is_array) {
diff --git a/src/panfrost/compiler/valhall/va_insert_flow.c b/src/panfrost/compiler/valhall/va_insert_flow.c
index 5cbe6a13ad6..00560c60b31 100644
--- a/src/panfrost/compiler/valhall/va_insert_flow.c
+++ b/src/panfrost/compiler/valhall/va_insert_flow.c
@@ -115,9 +115,16 @@ bi_ld_vary_writes_hidden_register(const bi_instr *I)
 static bool
 bi_is_memory_access(const bi_instr *I)
 {
-   /* On the attribute unit but functionally a general memory load */
-   if (I->op == BI_OPCODE_LD_ATTR_TEX)
+   /* Some instructions on the attribute unit are functionally
+      a general memory load */
+   switch (I->op) {
+   case BI_OPCODE_LD_ATTR_TEX:
+   case BI_OPCODE_LD_TEX:
+   case BI_OPCODE_LD_TEX_IMM:
       return true;
+   default:
+      break;
+   }
 
    /* UBOs are read-only so there are no ordering constriants */
    if (I->seg == BI_SEG_UBO)
diff --git a/src/panfrost/lib/genxml/v10.xml b/src/panfrost/lib/genxml/v10.xml
index 249aaff7850..6d5cb5e244a 100644
--- a/src/panfrost/lib/genxml/v10.xml
+++ b/src/panfrost/lib/genxml/v10.xml
@@ -969,7 +969,7 @@
     <field name="Wrap Mode R" size="4" start="0:8" type="Wrap Mode" default="Clamp to Edge"/>
     <field name="Wrap Mode T" size="4" start="0:12" type="Wrap Mode" default="Clamp to Edge"/>
     <field name="Wrap Mode S" size="4" start="0:16" type="Wrap Mode" default="Clamp to Edge"/>
-    <field name="Round to nearest even" size="1" start="0:21" type="bool" default="false"/>
+    <field name="Round to nearest even" size="1" start="0:21" type="bool" default="true"/>
     <!--- Disable sRGB-to-linear conversion (assume linear) -->
     <field name="sRGB override" size="1" start="0:22" type="bool" default="false"/>
     <field name="Seamless Cube Map" size="1" start="0:23" type="bool" default="true"/>
diff --git a/src/panfrost/lib/genxml/v6.xml b/src/panfrost/lib/genxml/v6.xml
index 9d042c4db93..86622677991 100644
--- a/src/panfrost/lib/genxml/v6.xml
+++ b/src/panfrost/lib/genxml/v6.xml
@@ -632,7 +632,7 @@
     <field name="Wrap Mode R" size="4" start="0:8" type="Wrap Mode" default="Clamp to Edge"/>
     <field name="Wrap Mode T" size="4" start="0:12" type="Wrap Mode" default="Clamp to Edge"/>
     <field name="Wrap Mode S" size="4" start="0:16" type="Wrap Mode" default="Clamp to Edge"/>
-    <field name="Round to nearest even" size="1" start="0:21" type="bool" default="false"/>
+    <field name="Round to nearest even" size="1" start="0:21" type="bool" default="true"/>
     <!--- Disable sRGB-to-linear conversion (assume linear) -->
     <field name="sRGB override" size="1" start="0:22" type="bool" default="false"/>
     <field name="Seamless Cube Map" size="1" start="0:23" type="bool" default="true"/>
diff --git a/src/panfrost/lib/genxml/v7.xml b/src/panfrost/lib/genxml/v7.xml
index 7e0b794ec85..da560d2733a 100644
--- a/src/panfrost/lib/genxml/v7.xml
+++ b/src/panfrost/lib/genxml/v7.xml
@@ -696,7 +696,7 @@
     <field name="Wrap Mode R" size="4" start="0:8" type="Wrap Mode" default="Clamp to Edge"/>
     <field name="Wrap Mode T" size="4" start="0:12" type="Wrap Mode" default="Clamp to Edge"/>
     <field name="Wrap Mode S" size="4" start="0:16" type="Wrap Mode" default="Clamp to Edge"/>
-    <field name="Round to nearest even" size="1" start="0:21" type="bool" default="false"/>
+    <field name="Round to nearest even" size="1" start="0:21" type="bool" default="true"/>
     <!--- Disable sRGB-to-linear conversion (assume linear) -->
     <field name="sRGB override" size="1" start="0:22" type="bool" default="false"/>
     <field name="Seamless Cube Map" size="1" start="0:23" type="bool" default="true"/>
diff --git a/src/panfrost/lib/genxml/v9.xml b/src/panfrost/lib/genxml/v9.xml
index c08d49e2025..961f38badfa 100644
--- a/src/panfrost/lib/genxml/v9.xml
+++ b/src/panfrost/lib/genxml/v9.xml
@@ -623,7 +623,7 @@
     <field name="Wrap Mode R" size="4" start="0:8" type="Wrap Mode" default="Clamp to Edge"/>
     <field name="Wrap Mode T" size="4" start="0:12" type="Wrap Mode" default="Clamp to Edge"/>
     <field name="Wrap Mode S" size="4" start="0:16" type="Wrap Mode" default="Clamp to Edge"/>
-    <field name="Round to nearest even" size="1" start="0:21" type="bool" default="false"/>
+    <field name="Round to nearest even" size="1" start="0:21" type="bool" default="true"/>
     <!--- Disable sRGB-to-linear conversion (assume linear) -->
     <field name="sRGB override" size="1" start="0:22" type="bool" default="false"/>
     <field name="Seamless Cube Map" size="1" start="0:23" type="bool" default="true"/>
diff --git a/src/panfrost/lib/kmod/pan_kmod.c b/src/panfrost/lib/kmod/pan_kmod.c
index 395bc378f31..b6b43a5cf3b 100644
--- a/src/panfrost/lib/kmod/pan_kmod.c
+++ b/src/panfrost/lib/kmod/pan_kmod.c
@@ -7,6 +7,7 @@
 #include <string.h>
 #include <xf86drm.h>
 
+#include "util/u_memory.h"
 #include "util/macros.h"
 #include "pan_kmod.h"
 
@@ -26,28 +27,19 @@ static void *
 default_zalloc(const struct pan_kmod_allocator *allocator, size_t size,
                UNUSED bool transient)
 {
-   return rzalloc_size(allocator, size);
+   return os_calloc(1, size);
 }
 
 static void
 default_free(const struct pan_kmod_allocator *allocator, void *data)
 {
-   return ralloc_free(data);
+   os_free(data);
 }
 
-static const struct pan_kmod_allocator *
-create_default_allocator(void)
-{
-   struct pan_kmod_allocator *allocator =
-      rzalloc(NULL, struct pan_kmod_allocator);
-
-   if (allocator) {
-      allocator->zalloc = default_zalloc;
-      allocator->free = default_free;
-   }
-
-   return allocator;
-}
+static const struct pan_kmod_allocator default_allocator = {
+   .zalloc = default_zalloc,
+   .free = default_free,
+};
 
 struct pan_kmod_dev *
 pan_kmod_dev_create(int fd, uint32_t flags,
@@ -59,28 +51,18 @@ pan_kmod_dev_create(int fd, uint32_t flags,
    if (!version)
       return NULL;
 
-   if (!allocator) {
-      allocator = create_default_allocator();
-      if (!allocator)
-         goto out_free_version;
-   }
+   if (!allocator)
+      allocator = &default_allocator;
 
    for (unsigned i = 0; i < ARRAY_SIZE(drivers); i++) {
       if (!strcmp(drivers[i].name, version->name)) {
          const struct pan_kmod_ops *ops = drivers[i].ops;
 
          dev = ops->dev_create(fd, flags, version, allocator);
-         if (dev)
-            goto out_free_version;
-
          break;
       }
    }
 
-   if (allocator->zalloc == default_zalloc)
-      ralloc_free((void *)allocator);
-
-out_free_version:
    drmFreeVersion(version);
    return dev;
 }
@@ -88,12 +70,7 @@ out_free_version:
 void
 pan_kmod_dev_destroy(struct pan_kmod_dev *dev)
 {
-   const struct pan_kmod_allocator *allocator = dev->allocator;
-
    dev->ops->dev_destroy(dev);
-
-   if (allocator->zalloc == default_zalloc)
-      ralloc_free((void *)allocator);
 }
 
 struct pan_kmod_bo *
diff --git a/src/panfrost/lib/pan_blitter.c b/src/panfrost/lib/pan_blitter.c
index 84bcba6361f..9ffe0c16acc 100644
--- a/src/panfrost/lib/pan_blitter.c
+++ b/src/panfrost/lib/pan_blitter.c
@@ -457,10 +457,11 @@ pan_blitter_get_blit_shader(struct panfrost_device *dev,
 
       coord_comps = MAX2(coord_comps, (key->surfaces[i].dim ?: 3) +
                                          (key->surfaces[i].array ? 1 : 0));
-      first = false;
 
-      if (sig_offset >= sizeof(sig))
+      if (sig_offset >= sizeof(sig)) {
+         first = false;
          continue;
+      }
 
       sig_offset +=
          snprintf(sig + sig_offset, sizeof(sig) - sig_offset,
@@ -468,6 +469,8 @@ pan_blitter_get_blit_shader(struct panfrost_device *dev,
                   first ? "" : ",", gl_frag_result_name(key->surfaces[i].loc),
                   type_str, dim_str, key->surfaces[i].array ? "[]" : "",
                   key->surfaces[i].src_samples, key->surfaces[i].dst_samples);
+
+      first = false;
    }
 
    nir_builder b = nir_builder_init_simple_shader(
diff --git a/src/panfrost/midgard/midgard_compile.c b/src/panfrost/midgard/midgard_compile.c
index 2864b3195bc..631bc1e6cbc 100644
--- a/src/panfrost/midgard/midgard_compile.c
+++ b/src/panfrost/midgard/midgard_compile.c
@@ -1263,12 +1263,32 @@ emit_varying_read(compiler_context *ctx, unsigned dest, unsigned offset,
    ins.load_store.arg_reg = REGISTER_LDST_ZERO;
    ins.load_store.index_format = midgard_index_address_u32;
 
-   /* For flat shading, we always use .u32 and require 32-bit mode. For
-    * smooth shading, we use the appropriate floating-point type.
+   /* For flat shading, for GPUs supporting auto32, we always use .u32 and
+    * require 32-bit mode. For smooth shading, we use the appropriate
+    * floating-point type.
     *
     * This could be optimized, but it makes it easy to check correctness.
     */
-   if (flat) {
+   if (ctx->quirks & MIDGARD_NO_AUTO32) {
+      switch (type) {
+      case nir_type_uint32:
+      case nir_type_bool32:
+         ins.op = midgard_op_ld_vary_32u;
+         break;
+      case nir_type_int32:
+         ins.op = midgard_op_ld_vary_32i;
+         break;
+      case nir_type_float32:
+         ins.op = midgard_op_ld_vary_32;
+         break;
+      case nir_type_float16:
+         ins.op = midgard_op_ld_vary_16;
+         break;
+      default:
+         unreachable("Attempted to load unknown type");
+         break;
+      }
+   } else if (flat) {
       assert(nir_alu_type_get_type_size(type) == 32);
       ins.op = midgard_op_ld_vary_32u;
    } else {
@@ -2896,6 +2916,7 @@ midgard_compile_shader_nir(nir_shader *nir,
    ctx->ssa_constants = _mesa_hash_table_u64_create(ctx);
 
    /* Collect varyings after lowering I/O */
+   info->quirk_no_auto32 = (ctx->quirks & MIDGARD_NO_AUTO32);
    pan_nir_collect_varyings(nir, info);
 
    /* Optimisation passes */
diff --git a/src/panfrost/midgard/midgard_quirks.h b/src/panfrost/midgard/midgard_quirks.h
index 3003dbdf7c2..fd7f797e04b 100644
--- a/src/panfrost/midgard/midgard_quirks.h
+++ b/src/panfrost/midgard/midgard_quirks.h
@@ -66,11 +66,19 @@
 
 #define MIDGARD_NO_OOO (1 << 5)
 
+/* Disable auto32 type (apparently broken on T60x). */
+
+#define MIDGARD_NO_AUTO32 (1 << 6)
+
 static inline unsigned
 midgard_get_quirks(unsigned gpu_id)
 {
    switch (gpu_id) {
    case 0x600:
+      return MIDGARD_OLD_BLEND | MIDGARD_BROKEN_BLEND_LOADS |
+             MIDGARD_BROKEN_LOD | MIDGARD_NO_UPPER_ALU | MIDGARD_NO_OOO |
+             MIDGARD_NO_AUTO32;
+
    case 0x620:
       return MIDGARD_OLD_BLEND | MIDGARD_BROKEN_BLEND_LOADS |
              MIDGARD_BROKEN_LOD | MIDGARD_NO_UPPER_ALU | MIDGARD_NO_OOO;
diff --git a/src/panfrost/util/pan_collect_varyings.c b/src/panfrost/util/pan_collect_varyings.c
index b5cc72c51ab..b69e255290a 100644
--- a/src/panfrost/util/pan_collect_varyings.c
+++ b/src/panfrost/util/pan_collect_varyings.c
@@ -67,10 +67,17 @@ struct slot_info {
    unsigned index;
 };
 
+struct walk_varyings_data {
+   struct pan_shader_info *info;
+   struct slot_info *slots;
+};
+
 static bool
 walk_varyings(UNUSED nir_builder *b, nir_instr *instr, void *data)
 {
-   struct slot_info *slots = data;
+   struct walk_varyings_data *wv_data = data;
+   struct pan_shader_info *info = wv_data->info;
+   struct slot_info *slots = wv_data->slots;
 
    if (instr->type != nir_instr_type_intrinsic)
       return false;
@@ -113,8 +120,9 @@ walk_varyings(UNUSED nir_builder *b, nir_instr *instr, void *data)
     * only to determine the type, and the GL linker uses the type from the
     * fragment shader instead.
     */
-   bool flat = (intr->intrinsic != nir_intrinsic_load_interpolated_input);
-   nir_alu_type type = flat ? nir_type_uint : nir_type_float;
+   bool flat = intr->intrinsic != nir_intrinsic_load_interpolated_input;
+   bool auto32 = !info->quirk_no_auto32;
+   nir_alu_type type = (flat && auto32) ? nir_type_uint : nir_type_float;
 
    /* Demote interpolated float varyings to fp16 where possible. We do not
     * demote flat varyings, including integer varyings, due to various
@@ -161,7 +169,8 @@ pan_nir_collect_varyings(nir_shader *s, struct pan_shader_info *info)
       return;
 
    struct slot_info slots[64] = {0};
-   nir_shader_instructions_pass(s, walk_varyings, nir_metadata_all, slots);
+   struct walk_varyings_data wv_data = {info, slots};
+   nir_shader_instructions_pass(s, walk_varyings, nir_metadata_all, &wv_data);
 
    struct pan_shader_varying *varyings = (s->info.stage == MESA_SHADER_VERTEX)
                                             ? info->varyings.output
diff --git a/src/panfrost/util/pan_ir.h b/src/panfrost/util/pan_ir.h
index 5551fc7526a..46d1a530a3e 100644
--- a/src/panfrost/util/pan_ir.h
+++ b/src/panfrost/util/pan_ir.h
@@ -307,6 +307,9 @@ struct pan_shader_info {
 
    uint32_t ubo_mask;
 
+   /* Quirk for GPUs that does not support auto32 types. */
+   bool quirk_no_auto32;
+
    union {
       struct bifrost_shader_info bifrost;
       struct midgard_shader_info midgard;
diff --git a/src/panfrost/vulkan/panvk_formats.c b/src/panfrost/vulkan/panvk_formats.c
index 956d4691677..3186b5dedff 100644
--- a/src/panfrost/vulkan/panvk_formats.c
+++ b/src/panfrost/vulkan/panvk_formats.c
@@ -414,6 +414,13 @@ panvk_GetPhysicalDeviceImageFormatProperties2(
     *    present and VkExternalImageFormatProperties will be ignored.
     */
    if (external_info && external_info->handleType != 0) {
+      VkExternalImageFormatProperties fallback_external_props;
+
+      if (!external_props) {
+         memset(&fallback_external_props, 0, sizeof(fallback_external_props));
+         external_props = &fallback_external_props;
+      }
+
       result = panvk_get_external_image_format_properties(
          physical_device, base_info, external_info->handleType,
          &external_props->externalMemoryProperties);
diff --git a/src/panfrost/vulkan/panvk_vX_cmd_buffer.c b/src/panfrost/vulkan/panvk_vX_cmd_buffer.c
index c26790c68f8..ef07183f17d 100644
--- a/src/panfrost/vulkan/panvk_vX_cmd_buffer.c
+++ b/src/panfrost/vulkan/panvk_vX_cmd_buffer.c
@@ -532,6 +532,7 @@ panvk_fill_non_vs_attribs(struct panvk_cmd_buffer *cmdbuf,
          pan_pack(attribs + offset, ATTRIBUTE, cfg) {
             cfg.buffer_index = first_buf + (img_idx + i) * 2;
             cfg.format = desc_state->sets[s]->img_fmts[i];
+            cfg.offset_enable = false;
          }
          offset += pan_size(ATTRIBUTE);
       }
diff --git a/src/panfrost/vulkan/panvk_vX_cs.c b/src/panfrost/vulkan/panvk_vX_cs.c
index c78e4ed701c..c40bec7ca5d 100644
--- a/src/panfrost/vulkan/panvk_vX_cs.c
+++ b/src/panfrost/vulkan/panvk_vX_cs.c
@@ -116,6 +116,7 @@ panvk_emit_varying(const struct panvk_device *dev,
       cfg.buffer_index = varyings->varying[loc].buf;
       cfg.offset = varyings->varying[loc].offset;
       cfg.format = panvk_varying_hw_format(dev, varyings, stage, idx);
+      cfg.offset_enable = false;
    }
 }
 
@@ -286,6 +287,7 @@ panvk_emit_attrib(const struct panvk_device *dev,
    pan_pack(attrib, ATTRIBUTE, cfg) {
       cfg.buffer_index = buf_idx * 2;
       cfg.offset = attribs->attrib[idx].offset + (bufs[buf_idx].address & 63);
+      cfg.offset_enable = true;
 
       if (buf_info->per_instance)
          cfg.offset += draw->first_instance * buf_info->stride;
diff --git a/src/panfrost/vulkan/panvk_vX_meta_copy.c b/src/panfrost/vulkan/panvk_vX_meta_copy.c
index 6abb644e76c..434835155d0 100644
--- a/src/panfrost/vulkan/panvk_vX_meta_copy.c
+++ b/src/panfrost/vulkan/panvk_vX_meta_copy.c
@@ -82,6 +82,7 @@ panvk_meta_copy_emit_varying(struct pan_pool *pool, mali_ptr coordinates,
    pan_pack(varying.cpu, ATTRIBUTE, cfg) {
       cfg.buffer_index = 0;
       cfg.format = pool->dev->formats[PIPE_FORMAT_R32G32B32_FLOAT].hw;
+      cfg.offset_enable = false;
    }
 
    *varyings = varying.gpu;
diff --git a/src/util/00-mesa-defaults.conf b/src/util/00-mesa-defaults.conf
index ba42e550260..d88dc56f193 100644
--- a/src/util/00-mesa-defaults.conf
+++ b/src/util/00-mesa-defaults.conf
@@ -431,6 +431,13 @@ TODO: document the other workarounds.
             <option name="allow_glsl_120_subset_in_110" value="true" />
         </application>
 
+        <application name="Joe Danger (Wine)" executable="JoeDanger.exe">
+            <option name="allow_glsl_120_subset_in_110" value="true" />
+        </application>
+        <application name="Joe Danger 2 (Wine)" executable="JoeDanger2.exe">
+            <option name="allow_glsl_120_subset_in_110" value="true" />
+        </application>
+
         <application name="BETA CAE Systems - GL detect tool" executable="detect_opengl_tool">
             <option name="mesa_extension_override" value="-GL_MESA_pack_invert -GL_MESA_framebuffer_flip_y -GL_MESA_window_pos" />
         </application>
@@ -1220,6 +1227,9 @@ TODO: document the other workarounds.
         <application name="The Finals" executable="Discovery.exe">
             <option name="force_vk_vendor" value="-1" />
         </application>
+        <application name="Shadow of the Tomb Raider" executable="SOTTR.exe">
+            <option name="force_vk_vendor" value="-1" />
+        </application>
         <!--
         Disable 16-bit feature on zink and angle so that GLES mediump doesn't
         lower to our inefficent 16-bit shader support.  No need to do so for
diff --git a/src/util/00-radv-defaults.conf b/src/util/00-radv-defaults.conf
index 42fda93e1ec..648484edcc5 100644
--- a/src/util/00-radv-defaults.conf
+++ b/src/util/00-radv-defaults.conf
@@ -85,6 +85,7 @@ Application bugs worked around in this file:
         <application name="RAGE 2" executable="RAGE2.exe">
             <option name="radv_enable_mrt_output_nan_fixup" value="true" />
             <option name="radv_app_layer" value="rage2" />
+            <option name="radv_zero_vram" value="true" />
         </application>
 
         <application name="The Surge 2" application_name_match="Fledge">
@@ -135,6 +136,7 @@ Application bugs worked around in this file:
 
         <application name="RDR2" application_name_match="Red Dead Redemption 2">
             <option name="radv_enable_unified_heap_on_apu" value="true" />
+            <option name="radv_zero_vram" value="true" />
         </application>
 
         <application name="Metro Exodus (Linux native)" application_name_match="metroexodus">
@@ -163,6 +165,10 @@ Application bugs worked around in this file:
             <option name="radv_force_active_accel_struct_leaves" value="true" />
         </application>
 
+        <application name="Helldivers 2" executable="helldivers2.exe">
+            <option name="radv_force_pstate_peak_gfx11_dgpu" value="true" />
+        </application>
+
         <!-- OpenGL Game workarounds (zink) -->
         <application name="Black Geyser: Couriers of Darkness" executable="BlackGeyser.x86_64">
             <option name="radv_zero_vram" value="true" />
@@ -195,5 +201,16 @@ Application bugs worked around in this file:
         <application name="Rocket League" executable="RocketLeague">
             <option name="radv_zero_vram" value="true" />
         </application>
+        <application name="Crystal Project" executable="Crystal Project.bin.x86_64">
+            <option name="radv_zero_vram" value="true" />
+        </application>
+
+        <application name="Half-Life Alyx" application_name_match="hlvr">
+            <option name="dual_color_blend_by_location" value="true" />
+        </application>
+
+        <application name="Enshrouded" executable="enshrouded.exe">
+            <option name="radv_zero_vram" value="true"/>
+        </application>
     </device>
 </driconf>
diff --git a/src/util/bitset.h b/src/util/bitset.h
index cffbb73ecce..d5abb6c1f7c 100644
--- a/src/util/bitset.h
+++ b/src/util/bitset.h
@@ -209,7 +209,8 @@ __bitset_shl(BITSET_WORD *x, unsigned amount, unsigned n)
  */
 #define BITSET_TEST_RANGE_INSIDE_WORD(x, b, e, mask) \
    (BITSET_BITWORD(b) == BITSET_BITWORD(e) ? \
-   (((x)[BITSET_BITWORD(b)] & BITSET_RANGE(b, e)) == mask) : \
+   (((x)[BITSET_BITWORD(b)] & BITSET_RANGE(b, e)) == \
+   (((BITSET_WORD)mask) << (b % BITSET_WORDBITS))) : \
    (assert (!"BITSET_TEST_RANGE: bit range crosses word boundary"), 0))
 #define BITSET_SET_RANGE_INSIDE_WORD(x, b, e) \
    (BITSET_BITWORD(b) == BITSET_BITWORD(e) ? \
diff --git a/src/util/driconf.h b/src/util/driconf.h
index da880b27a20..fdda1015453 100644
--- a/src/util/driconf.h
+++ b/src/util/driconf.h
@@ -694,6 +694,10 @@
    DRI_CONF_OPT_B(radv_legacy_sparse_binding, def, \
                   "Enable legacy sparse binding (with implicit synchronization) on the graphics and compute queue")
 
+#define DRI_CONF_RADV_FORCE_PSTATE_PEAK_GFX11_DGPU(def) \
+   DRI_CONF_OPT_B(radv_force_pstate_peak_gfx11_dgpu, def, \
+                  "Force the performance level to profile_peak (all clocks to the highest levels) for RDNA3 dGPUs")
+
 /**
  * Overrides for forcing re-compilation of pipelines when RADV_BUILD_ID_OVERRIDE is enabled.
  * These need to be bumped every time a compiler bugfix is backported (up to 8 shader
diff --git a/src/util/futex.c b/src/util/futex.c
index fb6072e8f4c..30520a6f048 100644
--- a/src/util/futex.c
+++ b/src/util/futex.c
@@ -142,7 +142,7 @@ int futex_wait(uint32_t *addr, int32_t value, const struct timespec *timeout)
       struct timespec tsnow;
       timespec_get(&tsnow, TIME_UTC);
 
-      timeout_ms = (timeout->tv_sec - tsnow.tv_nsec) * 1000 +
+      timeout_ms = (timeout->tv_sec - tsnow.tv_sec) * 1000 +
                    (timeout->tv_nsec - tsnow.tv_nsec) / 1000000;
    }
 
diff --git a/src/util/u_debug.c b/src/util/u_debug.c
index a86b37a67e9..3a5d3a691a8 100644
--- a/src/util/u_debug.c
+++ b/src/util/u_debug.c
@@ -423,18 +423,17 @@ parse_debug_string(const char *debug,
 
    if (debug != NULL) {
       for (; control->string != NULL; control++) {
-         if (!strncmp(debug, "all", strlen("all"))) {
-            flag |= control->flag;
+         const char *s = debug;
+         unsigned n;
 
-         } else {
-            const char *s = debug;
-            unsigned n;
+         for (; n = strcspn(s, ", "), *s; s += MAX2(1, n)) {
+            if (!n)
+               continue;
 
-            for (; n = strcspn(s, ", "), *s; s += MAX2(1, n)) {
-               if (strlen(control->string) == n &&
-                   !strncmp(control->string, s, n))
-                  flag |= control->flag;
-            }
+            if (!strncmp("all", s, n) ||
+                (strlen(control->string) == n &&
+                !strncmp(control->string, s, n)))
+               flag |= control->flag;
          }
       }
    }
diff --git a/src/virtio/vulkan/vn_descriptor_set.c b/src/virtio/vulkan/vn_descriptor_set.c
index a1899c3b409..f079262421b 100644
--- a/src/virtio/vulkan/vn_descriptor_set.c
+++ b/src/virtio/vulkan/vn_descriptor_set.c
@@ -390,6 +390,8 @@ vn_CreateDescriptorPool(VkDevice device,
    vn_async_vkCreateDescriptorPool(dev->primary_ring, device, pCreateInfo,
                                    NULL, &pool_handle);
 
+   vn_tls_set_async_pipeline_create();
+
    *pDescriptorPool = pool_handle;
 
    return VK_SUCCESS;
diff --git a/src/virtio/vulkan/vn_device.c b/src/virtio/vulkan/vn_device.c
index 00ff23ebced..61e49de673e 100644
--- a/src/virtio/vulkan/vn_device.c
+++ b/src/virtio/vulkan/vn_device.c
@@ -570,6 +570,8 @@ vn_CreateDevice(VkPhysicalDevice physicalDevice,
       vn_log(instance, "%s", physical_dev->properties.vulkan_1_2.driverInfo);
    }
 
+   vn_tls_set_async_pipeline_create();
+
    *pDevice = vn_device_to_handle(dev);
 
    return VK_SUCCESS;
diff --git a/src/virtio/vulkan/vn_image.c b/src/virtio/vulkan/vn_image.c
index e7a5a791895..ccb7ca0ad26 100644
--- a/src/virtio/vulkan/vn_image.c
+++ b/src/virtio/vulkan/vn_image.c
@@ -231,8 +231,15 @@ vn_image_store_reqs_in_cache(struct vn_device *dev,
    assert(cache->ht);
 
    simple_mtx_lock(&cache->mutex);
-   uint32_t cache_entry_count = _mesa_hash_table_num_entries(cache->ht);
-   if (cache_entry_count == IMAGE_REQS_CACHE_MAX_ENTRIES) {
+
+   /* Check if entry was added before lock */
+   if (_mesa_hash_table_search(cache->ht, key)) {
+      simple_mtx_unlock(&cache->mutex);
+      return;
+   }
+
+   if (_mesa_hash_table_num_entries(cache->ht) ==
+       IMAGE_REQS_CACHE_MAX_ENTRIES) {
       /* Evict/use the last entry in the lru list for this new entry */
       cache_entry =
          list_last_entry(&cache->lru, struct vn_image_reqs_cache_entry, head);
@@ -242,11 +249,11 @@ vn_image_store_reqs_in_cache(struct vn_device *dev,
    } else {
       cache_entry = vk_zalloc(alloc, sizeof(*cache_entry), VN_DEFAULT_ALIGN,
                               VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
+      if (!cache_entry) {
+         simple_mtx_unlock(&cache->mutex);
+         return;
+      }
    }
-   simple_mtx_unlock(&cache->mutex);
-
-   if (!cache_entry)
-      return;
 
    for (uint32_t i = 0; i < plane_count; i++)
       cache_entry->requirements[i] = requirements[i];
@@ -254,12 +261,10 @@ vn_image_store_reqs_in_cache(struct vn_device *dev,
    memcpy(cache_entry->key, key, SHA1_DIGEST_LENGTH);
    cache_entry->plane_count = plane_count;
 
-   simple_mtx_lock(&cache->mutex);
-   if (!_mesa_hash_table_search(cache->ht, cache_entry->key)) {
-      _mesa_hash_table_insert(dev->image_reqs_cache.ht, cache_entry->key,
-                              cache_entry);
-      list_add(&cache_entry->head, &cache->lru);
-   }
+   _mesa_hash_table_insert(dev->image_reqs_cache.ht, cache_entry->key,
+                           cache_entry);
+   list_add(&cache_entry->head, &cache->lru);
+
    simple_mtx_unlock(&cache->mutex);
 }
 
diff --git a/src/virtio/vulkan/vn_pipeline.c b/src/virtio/vulkan/vn_pipeline.c
index 835b55852de..65b617eed3f 100644
--- a/src/virtio/vulkan/vn_pipeline.c
+++ b/src/virtio/vulkan/vn_pipeline.c
@@ -590,27 +590,48 @@ vn_create_pipeline_handles(struct vn_device *dev,
    return true;
 }
 
-/** For vkCreate*Pipelines.  */
 static void
-vn_destroy_failed_pipelines(struct vn_device *dev,
-                            uint32_t create_info_count,
-                            VkPipeline *pipelines,
-                            const VkAllocationCallbacks *alloc)
+vn_destroy_pipeline_handles_internal(struct vn_device *dev,
+                                     uint32_t pipeline_count,
+                                     VkPipeline *pipeline_handles,
+                                     const VkAllocationCallbacks *alloc,
+                                     bool failed_only)
 {
-   for (uint32_t i = 0; i < create_info_count; i++) {
-      struct vn_pipeline *pipeline = vn_pipeline_from_handle(pipelines[i]);
+   for (uint32_t i = 0; i < pipeline_count; i++) {
+      struct vn_pipeline *pipeline =
+         vn_pipeline_from_handle(pipeline_handles[i]);
 
-      if (pipeline->base.id == 0) {
+      if (!failed_only || pipeline->base.id == 0) {
          if (pipeline->layout) {
             vn_pipeline_layout_unref(dev, pipeline->layout);
          }
          vn_object_base_fini(&pipeline->base);
          vk_free(alloc, pipeline);
-         pipelines[i] = VK_NULL_HANDLE;
+         pipeline_handles[i] = VK_NULL_HANDLE;
       }
    }
 }
 
+static inline void
+vn_destroy_pipeline_handles(struct vn_device *dev,
+                            uint32_t pipeline_count,
+                            VkPipeline *pipeline_handles,
+                            const VkAllocationCallbacks *alloc)
+{
+   vn_destroy_pipeline_handles_internal(dev, pipeline_count, pipeline_handles,
+                                        alloc, false);
+}
+
+static inline void
+vn_destroy_failed_pipeline_handles(struct vn_device *dev,
+                                   uint32_t pipeline_count,
+                                   VkPipeline *pipeline_handles,
+                                   const VkAllocationCallbacks *alloc)
+{
+   vn_destroy_pipeline_handles_internal(dev, pipeline_count, pipeline_handles,
+                                        alloc, true);
+}
+
 #define VN_PIPELINE_CREATE_SYNC_MASK                                         \
    (VK_PIPELINE_CREATE_FAIL_ON_PIPELINE_COMPILE_REQUIRED_BIT |               \
     VK_PIPELINE_CREATE_EARLY_RETURN_ON_FAILURE_BIT)
@@ -1544,7 +1565,7 @@ vn_CreateGraphicsPipelines(VkDevice device,
    pCreateInfos = vn_fix_graphics_pipeline_create_infos(
       dev, createInfoCount, pCreateInfos, fix_descs, &fix_tmp, alloc);
    if (!pCreateInfos) {
-      vn_destroy_failed_pipelines(dev, createInfoCount, pPipelines, alloc);
+      vn_destroy_pipeline_handles(dev, createInfoCount, pPipelines, alloc);
       STACK_ARRAY_FINISH(fix_descs);
       return vn_error(dev->instance, VK_ERROR_OUT_OF_HOST_MEMORY);
    }
@@ -1572,7 +1593,7 @@ vn_CreateGraphicsPipelines(VkDevice device,
    struct vn_ring *target_ring = vn_get_target_ring(dev);
    if (!target_ring) {
       vk_free(alloc, fix_tmp);
-      vn_destroy_failed_pipelines(dev, createInfoCount, pPipelines, alloc);
+      vn_destroy_pipeline_handles(dev, createInfoCount, pPipelines, alloc);
       STACK_ARRAY_FINISH(fix_descs);
       return vn_error(dev->instance, VK_ERROR_OUT_OF_HOST_MEMORY);
    }
@@ -1582,7 +1603,8 @@ vn_CreateGraphicsPipelines(VkDevice device,
          target_ring, device, pipelineCache, createInfoCount, pCreateInfos,
          NULL, pPipelines);
       if (result != VK_SUCCESS)
-         vn_destroy_failed_pipelines(dev, createInfoCount, pPipelines, alloc);
+         vn_destroy_failed_pipeline_handles(dev, createInfoCount, pPipelines,
+                                            alloc);
    } else {
       vn_async_vkCreateGraphicsPipelines(target_ring, device, pipelineCache,
                                          createInfoCount, pCreateInfos, NULL,
@@ -1633,7 +1655,7 @@ vn_CreateComputePipelines(VkDevice device,
 
    struct vn_ring *target_ring = vn_get_target_ring(dev);
    if (!target_ring) {
-      vn_destroy_failed_pipelines(dev, createInfoCount, pPipelines, alloc);
+      vn_destroy_pipeline_handles(dev, createInfoCount, pPipelines, alloc);
       return vn_error(dev->instance, VK_ERROR_OUT_OF_HOST_MEMORY);
    }
 
@@ -1642,7 +1664,8 @@ vn_CreateComputePipelines(VkDevice device,
          target_ring, device, pipelineCache, createInfoCount, pCreateInfos,
          NULL, pPipelines);
       if (result != VK_SUCCESS)
-         vn_destroy_failed_pipelines(dev, createInfoCount, pPipelines, alloc);
+         vn_destroy_failed_pipeline_handles(dev, createInfoCount, pPipelines,
+                                            alloc);
    } else {
       vn_async_vkCreateComputePipelines(target_ring, device, pipelineCache,
                                         createInfoCount, pCreateInfos, NULL,
diff --git a/src/virtio/vulkan/vn_queue.c b/src/virtio/vulkan/vn_queue.c
index 22d469bba2a..703958fbf66 100644
--- a/src/virtio/vulkan/vn_queue.c
+++ b/src/virtio/vulkan/vn_queue.c
@@ -600,7 +600,7 @@ vn_queue_submission_add_query_feedback(struct vn_queue_submission *submit,
    VkCommandBuffer *feedback_cmd_handle =
       vn_get_feedback_cmd_handle(submit, feedback_cmds, cmd_count);
    const uint32_t stride = submit->batch_type == VK_STRUCTURE_TYPE_SUBMIT_INFO
-                              ? sizeof(VkCommandBuffer *)
+                              ? sizeof(VkCommandBuffer)
                               : sizeof(VkCommandBufferSubmitInfo);
 
    struct vn_feedback_cmd_pool *feedback_cmd_pool = NULL;
@@ -857,7 +857,7 @@ vn_queue_submission_setup_batches(struct vn_queue_submission *submit)
     * to modify cmd buffer.
     * Only needed for non-empty submissions
     */
-   if (submit->batches) {
+   if (submit->batch_count) {
       memcpy(submit->temp.batches, submit->batches,
              batch_size * submit->batch_count);
    }
diff --git a/src/virtio/vulkan/vn_ring.c b/src/virtio/vulkan/vn_ring.c
index 085a24f1551..39c16acbeaa 100644
--- a/src/virtio/vulkan/vn_ring.c
+++ b/src/virtio/vulkan/vn_ring.c
@@ -343,7 +343,7 @@ vn_ring_destroy(struct vn_ring *ring)
 
    list_for_each_entry_safe(struct vn_ring_submit, submit,
                             &ring->free_submits, head)
-      vk_free(alloc, submit);
+      free(submit);
 
    vn_cs_encoder_fini(&ring->upload);
    vn_renderer_shmem_unref(ring->instance->renderer, ring->shmem);
@@ -362,7 +362,6 @@ vn_ring_get_id(struct vn_ring *ring)
 static struct vn_ring_submit *
 vn_ring_get_submit(struct vn_ring *ring, uint32_t shmem_count)
 {
-   const VkAllocationCallbacks *alloc = &ring->instance->base.base.alloc;
    const uint32_t min_shmem_count = 2;
    struct vn_ring_submit *submit;
 
@@ -375,8 +374,7 @@ vn_ring_get_submit(struct vn_ring *ring, uint32_t shmem_count)
    } else {
       const size_t submit_size = offsetof(
          struct vn_ring_submit, shmems[MAX2(shmem_count, min_shmem_count)]);
-      submit = vk_alloc(alloc, submit_size, VN_DEFAULT_ALIGN,
-                        VK_SYSTEM_ALLOCATION_SCOPE_INSTANCE);
+      submit = malloc(submit_size);
    }
 
    return submit;
@@ -419,8 +417,7 @@ vn_ring_submit_internal(struct vn_ring *ring,
 }
 
 static const struct vn_cs_encoder *
-vn_ring_submission_get_cs(struct vn_ring *ring,
-                          struct vn_ring_submission *submit,
+vn_ring_submission_get_cs(struct vn_ring_submission *submit,
                           const struct vn_cs_encoder *cs,
                           bool direct)
 {
@@ -445,9 +442,7 @@ vn_ring_submission_get_cs(struct vn_ring *ring,
       desc_count, descs, NULL, 0, NULL, 0);
    void *exec_data = submit->indirect.data;
    if (exec_size > sizeof(submit->indirect.data)) {
-      const VkAllocationCallbacks *alloc = &ring->instance->base.base.alloc;
-      exec_data = vk_alloc(alloc, exec_size, VN_DEFAULT_ALIGN,
-                           VK_SYSTEM_ALLOCATION_SCOPE_INSTANCE);
+      exec_data = malloc(exec_size);
       if (!exec_data) {
          STACK_ARRAY_FINISH(descs);
          return NULL;
@@ -495,13 +490,11 @@ vn_ring_submission_get_ring_submit(struct vn_ring *ring,
 }
 
 static inline void
-vn_ring_submission_cleanup(struct vn_ring *ring,
-                           struct vn_ring_submission *submit)
+vn_ring_submission_cleanup(struct vn_ring_submission *submit)
 {
-   const VkAllocationCallbacks *alloc = &ring->instance->base.base.alloc;
    if (submit->cs == &submit->indirect.cs &&
        submit->indirect.buffer.base != submit->indirect.data)
-      vk_free(alloc, submit->indirect.buffer.base);
+      free(submit->indirect.buffer.base);
 }
 
 static VkResult
@@ -511,14 +504,14 @@ vn_ring_submission_prepare(struct vn_ring *ring,
                            struct vn_renderer_shmem *extra_shmem,
                            bool direct)
 {
-   submit->cs = vn_ring_submission_get_cs(ring, submit, cs, direct);
+   submit->cs = vn_ring_submission_get_cs(submit, cs, direct);
    if (!submit->cs)
       return VK_ERROR_OUT_OF_HOST_MEMORY;
 
    submit->submit =
       vn_ring_submission_get_ring_submit(ring, cs, extra_shmem, direct);
    if (!submit->submit) {
-      vn_ring_submission_cleanup(ring, submit);
+      vn_ring_submission_cleanup(submit);
       return VK_ERROR_OUT_OF_HOST_MEMORY;
    }
 
@@ -586,7 +579,7 @@ vn_ring_submit_locked(struct vn_ring *ring,
                                 vn_cs_encoder_get_len(&local_enc));
    }
 
-   vn_ring_submission_cleanup(ring, &submit);
+   vn_ring_submission_cleanup(&submit);
 
    if (ring_seqno)
       *ring_seqno = seqno;
diff --git a/src/vulkan/runtime/vk_command_buffer.h b/src/vulkan/runtime/vk_command_buffer.h
index 9d2b818655c..e49b3077d34 100644
--- a/src/vulkan/runtime/vk_command_buffer.h
+++ b/src/vulkan/runtime/vk_command_buffer.h
@@ -174,6 +174,12 @@ struct vk_command_buffer {
    struct vk_framebuffer *framebuffer;
    VkRect2D render_area;
 
+   /**
+    * True if we are currently inside a CmdPipelineBarrier() is inserted by
+    * the runtime's vk_render_pass.c
+    */
+   bool runtime_rp_barrier;
+
    /* This uses the same trick as STACK_ARRAY */
    struct vk_attachment_state *attachments;
    struct vk_attachment_state _attachments[8];
diff --git a/src/vulkan/runtime/vk_graphics_state.c b/src/vulkan/runtime/vk_graphics_state.c
index d4f42c3083b..65e24157102 100644
--- a/src/vulkan/runtime/vk_graphics_state.c
+++ b/src/vulkan/runtime/vk_graphics_state.c
@@ -1283,9 +1283,11 @@ vk_graphics_pipeline_state_fill(const struct vk_device *device,
       vk_find_struct_const(info->pNext, GRAPHICS_PIPELINE_LIBRARY_CREATE_INFO_EXT);
    const VkPipelineLibraryCreateInfoKHR *lib_info =
       vk_find_struct_const(info->pNext, PIPELINE_LIBRARY_CREATE_INFO_KHR);
+   
+   VkPipelineCreateFlags2KHR pipeline_flags = vk_graphics_pipeline_create_flags(info);
 
    VkShaderStageFlagBits allowed_stages;
-   if (!(info->flags & VK_PIPELINE_CREATE_LIBRARY_BIT_KHR)) {
+   if (!(pipeline_flags & VK_PIPELINE_CREATE_2_LIBRARY_BIT_KHR)) {
       allowed_stages = VK_SHADER_STAGE_ALL_GRAPHICS |
                        VK_SHADER_STAGE_TASK_BIT_EXT |
                        VK_SHADER_STAGE_MESH_BIT_EXT;
@@ -1330,7 +1332,7 @@ vk_graphics_pipeline_state_fill(const struct vk_device *device,
    if (gpl_info) {
       lib = gpl_info->flags;
    } else if ((lib_info && lib_info->libraryCount > 0) ||
-              (info->flags & VK_PIPELINE_CREATE_LIBRARY_BIT_KHR)) {
+              (pipeline_flags & VK_PIPELINE_CREATE_2_LIBRARY_BIT_KHR)) {
      /*
       * From the Vulkan 1.3.210 spec:
       *    "If this structure is omitted, and either VkGraphicsPipelineCreateInfo::flags
diff --git a/src/vulkan/runtime/vk_pipeline.c b/src/vulkan/runtime/vk_pipeline.c
index 50a87e13a3c..8a6bf431446 100644
--- a/src/vulkan/runtime/vk_pipeline.c
+++ b/src/vulkan/runtime/vk_pipeline.c
@@ -49,6 +49,15 @@ vk_pipeline_shader_stage_is_null(const VkPipelineShaderStageCreateInfo *info)
    return true;
 }
 
+bool
+vk_pipeline_shader_stage_has_identifier(const VkPipelineShaderStageCreateInfo *info)
+{
+   const VkPipelineShaderStageModuleIdentifierCreateInfoEXT *id_info =
+      vk_find_struct_const(info->pNext, PIPELINE_SHADER_STAGE_MODULE_IDENTIFIER_CREATE_INFO_EXT);
+
+   return id_info && id_info->identifierSize != 0;
+}
+
 static nir_shader *
 get_builtin_nir(const VkPipelineShaderStageCreateInfo *info)
 {
diff --git a/src/vulkan/runtime/vk_pipeline.h b/src/vulkan/runtime/vk_pipeline.h
index 1ca32a1428e..94d3a77b41e 100644
--- a/src/vulkan/runtime/vk_pipeline.h
+++ b/src/vulkan/runtime/vk_pipeline.h
@@ -53,6 +53,9 @@ typedef struct VkPipelineShaderStageNirCreateInfoMESA {
 bool
 vk_pipeline_shader_stage_is_null(const VkPipelineShaderStageCreateInfo *info);
 
+bool
+vk_pipeline_shader_stage_has_identifier(const VkPipelineShaderStageCreateInfo *info);
+
 VkResult
 vk_pipeline_shader_stage_to_nir(struct vk_device *device,
                                 const VkPipelineShaderStageCreateInfo *info,
diff --git a/src/vulkan/runtime/vk_render_pass.c b/src/vulkan/runtime/vk_render_pass.c
index 826fd21a9f3..9eb69987383 100644
--- a/src/vulkan/runtime/vk_render_pass.c
+++ b/src/vulkan/runtime/vk_render_pass.c
@@ -1392,13 +1392,40 @@ can_use_attachment_initial_layout(struct vk_command_buffer *cmd_buffer,
    return true;
 }
 
-static void
-set_attachment_layout(struct vk_command_buffer *cmd_buffer,
-                      uint32_t att_idx,
-                      uint32_t view_mask,
-                      VkImageLayout layout,
-                      VkImageLayout stencil_layout)
+uint32_t
+vk_command_buffer_get_attachment_layout(const struct vk_command_buffer *cmd_buffer,
+                                        const struct vk_image *image,
+                                        VkImageLayout *out_layout,
+                                        VkImageLayout *out_stencil_layout)
+{
+   const struct vk_render_pass *render_pass = cmd_buffer->render_pass;
+   assert(render_pass != NULL);
+
+   const struct vk_subpass *subpass =
+      &render_pass->subpasses[cmd_buffer->subpass_idx];
+   int first_view = ffs(subpass->view_mask) - 1;
+
+   for (uint32_t a = 0; a < render_pass->attachment_count; a++) {
+      if (cmd_buffer->attachments[a].image_view->image == image) {
+         *out_layout = cmd_buffer->attachments[a].views[first_view].layout;
+         *out_stencil_layout =
+            cmd_buffer->attachments[a].views[first_view].stencil_layout;
+         return a;
+      }
+   }
+   unreachable("Image not found in attachments");
+}
+
+void
+vk_command_buffer_set_attachment_layout(struct vk_command_buffer *cmd_buffer,
+                                        uint32_t att_idx,
+                                        VkImageLayout layout,
+                                        VkImageLayout stencil_layout)
 {
+   const struct vk_render_pass *render_pass = cmd_buffer->render_pass;
+   const struct vk_subpass *subpass =
+      &render_pass->subpasses[cmd_buffer->subpass_idx];
+   uint32_t view_mask = subpass->view_mask;
    struct vk_attachment_state *att_state = &cmd_buffer->attachments[att_idx];
 
    u_foreach_bit(view, view_mask) {
@@ -1650,9 +1677,10 @@ begin_subpass(struct vk_command_buffer *cmd_buffer,
             };
             __vk_append_struct(color_attachment, color_initial_layout);
 
-            set_attachment_layout(cmd_buffer, sp_att->attachment,
-                                  subpass->view_mask,
-                                  sp_att->layout, VK_IMAGE_LAYOUT_UNDEFINED);
+            vk_command_buffer_set_attachment_layout(cmd_buffer,
+                                                    sp_att->attachment,
+                                                    sp_att->layout,
+                                                    VK_IMAGE_LAYOUT_UNDEFINED);
          }
       } else {
          /* We've seen at least one of the views of this attachment before so
@@ -1770,9 +1798,10 @@ begin_subpass(struct vk_command_buffer *cmd_buffer,
                                   &stencil_initial_layout);
             }
 
-            set_attachment_layout(cmd_buffer, sp_att->attachment,
-                                  subpass->view_mask,
-                                  sp_att->layout, sp_att->stencil_layout);
+            vk_command_buffer_set_attachment_layout(cmd_buffer,
+                                                    sp_att->attachment,
+                                                    sp_att->layout,
+                                                    sp_att->stencil_layout);
          }
       } else {
          /* We've seen at least one of the views of this attachment before so
@@ -2048,8 +2077,10 @@ begin_subpass(struct vk_command_buffer *cmd_buffer,
          .pImageMemoryBarriers = image_barrier_count > 0 ?
                                  image_barriers : NULL,
       };
+      cmd_buffer->runtime_rp_barrier = true;
       disp->CmdPipelineBarrier2(vk_command_buffer_to_handle(cmd_buffer),
                                 &dependency_info);
+      cmd_buffer->runtime_rp_barrier = false;
    }
 
    STACK_ARRAY_FINISH(image_barriers);
@@ -2227,8 +2258,10 @@ end_subpass(struct vk_command_buffer *cmd_buffer,
          .memoryBarrierCount = 1,
          .pMemoryBarriers = &mem_barrier,
       };
+      cmd_buffer->runtime_rp_barrier = true;
       disp->CmdPipelineBarrier2(vk_command_buffer_to_handle(cmd_buffer),
                                 &dependency_info);
+      cmd_buffer->runtime_rp_barrier = false;
    }
 }
 
@@ -2455,8 +2488,10 @@ vk_common_CmdEndRenderPass2(VkCommandBuffer commandBuffer,
          .imageMemoryBarrierCount = image_barrier_count,
          .pImageMemoryBarriers = image_barriers,
       };
+      cmd_buffer->runtime_rp_barrier = true;
       disp->CmdPipelineBarrier2(vk_command_buffer_to_handle(cmd_buffer),
                                 &dependency_info);
+      cmd_buffer->runtime_rp_barrier = false;
    }
 
    STACK_ARRAY_FINISH(image_barriers);
diff --git a/src/vulkan/runtime/vk_render_pass.h b/src/vulkan/runtime/vk_render_pass.h
index 71ba81ec059..9acd65aa3ad 100644
--- a/src/vulkan/runtime/vk_render_pass.h
+++ b/src/vulkan/runtime/vk_render_pass.h
@@ -29,6 +29,9 @@
 extern "C" {
 #endif
 
+struct vk_command_buffer;
+struct vk_image;
+
 /**
  * Pseudo-extension struct that may be chained into VkRenderingAttachmentInfo
  * to indicate an initial layout for the attachment.  This is only allowed if
@@ -425,9 +428,9 @@ vk_subpass_dependency_is_fb_local(const VkSubpassDependency2 *dep,
       VK_PIPELINE_STAGE_2_LATE_FRAGMENT_TESTS_BIT |
       VK_PIPELINE_STAGE_2_COLOR_ATTACHMENT_OUTPUT_BIT;
 
-   const VkPipelineStageFlags2 src_framebuffer_space_stages = 
+   const VkPipelineStageFlags2 src_framebuffer_space_stages =
       framebuffer_space_stages | VK_PIPELINE_STAGE_2_TOP_OF_PIPE_BIT;
-   const VkPipelineStageFlags2 dst_framebuffer_space_stages = 
+   const VkPipelineStageFlags2 dst_framebuffer_space_stages =
       framebuffer_space_stages | VK_PIPELINE_STAGE_2_BOTTOM_OF_PIPE_BIT;
 
    /* Check for frambuffer-space dependency. */
@@ -439,6 +442,18 @@ vk_subpass_dependency_is_fb_local(const VkSubpassDependency2 *dep,
    return dep->dependencyFlags & VK_DEPENDENCY_BY_REGION_BIT;
 }
 
+uint32_t
+vk_command_buffer_get_attachment_layout(const struct vk_command_buffer *cmd_buffer,
+                                        const struct vk_image *image,
+                                        VkImageLayout *out_layout,
+                                        VkImageLayout *out_stencil_layout);
+
+void
+vk_command_buffer_set_attachment_layout(struct vk_command_buffer *cmd_buffer,
+                                        uint32_t att_idx,
+                                        VkImageLayout layout,
+                                        VkImageLayout stencil_layout);
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/src/vulkan/runtime/vk_video.c b/src/vulkan/runtime/vk_video.c
index f3153b4a1f6..9934e1271a9 100644
--- a/src/vulkan/runtime/vk_video.c
+++ b/src/vulkan/runtime/vk_video.c
@@ -579,7 +579,7 @@ vk_video_derive_h264_scaling_list(const StdVideoH264SequenceParameterSet *sps,
       {
          if (sps->pScalingLists->scaling_list_present_mask & (1 << i))
             memcpy(temp.ScalingList4x4[i],
-                   pps->pScalingLists->ScalingList4x4[i],
+                   sps->pScalingLists->ScalingList4x4[i],
                    STD_VIDEO_H264_SCALING_LIST_4X4_NUM_ELEMENTS);
          else /* fall-back rule A */
          {
@@ -602,7 +602,7 @@ vk_video_derive_h264_scaling_list(const StdVideoH264SequenceParameterSet *sps,
       {
          int i = j + STD_VIDEO_H264_SCALING_LIST_4X4_NUM_LISTS;
          if (sps->pScalingLists->scaling_list_present_mask & (1 << i))
-            memcpy(temp.ScalingList8x8[j], pps->pScalingLists->ScalingList8x8[j],
+            memcpy(temp.ScalingList8x8[j], sps->pScalingLists->ScalingList8x8[j],
                    STD_VIDEO_H264_SCALING_LIST_8X8_NUM_ELEMENTS);
          else /* fall-back rule A */
          {
@@ -1314,9 +1314,9 @@ enum HEVCNALUnitType {
 };
 
 unsigned
-vk_video_get_h265_nal_unit(StdVideoH265PictureType pic_type, bool irap_pic_flag)
+vk_video_get_h265_nal_unit(const StdVideoEncodeH265PictureInfo *pic_info)
 {
-   switch (pic_type) {
+   switch (pic_info->pic_type) {
    case STD_VIDEO_H265_PICTURE_TYPE_IDR:
       return HEVC_NAL_IDR_W_RADL;
    case STD_VIDEO_H265_PICTURE_TYPE_I:
@@ -1324,10 +1324,16 @@ vk_video_get_h265_nal_unit(StdVideoH265PictureType pic_type, bool irap_pic_flag)
    case STD_VIDEO_H265_PICTURE_TYPE_P:
       return HEVC_NAL_TRAIL_R;
    case STD_VIDEO_H265_PICTURE_TYPE_B:
-      if (irap_pic_flag)
-         return HEVC_NAL_RASL_R;
+      if (pic_info->flags.IrapPicFlag)
+         if (pic_info->flags.is_reference)
+            return HEVC_NAL_RASL_R;
+         else
+            return HEVC_NAL_RASL_N;
       else
-         return HEVC_NAL_TRAIL_R;
+          if (pic_info->flags.is_reference)
+            return HEVC_NAL_TRAIL_R;
+         else
+            return HEVC_NAL_TRAIL_N;
       break;
    default:
       assert(0);
diff --git a/src/vulkan/runtime/vk_video.h b/src/vulkan/runtime/vk_video.h
index 71248537e25..b02da73ce42 100644
--- a/src/vulkan/runtime/vk_video.h
+++ b/src/vulkan/runtime/vk_video.h
@@ -262,7 +262,7 @@ vk_video_encode_h264_pps(StdVideoH264PictureParameterSet *pps,
                          void *data_ptr);
 
 unsigned
-vk_video_get_h265_nal_unit(StdVideoH265PictureType pic_type, bool irap_pic_flag);
+vk_video_get_h265_nal_unit(const StdVideoEncodeH265PictureInfo *pic_info);
 
 void
 vk_video_encode_h265_vps(StdVideoH265VideoParameterSet *vps,
diff --git a/src/vulkan/util/vk_dispatch_table_gen.py b/src/vulkan/util/vk_dispatch_table_gen.py
index 2db977b25de..7a3b459b371 100644
--- a/src/vulkan/util/vk_dispatch_table_gen.py
+++ b/src/vulkan/util/vk_dispatch_table_gen.py
@@ -152,6 +152,62 @@ ${entrypoint_table('instance', instance_entrypoints)}
 ${entrypoint_table('physical_device', physical_device_entrypoints)}
 ${entrypoint_table('device', device_entrypoints)}
 
+<%def name="uncompacted_dispatch_table(entrypoints)">
+% for e in entrypoints:
+  % if e.alias:
+    <% continue %>
+  % endif
+  % if e.guard is not None:
+#ifdef ${e.guard}
+  % endif
+    PFN_vk${e.name} ${e.name};
+  % if e.aliases:
+    % for a in e.aliases:
+    PFN_vk${a.name} ${a.name};
+    % endfor
+  % endif
+  % if e.guard is not None:
+#else
+    PFN_vkVoidFunction ${e.name};
+    % if e.aliases:
+      % for a in e.aliases:
+        PFN_vkVoidFunction ${a.name};
+      % endfor
+    % endif
+#endif
+  % endif
+% endfor
+</%def>
+
+
+struct vk_instance_uncompacted_dispatch_table {
+  ${uncompacted_dispatch_table(instance_entrypoints)}
+};
+
+struct vk_physical_device_uncompacted_dispatch_table {
+  ${uncompacted_dispatch_table(physical_device_entrypoints)}
+};
+
+struct vk_device_uncompacted_dispatch_table {
+  ${uncompacted_dispatch_table(device_entrypoints)}
+};
+
+struct vk_uncompacted_dispatch_table {
+    union {
+        struct {
+            struct vk_instance_uncompacted_dispatch_table instance;
+            struct vk_physical_device_uncompacted_dispatch_table physical_device;
+            struct vk_device_uncompacted_dispatch_table device;
+        };
+
+        struct {
+            ${uncompacted_dispatch_table(instance_entrypoints)}
+            ${uncompacted_dispatch_table(physical_device_entrypoints)}
+            ${uncompacted_dispatch_table(device_entrypoints)}
+        };
+    };
+};
+
 void
 vk_instance_dispatch_table_load(struct vk_instance_dispatch_table *table,
                                 PFN_vkGetInstanceProcAddr gpa,
@@ -165,6 +221,19 @@ vk_device_dispatch_table_load(struct vk_device_dispatch_table *table,
                               PFN_vkGetDeviceProcAddr gpa,
                               VkDevice device);
 
+void
+vk_instance_uncompacted_dispatch_table_load(struct vk_instance_uncompacted_dispatch_table *table,
+                                PFN_vkGetInstanceProcAddr gpa,
+                                VkInstance instance);
+void
+vk_physical_device_uncompacted_dispatch_table_load(struct vk_physical_device_uncompacted_dispatch_table *table,
+                                       PFN_vkGetInstanceProcAddr gpa,
+                                       VkInstance instance);
+void
+vk_device_uncompacted_dispatch_table_load(struct vk_device_uncompacted_dispatch_table *table,
+                              PFN_vkGetDeviceProcAddr gpa,
+                              VkDevice device);
+
 void vk_instance_dispatch_table_from_entrypoints(
     struct vk_instance_dispatch_table *dispatch_table,
     const struct vk_instance_entrypoint_table *entrypoint_table,
@@ -267,6 +336,46 @@ ${load_dispatch_table('physical_device', 'VkInstance', 'GetInstanceProcAddr',
 ${load_dispatch_table('device', 'VkDevice', 'GetDeviceProcAddr',
                       device_entrypoints)}
 
+<%def name="load_uncompacted_dispatch_table(type, VkType, ProcAddr, entrypoints)">
+void
+vk_${type}_uncompacted_dispatch_table_load(struct vk_${type}_uncompacted_dispatch_table *table,
+                               PFN_vk${ProcAddr} gpa,
+                               ${VkType} obj)
+{
+% if type != 'physical_device':
+    table->${ProcAddr} = gpa;
+% endif
+% for e in entrypoints:
+  % if e.alias or e.name == '${ProcAddr}':
+    <% continue %>
+  % endif
+  % if e.guard is not None:
+#ifdef ${e.guard}
+  % endif
+    table->${e.name} = (PFN_vk${e.name}) gpa(obj, "vk${e.name}");
+  % for a in e.aliases:
+    table->${a.name} = (PFN_vk${a.name}) gpa(obj, "vk${a.name}");
+    if (table->${e.name} && !table->${a.name})
+       table->${a.name} = (PFN_vk${a.name}) table->${e.name};
+    if (!table->${e.name})
+       table->${e.name} = (PFN_vk${e.name}) table->${a.name};
+  % endfor
+  % if e.guard is not None:
+#endif
+  % endif
+% endfor
+}
+</%def>
+
+${load_uncompacted_dispatch_table('instance', 'VkInstance', 'GetInstanceProcAddr',
+                      instance_entrypoints)}
+
+${load_uncompacted_dispatch_table('physical_device', 'VkInstance', 'GetInstanceProcAddr',
+                      physical_device_entrypoints)}
+
+${load_uncompacted_dispatch_table('device', 'VkDevice', 'GetDeviceProcAddr',
+                      device_entrypoints)}
+
 
 struct string_map_entry {
    uint32_t name;
diff --git a/src/vulkan/wsi/wsi_common_drm.c b/src/vulkan/wsi/wsi_common_drm.c
index 9fb802cb2ed..6dfb5255644 100644
--- a/src/vulkan/wsi/wsi_common_drm.c
+++ b/src/vulkan/wsi/wsi_common_drm.c
@@ -542,7 +542,7 @@ wsi_create_native_image_mem(const struct wsi_swapchain *chain,
 
       for (uint32_t p = 0; p < image->num_planes; p++) {
          const VkImageSubresource image_subresource = {
-            .aspectMask = VK_IMAGE_ASPECT_PLANE_0_BIT << p,
+            .aspectMask = VK_IMAGE_ASPECT_MEMORY_PLANE_0_BIT_EXT << p,
             .mipLevel = 0,
             .arrayLayer = 0,
          };
diff --git a/src/vulkan/wsi/wsi_common_x11.c b/src/vulkan/wsi/wsi_common_x11.c
index 07a80458a7c..849c78cd171 100644
--- a/src/vulkan/wsi/wsi_common_x11.c
+++ b/src/vulkan/wsi/wsi_common_x11.c
@@ -1670,21 +1670,23 @@ x11_present_to_x11_dri3(struct x11_swapchain *chain, uint32_t image_index,
    image->present_queued = true;
    image->serial = (uint32_t) chain->send_sbc;
 
-   xcb_present_pixmap(chain->conn,
-                      chain->window,
-                      image->pixmap,
-                      image->serial,
-                      0,                            /* valid */
-                      image->update_area,           /* update */
-                      0,                            /* x_off */
-                      0,                            /* y_off */
-                      XCB_NONE,                     /* target_crtc */
-                      XCB_NONE,
-                      image->sync_fence,
-                      options,
-                      target_msc,
-                      divisor,
-                      remainder, 0, NULL);
+   xcb_void_cookie_t cookie =
+      xcb_present_pixmap(chain->conn,
+                         chain->window,
+                         image->pixmap,
+                         image->serial,
+                         0,                            /* valid */
+                         image->update_area,           /* update */
+                         0,                            /* x_off */
+                         0,                            /* y_off */
+                         XCB_NONE,                     /* target_crtc */
+                         XCB_NONE,
+                         image->sync_fence,
+                         options,
+                         target_msc,
+                         divisor,
+                         remainder, 0, NULL);
+   xcb_discard_reply(chain->conn, cookie.sequence);
    xcb_flush(chain->conn);
    return x11_swapchain_result(chain, VK_SUCCESS);
 }
@@ -1711,7 +1713,7 @@ x11_present_to_x11_sw(struct x11_swapchain *chain, uint32_t image_index,
                              chain->gc,
                              image->base.row_pitches[0] / 4,
                              chain->extent.height,
-                             0,0,0,24,
+                             0,0,0,chain->depth,
                              image->base.row_pitches[0] * chain->extent.height,
                              image->base.cpu_map);
       xcb_discard_reply(chain->conn, cookie.sequence);
@@ -1726,7 +1728,7 @@ x11_present_to_x11_sw(struct x11_swapchain *chain, uint32_t image_index,
                                 chain->gc,
                                 image->base.row_pitches[0] / 4,
                                 this_lines,
-                                0,y_start,0,24,
+                                0,y_start,0,chain->depth,
                                 this_lines * stride_b,
                                 (const uint8_t *)myptr + (y_start * stride_b));
          xcb_discard_reply(chain->conn, cookie.sequence);
diff --git a/subprojects/perfetto.wrap b/subprojects/perfetto.wrap
index 8e86d842501..e92c50b6664 100644
--- a/subprojects/perfetto.wrap
+++ b/subprojects/perfetto.wrap
@@ -2,4 +2,4 @@
 directory = perfetto
 
 url = https://android.googlesource.com/platform/external/perfetto
-revision = v29.0
+revision = v45.0
diff --git a/src/egl/drivers/dri2/platform_wayland.c b/src/egl/drivers/dri2/platform_wayland.c
index 1f718ef8a74..ec7574bc0d4 100644
--- a/src/egl/drivers/dri2/platform_wayland.c
+++ b/src/egl/drivers/dri2/platform_wayland.c
@@ -889,7 +889,7 @@ dri2_wl_create_pbuffer_surface(_EGLDisplay *disp, _EGLConfig *conf,
       goto cleanup_surf;
    }
 
-   visual_idx = dri2_wl_visual_idx_from_config(dri2_dpy, config, false);
+   visual_idx = dri2_wl_visual_idx_from_config(dri2_dpy, config);
    assert(visual_idx != -1);
 
    if (dri2_dpy->wl_dmabuf || dri2_dpy->wl_drm) {
